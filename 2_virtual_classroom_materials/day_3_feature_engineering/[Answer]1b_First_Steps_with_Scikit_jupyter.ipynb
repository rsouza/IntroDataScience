{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e8efc1a-199b-4a38-a942-07c683276cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# First steps with scikit-learn\n",
    "\n",
    "I presume that until now you were mainly using Pandas for Data Science. As you heard within our \"Introduction to Feature Engienering\", there is a good reason why we should expand our library toolkit with one that is more suited for **stateful transformations** - such as *scikit-learn*. Afterwards, we are also going to use it for **Machine Learning**.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "\n",
    "1.   Learn the basics about the sklearn documentation\n",
    "2.   Learn how to operate the core API (fit-transform/predict)\n",
    "3.   Learn how to imagine advanced api (composite estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "915e4a7f-a7ec-43ed-a796-aa99c5b0a244",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Documentation of sklearn\n",
    "\n",
    "Here is the homepage:  \n",
    "\n",
    "https://scikit-learn.org/stable/\n",
    "\n",
    "Please focus on the menu on the top, as this is the most important for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95a5e045-e1ea-48aa-ac71-3c1c86463458",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The three most important tabs for us are:\n",
    "\n",
    "\n",
    "1.   [**User Guide**](https://scikit-learn.org/stable/user_guide.html). We will head over here for *thorough guidelines* within a certain topic (e.g. Decision Trees).\n",
    "2.   [**API**](https://scikit-learn.org/stable/modules/classes.html). We will use this whenever we want to learn about a *particular method or module* (e.g. sklearn.impute).\n",
    "3.   [**More > Tutorial**](https://scikit-learn.org/stable/tutorial/index.html). We can use these for some *starting tutorials* (e.g. loading artificial datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4e8ea5-00e1-4c7d-83a2-6d59abcb53ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1 User Guide  \n",
    "\n",
    "User guides are amazing! These are not only a great resource for when we want to perform some operation in sklearn, but also when we want to learn about some topic. There is one important point to remember through: User guides usually cover _everything_ that sklearn offers on a certain topic. This means that the user guide contains basic, and advanced approaches at the same time. **It is therefore important to not get discouraged if you are not able to follow everything in the user guides**. \n",
    "\n",
    "Please go ahead and give the user guide on Preprocessing Data a try. This is among the first topics we will cover.  \n",
    "https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4badc997-6cdd-4ba4-840b-946287df82c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.2 API\n",
    "The actual transformers and estimators that we have available within sklearn are neatly documented within the [API tab](https://scikit-learn.org/stable/modules/classes.html). **Sklearn consists of modules**. You can see a list of modules on the left side. Our first task will be to learn which modules are the most important for us when we are starting with Data Science. Clearly, the [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) will be very useful. If you click on it, the following will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14cb13e9-0d90-47a5-a9c7-2ed64dd2c12f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "These are all the estimators and transformers that are available within the preprocessing module. Also, you can notice a nice cross-reference to [relevant user guides](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing). Our second task will be to understand the most important transformers within a certain module as again, it offers some essential functionalities as well as advanced functionalities.  \n",
    "\n",
    "For example, one of the first transformers that we will practice with is [**MinMaxScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). Please go ahead and click on it. The following page will appear. At the top you usually have a conceptual description of what this transformer does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68dcbab2-1924-40de-879b-fd28f0357196",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Just below of it, you will see the usable **parameters** for this estimator or transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127002ad-43ca-47f9-9a88-6d28ece170fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you scroll further down, you will see **attributes**. These are useful when we, for example, have already fitted the transformer and would like to examine the states that it learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2405f4b0-585f-499b-bf19-f8fd21b7b3da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And finally, if you scroll further down, you will see **methods** that are available with given estimator or transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ea5a5db-704d-46cc-acd3-ace43ede1929",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Core API\n",
    "It is time for some hands-on exercise! Firstly, we will use the datasets module and get one of the example (toy) datasets that are available within sklearn. In this case, it will be [iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14472eb-759e-453a-9018-d38e93315ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the module\n",
    "from sklearn import datasets\n",
    "# Get the dataset\n",
    "iris = datasets.load_iris()\n",
    "#It is a complex object, so we extract just a subset of the data.\n",
    "toy_data = iris.data[1:5]\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3fb244-5ced-422e-b33b-ac7cd0be7200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's now see if the API is really as powerful and standardized as we mentioned within the lecture. \n",
    "We will attempt to implement the [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). \n",
    "Based on what we know, a mere (1) instantiation, (2) fit and (3) transform should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4eff7a7-60d0-47ff-9d77-053da5d0298c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Instantiate\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit\n",
    "scaler.fit(toy_data)\n",
    "\n",
    "# Transform\n",
    "scaler.transform(toy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0471b4c9-db3f-474b-80d3-33493a0755c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The transformation went through! Supposedly, the transformer should also be trained and contain the states required to replicate this transformation. We can examine its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c00c81c-74a8-4691-a875-ba8ee92ed07c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler.data_range_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb56ef23-3902-47a7-81f5-4b9fbca35724",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This looks fairly reasonable. If we look at the third column of the original data, it indeed ranges between 1.3 and 1.5. The transformer learned that its range was 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "249f746e-004d-4025-a5ba-b862e0155054",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## TASK: Your turn\n",
    "Your task is now to try your first transformation with sklearn.\n",
    "Please use the same dataset (`toy_data`) and use\n",
    "[`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "from the preprocessing module. \n",
    "For now, please do not worry what exactly the transformer is doing. \n",
    "In a few lectures from now you will learn about it. \n",
    "It is important that you have your first touch with the Core API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f100da-b157-4a81-b415-aef5368d2842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Instantiate\n",
    "newscaler = StandardScaler()\n",
    "# Fit\n",
    "newscaler.fit(toy_data)\n",
    "# Transform\n",
    "newscaler.transform(toy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1637919a-715e-4352-96b4-6542dd2261c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Advanced API\n",
    "In the upcoming lectures you are going to learn about a lot of individual transformations we can perform, as well as quite a few predictive models that you can implement.  \n",
    "\n",
    "Once we do that, we will arrive at the topic of **composite estimators**. These are part of the Advanced API that sklearn offers. We will learn that it is important to **encapsulate and wrap together** our transformations and predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2345d06-45ee-4e4d-830f-af43f129ebb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.1 Without Advanced API: Ugly\n",
    "Let's at first see what life would look like if there weren't composite estimators.  \n",
    "\n",
    "We want the following to happen:\n",
    "\n",
    "\n",
    "*   Our **feature engineering** should contain two steps that follow each other - (1) StandardScaler (2) PCA reduction.\n",
    "*   On top of our engineered features we would like to fit a **predictive model** - DecisionTree.  \n",
    "\n",
    "Please do not worry if you are not sure what these steps conceptually represent. We will learn that soon, now we are focused on the Advanced API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "addfbe9b-a4a4-4443-8327-a740c8a3a38d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the module\n",
    "from sklearn import datasets\n",
    "\n",
    "# Get the dataset\n",
    "iris = datasets.load_iris()\n",
    "X_train = iris.data\n",
    "y_train = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f97e157-5a42-4ef1-baa1-bc4c8b4f592c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import what we need\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate the 3 transformers/estimators\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)\n",
    "decision_tree = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af3c229-6a6a-4b2a-9d87-c37eea82c83d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have the transformers instantiated. Now observe how I need to fit and transform the data through one transformer at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57eb6125-1732-484a-ba4c-8c21de167630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scaler first\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "#PCA second\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "#Decision Tree finally\n",
    "decision_tree.fit(X_train_pca, y_train)\n",
    "decision_tree.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4291125-3dff-4d4b-ac0a-88a02538144f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Even though it might not look like it from this simple example, this is really not optimal. You can imagine that with a complex dataset the number of various transformations that we would like to employ is high. Notice also how we are coming up with an alternation of the name of our training data after it undergoes certain transformation. This is very prone to some typo or error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e48894e-0812-4303-a02a-98770fb52336",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2 With Advanced API: Pretty & Neat\n",
    "The fact that the Basic API is so standardized allowed developers to develop **composite estimators**. In this example, we will use one of these called **pipeline**. We will learn about more composite estimators later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69bf0439-c57d-4081-8344-077f8546d9b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import what we need\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Compose an entire predictive pipeline at once!\n",
    "predictive_pipeline = Pipeline(([('scaler', StandardScaler()), \n",
    "                                 ('pca', PCA(n_components=2)),\n",
    "                                 ('decision_tree', DecisionTreeClassifier(max_depth=2))\n",
    "                                 ]))\n",
    "#Now we fit the whole pipeline with single call!\n",
    "predictive_pipeline.fit(X_train, y_train)\n",
    "\n",
    "#All that is left is to predict - again with the whole pipeline at once.\n",
    "predictive_pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc9bc2a8-9c59-4e66-875a-467d5298255c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is an example of how powerful the advanced API is. Notice, for example, how the first two operations are fitted without the target feature (`y_train`). You can see this by examining our previous code were we fitted them individually. The third estimator, however, is a supervised model that needs the target feature.  \n",
    "\n",
    "The pipeline is smart enough to provide target feature only to the estimators that need it and we do not have to worry about it, as we are providing the pipeline with both the independent features and a target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352051cd-ccfb-4600-bc68-cb3eb144d111",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Key Takeaways\n",
    "Alright, let's summarize some key takeaways:\n",
    "\n",
    "\n",
    "1.   It is highly recommended to be comfortable with navigating through the documentation of sklearn, just as we introduced it.\n",
    "2.   Understanding the Core API is very important, as it will be our bread and butter within the upcoming hands-on materials.\n",
    "3.   Appreciation that there is Advanced API. We will not work with it right away. Only once we go through all individual components of predictive pipeline, we will learn how to wrap these efficiently into composite transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "635343f6-ffd6-4977-b330-639be522e90d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Material adapted for RBI internal purposes with full permissions from original authors."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "[Answer]1b_First_Steps_with_Scikit_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
