{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b80b64ae-7553-45d9-97bc-f93aec88bbb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 0. Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07117b30-afd4-42be-86a4-a74cd41626bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import pandas library \n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import MinMaxScaler to scale the features\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd5ebf1-bf24-49c2-8a19-6111c844599a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load avocado dataset and store it in the variable dataframe\n",
    "dataframe = pd.read_csv('../../../Data/avocado.csv')\n",
    "# Get the first 5 rows\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8c2c1-4bfc-4757-803c-99aa7222866e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print a summary of the dataframe\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947ed057-17f7-40b5-bbca-5a917a987f00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use numerical variables to create DataFrame data\n",
    "data = dataframe.select_dtypes(include = ['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb15e22c-8ff9-46f8-ba22-df0979cb5f0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Variable scale / Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7a289d-21a2-4c29-8a72-b83d62816093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print descriptive statistics of these variables to see variable's magnitudes\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f82f20-b7b0-4a2e-9cae-3a8665478cc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, our variables have different magnitudes/scales. The minimum and maximum values of the variables are different. For example, the minimum and maximum value for the average price of avocados are 0.44 and 3.25, respectively. For small bags of avocados sold, the minimum and maximum values are 0 and 1.338459e+07, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5301934-80da-48ba-b1b5-8709ff424baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the range of numerical variables\n",
    "for col in['AveragePrice', 'Total Volume', 'Small Hass Avocado','Large Hass Avocado', 'Extra Large Hass Avocado', 'Total Bags',\n",
    "            'Small Bags', 'Large Bags', 'XLarge Bags']:\n",
    "    print(col, 'range is: ', data[col].max() - data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2c5e21a-b41f-4db9-9455-aca7caee3186",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The ranges of our variables are all different! \n",
    "\n",
    "# 2. Feature Scaling\n",
    "\n",
    "Models such as logistic regression, linear regression – or other models that involve a matrix – are very sensitive to different scales of input variables.\n",
    "If we use such data for model fitting, the result might end up creating a bias.\n",
    "Therefore feature scaling techniques are used before model fitting.\n",
    "\n",
    "As you can guess, feature scaling techniques change the scale of the variables.\n",
    "There are several ways how you can scale your features.\n",
    "In this notebook we'll demonstrate the\n",
    "[**MinMaxScaling**](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization))\n",
    "technique that scales variables to their minimum and maximum values.\n",
    "scikit learn offers the\n",
    "[`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "class for this purpose.\n",
    "You can find the documentation\n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
    "\n",
    "The formula for min-max scaling is: \n",
    "\n",
    "$$\n",
    "x_{\\text{std}} = \\frac{x-\\min(x)}{\\max(x)-\\min(x)}\n",
    "$$\n",
    "\n",
    "To return to the original scale this formula is used:\n",
    "\n",
    "$$\n",
    "x = x_{\\text{std}} * (\\max(x) - \\min(x)) + \\min(x)\n",
    "$$\n",
    "\n",
    "- our Scaler subtracts the minimum value from all observations in our dataset and divide it by the range of values\n",
    "- it will transform each feature individually between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da61d724-bfd8-4645-bb82-7a47dc944bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's split our dataset to training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Total Volume', 'Small Hass Avocado','Large Hass Avocado', \n",
    "                                                          'Extra Large Hass Avocado', 'Total Bags', \n",
    "                                                          'Small Bags', 'Large Bags', 'XLarge Bags']],\n",
    "                                                    data['AveragePrice'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47183fa2-58ec-423c-82b9-afc0f4d18f79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b21f5a-a845-4526-8d5a-f071466e4957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit X_train with scaler: this computes and saves the minimum and maximum values \n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e07a13-8158-416a-9cf3-481ccc0b4008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can access the maximum values using the .data_max attribute\n",
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ff92f5-8e99-4e22-99a3-a9640a7026ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can access the minimum values using .data_min attribute\n",
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73ed917-e3e0-4ed7-ae35-cd5ae92e5010",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform X_train and X_test with scaler and store it in the variables X_train_scaled and X_test_scaled\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0da0cd-1f22-4eae-8c12-41b79d299045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the scaled training dataset\n",
    "print('Mean: ', X_train_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_train_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_train_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e53c066-5abd-407d-ad87-75ad2704cf49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After this rescaling, all of the features have a range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822e118c-6b8a-4d3a-b96b-aa0dd660e6be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the scaled testing dataset\n",
    "print('Mean: ', X_test_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_test_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_test_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_test_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54fa521d-0b70-4765-b070-170aa5dcb866",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the range of the features in the test set is not exactly 0 to 1.\n",
    "This is because\n",
    "[`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "has only been trained on the training data `X_train`, not on `X_test`, to prevent data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69654f7c-7c56-4631-92ee-5389bb8bb49f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### TASK\n",
    "\n",
    "Imagine you've normalized the data using\n",
    "[`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "and delivered your work to the Senior Data scientist.\n",
    "He/she proposed you to scale the data using different scaling technique.\n",
    "The technique should transform the data such that its distribution will have a mean value of 0 and a standard deviation of 1.\n",
    "Find the right method [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3926724-8c33-45a0-b843-c8dd70830c48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK >>>> Import the selected Scaling class\n",
    "\n",
    "# TASK >>>> Create a scaler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97cb8465-73b6-4ecc-bd17-47c03f7606bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit X_train using scaler_technique\n",
    "scaler_technique.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c50e53-f058-4c65-98e6-75db44c84e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the scaled values\n",
    "scaler_technique.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c46115-dd55-4a55-8ceb-8e642fcea71b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform X_train using scaler_technique and store it in variable X_training_scaled\n",
    "X_training_scaled = scaler_technique.transform(X_train)\n",
    "\n",
    "# Print X_training_scaled\n",
    "X_training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e42541-0caa-4a21-b092-279dbb6f5b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repeat the scaling also for X_test and store it in variable X_testing_scaled\n",
    "X_testing_scaled = scaler_technique.transform(X_test)\n",
    "\n",
    "# Print X_testing_scaled\n",
    "X_testing_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1701a5-ca92-4e7d-bdde-fd130de40ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print mean and standard deviations of X_training_scaled\n",
    "print('Mean: ', X_training_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_training_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6223742b-fc1c-4fbd-9e51-bc7a7a33e8ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Appendix\n",
    "\n",
    "Data source: \n",
    "\n",
    "Avocado dataset: https://www.kaggle.com/neuromusic/avocado-prices\n",
    "\n",
    "Data license: Database: Open Database\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3_Numerical_Features_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
