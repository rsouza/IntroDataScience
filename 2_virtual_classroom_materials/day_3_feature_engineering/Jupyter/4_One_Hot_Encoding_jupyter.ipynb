{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee49ace5-ca8f-421b-bb50-4d23f83818ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 0. Loading libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fae928c-bffa-43e6-a84c-fd00091fc776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import pandas and numpy libraries\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import OneHotEncoder for one hot encoding \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Import LabelEncoder for target feature encoding\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "507eaacf-4c3c-4e41-b78d-d5d6c0a5049b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Why should we use Encoding ? \n",
    "\n",
    "As we already know, we can't throw the data right away into machine learning models.\n",
    "We need to treat them in a specific way so our model's algorithm can work with them.\n",
    "**Machine learning algorithm work with vectors of numbers**, so when it comes to values represented as a string there is an issue.\n",
    "[`scikit learn`](https://scikit-learn.org/stable/index.html), an industry-standard library used for machine learning, does not accept categorical values represented as strings as well.\n",
    "\n",
    "Imagine we have categorical variables stored as string in the dataset.\n",
    "For understanding what the encoding looks like, here's a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe063ff-0a3c-4cab-a595-ac13f606021d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this code\n",
    "dataframe = pd.DataFrame({'id': range(8), 'amount': [15,85,17,22,56,84,15,48],\n",
    "                          'color':['black','white','black','black','white','white','black','black'],\n",
    "                          })\n",
    "mapping = {'black': 1,\n",
    "          'white':0}\n",
    "# Mapping values\n",
    "mapped_df = dataframe['color'].map(mapping)\n",
    "# Comparison\n",
    "map_dataframe = pd.concat([dataframe, mapped_df], axis = 1)\n",
    "map_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc23927-c33b-48db-a58a-9dc8068a79bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The unique categories of the 'color' column have been converted into numerical form as 1 when the 'black' category is present and 0 otherwise. Of course, encoding categorical features using mapping or replacing can be very tedious and not effective if we have many categorical features and corresponding categories. Fortunately, you can find several encoding methods that serve for different encoding challenges.\n",
    "\n",
    "-------\n",
    "\n",
    "Categorical variables take only a limited number of possible values/categories and must be converted into numerical form. We should perform this conversion over the **training data** and propagate them to the unseen data (for example holdout data). \n",
    "\n",
    "**The main reason for this approach is that we do not know whether the future data will have all the categories present in the training data**. There could also be fewer or more categories. Therefore the encoders must learn patterns from the training data and use those learned categories in both training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42a0ecf3-6178-40c7-ae95-be098fed09cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook we will use the Titanic and the Mushrooms datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ca7a94-a3b5-4710-94b3-8f0e62ab028b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Titanic dataset using columns 'Survived','Sex','Embarked','Cabin' and store it in 'data'\n",
    "data = pd.read_csv('../../../Data/titanic_data.csv', usecols = ['Survived','Sex','Embarked','Cabin'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e2bb96-84d6-40d9-824f-52f4ef0b7857",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For this demonstration, let's capture only the first letter of 'Cabin'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ff8c13-9fa7-4849-be01-59bfcdd0b80b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Capture only first letter of Cabin using .str[0] \n",
    "data['Cabin'] = data['Cabin'].str[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6f034f-d49c-45ee-af14-65dc5fe0c627",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we split our data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e0d744-6486-4276-96e9-0973e36fdc47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separate the DataFrame into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Sex', 'Embarked','Cabin']],  \n",
    "                                                    data['Survived'],  \n",
    "                                                    test_size = 0.3,  \n",
    "                                                    random_state = 42)\n",
    "# Get the shape of training and testing set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9defa283-39ea-4b1d-898d-69a4540e5f22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cardinality of the categorical features\n",
    "\n",
    "Let's explore how many unique values each of the categorical features has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b046f28b-0a69-4add-80dd-bbe07dc3a60c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the unique values of categorical features\n",
    "for column in X_train.columns:\n",
    "    print(column)\n",
    "    print(X_train[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "386add69-41b0-4782-a7bb-a0c00ec2dfaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'll look at the methods for encoding these categories and how these methods handle missing values present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8f99681-498d-4968-8027-1254b23cdc29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1. One-Hot Encoding with Pandas\n",
    "\n",
    "We can use Pandas method\n",
    "[`pd.get_dummies()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "to encode the categorical features.\n",
    "In the real world this encoding method shouldn't be used in ML pipelines (computationally and memory ineffective).\n",
    "However, in the case of some simple data analysis you should be able to use it.\n",
    "We'll look at how it works and what its advantages and limitations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b653152-0e63-444c-acae-00903f0e7d5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the dummy variables of feature 'Sex' using pd.get_dummies() \n",
    "dummies = pd.get_dummies(X_train['Sex'])\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb1a116-abb8-4ba2-9450-ae864f6e6747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be92a64-de2d-45fc-bb59-7b5b504952ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The main advantages are that\n",
    "[`get_dummies()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "returns a DataFrame and preserves feature names for dummy variables.\n",
    "Also, we can use this method even if our data contains missing values. \n",
    "\n",
    "In this example it has created one column for the female category and one column for the male category according to its presence.\n",
    "We can compare the created dummy variables to the original 'Sex' variable using concatenation to see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef948fae-4561-496b-987b-620a26137e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concat the original Series 'Sex' from X_train with created dummy variables Series\n",
    "result = pd.concat([X_train['Sex'], pd.get_dummies(X_train['Sex'])], axis = 1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eceddf8-da47-4dac-825c-d766f3ddeff4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1 >>>> Get dummy variables for the column 'Embarked'\n",
    "#             Concat the original 'Embarked' Series with the created dummy variables Series\n",
    "#             Store it in the variable result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "617dd656-2524-4c28-8213-63fab79dcb8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Encoding into *k*-1 dummy variables**\n",
    "\n",
    "Categorical variables should be encoded by creating *k*-1 binary variables. What does it mean, and why should we use it? \n",
    "\n",
    "Here *k* represents the number of distinct categories. In the feature 'Sex' there are only two categories of sex: male or female, so *k* = 2. We only need to create one binary variable (*k*-1 = 1) and still have all the information contained in the original dataset. In other words, if the value is 0 in all the binary variables, then it must be 1 in the final (not present) binary variable.\n",
    "For example, if we have the variable with 5 categories (*k* = 5), we would create 4 binary variables (*k* - 1 = 4). \n",
    "\n",
    "This approach helps to eliminate the redundancy of the information. \n",
    "\n",
    "To create *k*-1 dummy variables we specify parameter `drop_first = True` to drop the first binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba257095-819e-442e-8b74-14de791f7338",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dummies_2 = pd.get_dummies(X_train['Sex'], drop_first = True)\n",
    "dummies_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48081508-4d4f-46d8-90b1-bc707455b213",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If we create dummy variables for the entire dataset, the prefixes (variables names) will be generated automatically. It doesn't return only 'male', but also the variable's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7031fe-0453-4355-9419-44fab7d46a37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get dummy variable for entire train set\n",
    "dummy_data = pd.get_dummies(X_train, drop_first = True)\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cf5293-9907-4590-b7e8-a3dde326a964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 2 >>>> Get dummy variables for the entire test set and store them in the variable dummy_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a64022-9669-4594-8812-bc879fb2203d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**KEY LEARNING** We can notice that training and testing sets have a different number of dummy variables. In the testing set there is no category _'Cabin T'_. Therefore dummy variables for this category cannot be created. As the training set and the testing set must be of the same shape, `scikit learn's` models won't accept these as inputs. **Our entire modeling pipeline can fail because of this! We did not save the \"state\" of how many dummies should leave this part.** The the pipeline fails, our model does not predict, money is lost, people scream in panic, senior engineers debug over night and protesters burn the cars in the streets! I think you get the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a27cd377-c3c0-412c-bbbb-daf31cc392c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. One-Hot Encoding with Scikit-learn\n",
    "\n",
    "The\n",
    "[`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "module offers the\n",
    "[`OneHotEncoder()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "class which encodes categorical features by creating binary columns for each unique category of variables using a one-hot encoding scheme.\n",
    "The output is not a DataFrame, but a NumPy array. You can find the documentation of\n",
    "[`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "----\n",
    "Firstly we need to create the encoder object where we can specify a set of parameters.\n",
    "Then we'll fit\n",
    "[`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "to the set `X_train`.\n",
    "There we first have to fill in missing values as\n",
    "[`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "doesn't except those.\n",
    "Using the `.categories_` attribute we'll find all of the determined categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c2ba4e9-0c0f-4b96-b6f9-49ceb2e28a35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before we start with scikit, don't forget that we need to get rid of the missing values. Let's just replace them with a string \"missing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3234660a-a029-4527-9759-3c65874de88c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.fillna('Missing')\n",
    "X_test = X_test.fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f6a53e-b11e-4917-9942-661123d8b01c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we get to scikit. If you are confused over the word \"sparse\", don't worry. It is just a cool concept of how we can store a matrix in a more memory efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32ac6ab-3117-4c55-9fec-8662b85e933b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "# Set parameter categories = 'auto' to determine the categories automatically from the training set\n",
    "# Set parameter sparse = False to return a dense array \n",
    "# Set parameter handle_unknown = 'error' to raise an error if an unknown categorical feature is present during the transform\n",
    "encoder = OneHotEncoder(categories='auto', sparse=False, handle_unknown='error')\n",
    "\n",
    "#  Fit the encoder \n",
    "encoder.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa84a30-93f7-46e3-8ec6-2ce0db7e1136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can inspect the categories used with the .categories_ attribute\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e696625-4d86-4dc8-b3a8-bb143af0ede1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To transform `X_train` using our encoder, we need to fill in missing values again. Since the output will be a NumPy array, we'll have to convert it to a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37d54df-b6b5-4694-b8ac-9e8f122e103c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform X_train using encoder \n",
    "training_set = encoder.transform(X_train)\n",
    "\n",
    "# Convert X_train to a DataFrame\n",
    "pd.DataFrame(training_set).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0afe0f57-b210-4820-ad6e-ee7599bbc5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, after transforming the data the names of the features are not returned, which is inconvenient for feature exploration. There is a method for retrieving these names called `.get_feature_names()` which we can apply to the columns. Let's repeat the entire process of transforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c8eac4-deb2-4337-aab8-87065f3eb6b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform X_train using one-hot encoding and return feature names\n",
    "training_set = encoder.transform(X_train)\n",
    "training_set = pd.DataFrame(training_set)\n",
    "training_set.columns = encoder.get_feature_names()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dff5ed-9225-4c21-9591-80d9115e5b4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 3 >>>> Transform X_test using one-hot encoding in the same way as we did with X_train and store it in the variable testing_set\n",
    "#             Inspect the first 5 rows to see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f61fe51-b014-462b-8a06-6c3c24588ea8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that after encoding the training set and testing set have the same number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e1589a-8e17-4f94-a686-64a1e9e10b47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Encoding target variable\n",
    "\n",
    "For encoding the target variable stored as a string datatype, we can use\n",
    "[`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "class from the scikit learn module.\n",
    "[`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "normalizes labels to have values between 0 and n_classes-1.\n",
    "You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder).\n",
    "\n",
    "Let's look at the simple example of using this class on dog breeds.\n",
    "Firstly we create a\n",
    "[`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "object and then we fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9566a8b6-4b27-47fe-815c-76ac90d11d01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create LabelEncoder object\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340fb95c-9441-4199-bf2a-7eb7f99e75c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit data using label_encoder\n",
    "label_encoder.fit(['Border Collie','Dachshund','Irish Setter','Papillon','Pug',\n",
    "                   'Pembroke Welsh Corgi','Dachshund','Hokkaido','Pug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8164748-1724-4b67-b17b-673c1d2a9f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After we fitted our data we can access the used categories\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c102b21f-680a-4f46-ad39-53780872c993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "encoded_labels = label_encoder.transform(['Border Collie','Dachshund','Irish Setter','Papillon','Pug',\n",
    "                                          'Pembroke Welsh Corgi','Dachshund','Hokkaido','Pug'])\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5fac905-6fa2-4ae4-ba72-8f582817d53e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instead of two binary values (0 and 1), we now have a sequence of numbers which are not in ascending order. The reason for this is that the numbering is assigned in alphabetical order.\n",
    "\n",
    "-------\n",
    "\n",
    "### TASK\n",
    "Now it's your turn to encode categorical variables in the **Mushrooms classification** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44de51d0-2e6e-4a29-8f88-d204fe1f36c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to create a list of selected features\n",
    "cols_to_use = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "               'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "               'stalk-shape', 'stalk-root', 'stalk-surface-above-ring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606cde08-e006-4c49-beb8-5db9fbd07778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset 'Data/mushrooms.csv' and store it in mushrooms\n",
    "# Specify parameter usecols = cols_to_use\n",
    "mushrooms = pd.read_csv('../../../Data/mushrooms.csv', usecols = cols_to_use)\n",
    "# Get the first 5 rows\n",
    "mushrooms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d6b078-2838-4c87-8ce0-cf9ca7f4507b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the unique values for all of the features in mushrooms that will be encoded\n",
    "for column in mushrooms.columns:\n",
    "    print(column)\n",
    "    print(mushrooms[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d0aa4e-1fb4-4a9a-a109-8c610fdda3e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You should see that one of the unique values there is '?' in the column 'stalk-root'. Replace this incorrectly stored value with 'Missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81495a51-cfe1-4c9e-8293-569357fcd199",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use .replace() method to replace '?' with 'Missing'\n",
    "mushrooms['stalk-root'] = mushrooms['stalk-root'].replace('?','Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2ce75c-8dce-4e89-b7ea-65e59a0d0abf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split mushrooms into training and testing set\n",
    "# Set test_size = 0.3\n",
    "# Set random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(mushrooms[['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "                                                               'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "                                                               'stalk-shape', 'stalk-root', 'stalk-surface-above-ring']], \n",
    "                                                    mushrooms['class'], \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888a43b4-d434-4417-beea-b4f11152d671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 4 >>>> Create a OneHotEncoder object where the categories will be automatically determined\n",
    "# The result will be a dense array\n",
    "# If an unknown categorical feature will be present during transform it will raise 'error'\n",
    "# Store it in the variable encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a878b61-71b3-412f-a725-c788f30a4581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 5 >>>> Fit X_train using encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b4f3b2-4eb0-4153-bc1b-ed26fcd34aca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 6 >>>> Get the used categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61673fc8-19f8-408e-b61a-24d2c97b969f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 7 >>>> Transform X_train and convert it to a Pandas DataFrame\n",
    "# You can assign it to X_train\n",
    "# Get the feature names and inspect the changes after transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf41bd0-def8-4da3-84d7-1b40f3e0e6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 8 >>>> Transform X_test and convert it to a Pandas DataFrame\n",
    "# You can assign it to X_test\n",
    "# Get the feature names and inspect the changes after transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a03e843-c117-40bf-8664-1e11f7205355",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our target feature 'class' also needs to be encoded. To do so, use [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c516bbb8-2bd4-441a-860e-c23bceee5f14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 9 >>>> Create LabelEncoder object and store it in variable labels_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed532d4e-74ce-4753-81fd-a697bab1ba23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 10 >>>> Fit y_train using labels_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64fe4bb-d0f6-4f8d-b7ad-6ed10af6bea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the used categories\n",
    "labels_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc287a82-1b11-4090-b021-3f251dc926af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 11 >>>> Transform the y_train data and assign to y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ae4b5d-c926-43ac-9d23-5c305738ec1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606c3ef7-d587-4d19-8d6e-c819e2c51fac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 12 >>>> Fit and transform y_test data in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef428f9-eef4-407a-b5cc-7893e8d2b54a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Appendix\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_One_Hot_Encoding_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
