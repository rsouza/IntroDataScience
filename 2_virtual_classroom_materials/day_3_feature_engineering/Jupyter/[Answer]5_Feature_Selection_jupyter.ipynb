{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f86df5-b1ff-45d7-a408-170322870b53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 0. Loading libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8936af29-fab8-40e6-8147-75661c1dd508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for this notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import VarianceThreshold to removes all low-variance features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72af77a-945c-4fdb-a9ff-4be9144fab15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "An **important** thing to remember is that we should perform feature selection in conjunction with the model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a4da15-9f97-4948-893b-c878adc270b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Constant Features\n",
    "Constant features do not provide any information useful for further analysis or predicting the target variable. These features provide only a single value for all of the observations in the dataset. Therefore, we can remove them from the dataset.\n",
    "\n",
    "We will be working with the subset of Santander Bank dataset \\\\(^{1}\\\\) (30 000 rows), which contain anonymized features to predict customer satisfaction regarding their experience with the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "743edefc-6c73-4f34-bee9-42323a53a4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the subset dataset called 'subset_santander.csv' and store it to variable data\n",
    "data = pd.read_csv('../../../Data/subset_santander.csv')\n",
    "\n",
    "# Print the shape of the dataframe and get the first 10 rows\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901fa60e-1bcd-4386-aafb-5bc931fb5ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Feature selection should be performed only on our training data to avoid overfitting. Let's split our dataset and drop our target feature 'TARGET'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba6219e-defb-42bb-9ae9-e907a30be090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(labels = ['TARGET'], axis = 1),\n",
    "                                                    data['TARGET'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Get the shape of training and testing set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2606ffd2-3513-43da-a354-9220367f646b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 370 features in our dataset. We can now look at whether there are some constant features in `X_train` set using the `.var()` method which computes the variance along the columns. Within this function we can specify argument `ddof = 1`. For more information see the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.var.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea7bd89-7159-47b0-8462-a639970594e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the features that have the variance equal to zero\n",
    "# Optional: Specify ddof = 1 within the `.var()` function \n",
    "\n",
    "our_constant_features = X_train.loc[:, X_train.var(ddof = 1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bfd99f3-cf23-4893-aed6-fa50c66df721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print our_constant_features\n",
    "our_constant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49fa2f3-6664-4945-9db7-3f9774e6a5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 64 features with zero variance, which will be removed from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55e7b80-2944-4787-b6bc-ee87d98a4e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove constant features from X_train, do not forget specify argument inplace = True\n",
    "X_train.drop(labels= our_constant_features, axis=1, inplace=True)\n",
    "# Remove constant features from X_test, do not forget specify argument inplace = True\n",
    "X_test.drop(labels= our_constant_features, axis=1, inplace=True)\n",
    "\n",
    "# Get the shape after removing constant features\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a984b1-b0d6-4454-92a0-61dafd5889b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Quasi-constant Features\n",
    "\n",
    "Quasi-constant features have very low variance (close to 0) and contain little information, which is not useful for us. These approximately constant features won't help the ML model's performance, therefore we should consider removing them. \n",
    "\n",
    "We could filter quasi-constant features with Pandas in a similar way as we did with constant features, with one difference - we would set a specific threshold. Nevertheless, now we'll leave Pandas behind and rather use scikit learn which offers a more convenient way to find quasi-constant features. \n",
    "\n",
    "In the `sklearn.feature_selection module` we can find a feature selector called `VarianceThreshold()`, which finds all features with low variance (based on a specified threshold) and removes them. You can find more information about this selector [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html).\n",
    "\n",
    "As a first step, we define our selector for quasi-constant features with a threshold of 0.01. In other words, this is the minimum value of the variance we want to have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61b8990-9c28-4f01-9371-adf6a9b39255",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define VarianceThreshold() object and specify parameter threshold = 0.01\n",
    "our_selector = VarianceThreshold(threshold = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6eec135-ce60-47bf-9fa3-452e5c6d253b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we fit `our_selector` with the `X_train` data to find quasi-constant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c91348-3597-43ac-8bd7-1e4eb8d010b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit X_train with our_selector\n",
    "our_selector.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b310b600-4031-4dab-bc3e-d1e70825d747",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using `get_support()` method, we can get all of the features we want to keep along with their names. This _mask_ we will use later to assign names to columns.\n",
    "\n",
    "*Note: You might wonder why we are saving the feature names in the `features_to_keep` variable. Scikit learn will always save the necessary state inside of the fitted transformer. However, in this example we only do this for our convience, so that we can later on go back from the nameless Numpy array to a nice dataframe with all the column names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ef22b5-7993-4a0e-8dd3-ac9475c7534d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the mask of features we want to keep in the dataset\n",
    "features_to_keep = X_train.columns[our_selector.get_support()]\n",
    "\n",
    "# Print the length\n",
    "print('The number of features that will be kept: {}'.format(len(features_to_keep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d515104-9323-42fe-937b-d4115666130e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The next step is to transform `X_train` and `X_test` using `our_selector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d65d4a-a461-4c41-966d-67d8cc3b6bbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform X_train and X_test = in this step, the quasi-constant featues will be finally removed\n",
    "X_train = our_selector.transform(X_train)\n",
    "X_test = our_selector.transform(X_test)\n",
    "\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d13ec56-fca7-4c86-bea9-06af9d242a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After the trnsformation `X_train` is a `numpy.ndarray` object and needs to be transformed into a Pandas DataFrame again. Here, we use our created `features_to_keep` variable to assign column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782641de-f52e-4ce8-b327-64c04200eb49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3541b1a5-def1-48b8-b29f-036b921ed091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train to a Pandas DataFrame\n",
    "X_train= pd.DataFrame(X_train)\n",
    "# Using the '.columns' attribute assign column names\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "# Convert X_test to a Pandas DataFrame\n",
    "X_test= pd.DataFrame(X_test)\n",
    "# Using the '.columns' attribute assign column names\n",
    "X_test.columns = features_to_keep\n",
    "\n",
    "# Get the first 5 rows of X_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9ab1fd-0993-45a6-8a6e-6ecf957d03d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Duplicated Features (READ-ONLY)\n",
    "Duplicated features are totally redundant features, thus not providing any useful or new information for improving the model's performance.\n",
    "\n",
    "To better understand how duplicated features can be treated using Pandas we create new DataFrame. We've already seen the `duplicated()` function which returns a boolean Series denoting duplicate rows. To identify duplicated features, we have to first transpose our data frame, in other words, we swap the rows and columns. More information [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html).\n",
    "\n",
    "You might wonder again, why are we not using the `scikit-learn`? The reason is that **duplicated features should be already addressed within data integration and preprocessing**. You might remember that these are the earliest stages. The reason is that duplicated values usually occur when we are merging data from various sources. It was not a priority for scikit developers to implement a specific transformer for this.\n",
    "\n",
    "We are doing an ugly operation of swapping rows and columns to make use of Pandas functionality and make this operation as easy as possible. Yes, we are only advising you to do this with a small dataset. If you have a *big* dataset, for example counted in TBs, you should not and most likely will not be able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e545ce9d-729e-4421-b6ad-246873e8a7d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to create new DataFrame\n",
    "our_data  =[(1, 'Colin Trevorrow', 124, 150 ,'Colin Trevorrow', 150, 'Jurassic World'),\n",
    "           (2, 'George Miller', 120, 55, 'George Miller', 55, 'Mad Max: Fury Road'),\n",
    "           (3, 'Robert Schwentke', 119, 112, 'Robert Schwentke', 112, 'Insurgent'),\n",
    "           (4, 'J.J. Abrams', 136, 220,'J.J. Abrams', 220, 'Star Wars: The Force Awakens'),\n",
    "           (5, 'James Wan', 137, 154, 'James Wan', 154, 'Furious 7'),\n",
    "           (6, 'Bruce Brown', 95, 25, 'Bruce Brown', 25, 'The Endless Summer'),\n",
    "           (7, 'Woody Allen', 80, 15, 'Woody Allen', 15, 'What`s Up, Tiger Lily?'),\n",
    "           (8, 'James Cameron', 162, 180, 'James Cameron', 180, 'Avatar'),\n",
    "           (9, 'Carl Tibbetts', 74, 44, 'Carl Tibbetts', 44, 'Black Mirror: White Christmas'),\n",
    "           (10, 'Harold P. Warren', 74, 8, 'Harold P. Warren', 8, 'Manos: The Hands of Fate')]\n",
    "\n",
    "movies = pd.DataFrame(our_data, columns= ['id', 'director', 'runtime','total_votes', 'name', 'number_of_votes', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054ca63b-e3f5-4d85-bff1-f6d0caf333ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print movies DataFrame\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35eafcb8-5c21-4ef6-9e75-462c667a0103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the shape of movies\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff361a1-446e-41ff-8684-062e0f1259f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, the movies DataFrame contains 10 rows and 7 features. Now we use the `.transpose()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be2b7d3-966b-4fd1-841e-0bfaf2860a88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transpose movies and store it to variable movies_transpose\n",
    "movies_transpose = movies.transpose()\n",
    "movies_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8769fa-6856-4bab-adb6-1da5cd45ce33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the shape of movies_transpose\n",
    "movies_transpose.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094f30e0-76c5-40b3-8c8f-cc32c38220ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After transposing, there are 7 rows (features) and 10 columns in `movies_transpose`.\n",
    "\n",
    "Now we apply chained `duplicated().sum()` function on `movies_transpose` that give us the total number of duplicated rows (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36af307-b9e6-42fc-83e8-4935f272e346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the total number of duplicated rows (features)\n",
    "movies_transpose.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be95eaeb-6c1e-4498-bc6c-0e73fd39423f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 2 duplicated rows (features), containing the same observations. We can drop duplicated rows using `.drop_duplicates()`. By setting `keep = 'first'` parameter, we determine which duplicated row we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e88481b-2beb-4ff2-a8f9-85e780f79d55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates and store the result in the variable unique_features\n",
    "unique_features = movies_transpose.drop_duplicates(keep = 'first').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8f175f-3cbb-4ac0-9a30-08d45c03b853",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get duplicated features and store the result in the variable duplicated_feature\n",
    "duplicated_features = [column for column in movies.columns if column not in unique_features]\n",
    "duplicated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9de650-0758-4248-a2c0-6da404f91033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the duplicated features from the original DataFrame\n",
    "movies.drop(labels = duplicated_features, axis = 1, inplace = True)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4955046-4d49-479d-87fe-8083bd87c86d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "However, this approach is not computationally and memory-efficient if you have a really large DataFrame with thousands of rows. As `scikit learn` does not offer a method to handle duplicated features, we need to create some function for this purpose. Then we drop duplicated features using Pandas' `.drop()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c769cb-3e5b-4037-b628-3ff6d72f6be9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty list for duplicated features\n",
    "features_duplicates = []\n",
    "\n",
    "# Create a for loop for iterating over the range of columns from the X_train set\n",
    "for col in range(len(X_train.columns)):\n",
    "    column_1 = X_train.columns[col]\n",
    "    # Find duplicated features by comparing columns using .equals\n",
    "    for column_2 in X_train.columns[col + 1:]:\n",
    "        if X_train[column_1].equals(X_train[column_2]):\n",
    "            features_duplicates.append(column_2)\n",
    "            \n",
    "len(features_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d892500-376a-435f-a703-65943dd60024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicated features from X_train and X_test\n",
    "X_train.drop(labels = features_duplicates, axis = 1, inplace = True)\n",
    "X_test.drop( labels = features_duplicates, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe4970f-4264-4995-acfd-dea15c2e3c04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9848d481-03d8-4749-b5e9-b167c3ccb47b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`scikit learn` module offers many methods such as selecting features based on their importance but we will not go there. You can find these methods in the [documentation](https://scikit-learn.org/stable/modules/feature_selection.html). Now we'll look at the correlation between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfe4a1a-bff0-43de-96f2-b2232189be69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Correlation\n",
    "Features with high correlation have almost the same effect on the target feature. We can visualize relationships between features using `.corr()` method to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827f8b30-13d4-4c30-8e28-c8b4bb0438b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the correlation among the columns and store it in variable correlation_matrix\n",
    "correlation_matrix = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b69fde-ed56-4dc9-bdda-aaf69d861d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the correlation matrix \n",
    "plt.figure(figsize=(11,11))\n",
    "sns.heatmap(correlation_matrix, cmap = 'Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61691573-bb45-4269-9d64-38c34c527665",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'll find the highly correlated features using a function based on correlation coefficients above the threshold of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb763904-4c6d-4e81-8903-afc365bcd235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    # Create set for correlated columns\n",
    "    corelated_cols = set()  \n",
    "    # Compute correlation \n",
    "    corr_matrix = dataset.corr()\n",
    "    for c in range(len(corr_matrix.columns)):\n",
    "        for j in range(c):\n",
    "            # Take absolute correlation coefficient value \n",
    "            # If abs values are above threshold ...\n",
    "            if abs(corr_matrix.iloc[c, j]) > threshold: \n",
    "                # ... Get name of column\n",
    "                colname = corr_matrix.columns[c]\n",
    "                corelated_cols.add(colname)\n",
    "    return corelated_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24897af-5eb7-421f-af6e-56c5a769f96b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use correlation function on X_train with threshold 0.8\n",
    "corr_features_to_drop = correlation(X_train, 0.8)\n",
    "len(set(corr_features_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43fa684-a6d4-4a6a-b111-dd100e50c427",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop correlated features from X_train and X_test\n",
    "X_train.drop(labels = corr_features_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(labels = corr_features_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46f6927-e28c-4395-aaaf-37fcfa9ff231",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20cdbd6-7f32-47f6-9963-28ab5d5f64fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## TASK\n",
    "\n",
    "You will be using an altered dataset containing variants of the Portuguese 'Vinho Verde' wine \\\\(^{2}\\\\). The features provide information about wine samples recorded based on physicochemical tests. There is also the target feature that denotes the quality score of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1352d0-7134-4232-9eb3-efc9cbf4760b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset 'wine_quality.csv' and store it to variable wine\n",
    "wine = pd.read_csv('../../../Data/quality_of_wine.csv', sep = ',', )\n",
    "# Get the first 10 rows\n",
    "wine.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb78246-8652-46d1-9fe6-9a0e41bed846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the dataframe's datatypes\n",
    "wine.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf51832c-3635-4032-8384-5e97f258a3dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Several numerical features are stored as float or integer, and one feature is stored as a string in our dataset. \n",
    "\n",
    "These numerical variables can be used to predict the quality of the wine samples. So the **'quality'** column is our **target feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785c80f2-6e4e-4894-815a-35862836a918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separate the dataset 'wine' into training and testing set\n",
    "# Store it in variables: X_training, X_testing, y_training, y_testing\n",
    "# Drop the target feature 'quality'\n",
    "# Set test_size = 0.3 and random_state = 42\n",
    "\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(wine.drop(labels = ['quality'], axis=1), \n",
    "                                                                wine['quality'],\n",
    "                                                                test_size = 0.3,\n",
    "                                                                random_state = 42)\n",
    "\n",
    "# Get the shape of training and testing set\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb214a3-e6ea-49b8-962d-d8202b07fcca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we already know there is one non-numerical variable ('type'). Let's look at the unique values of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39458d5-66de-4685-8b52-dea2cb19a508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print unique values of 'type' column in X_training and X_testing sets\n",
    "print(X_training['type'].unique())\n",
    "print(X_testing['type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93ff1b3-3517-41d3-9a8c-05d36be2300a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The datasets can also contain this type of constant feature stored as a string and have only one unique value/category. As this variable is not really helpful, we will drop it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e96d52-f377-4279-adc8-9157b41af01f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK >>>> Remove the constant feature from X_training using '.drop()'. Do not forget to specify the argument inplace = True\n",
    "X_training.drop('type', axis = 1, inplace = True)\n",
    "\n",
    "# TASK >>>> Remove the constant feature from X_testing using '.drop()'. Do not forget to specify the argument inplace = True\n",
    "X_testing.drop('type', axis = 1, inplace = True)\n",
    "\n",
    "# Get the shape of X_training and X_testing sets\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d62ee9-1ec1-41f2-8ef8-150b025c311d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we want to select only those features that have a variance above the threshold = 0.01. Again, we will find quasi-constant features using scikit learn's `VarianceThreshold` as we did in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a64e8f-c9d1-42a4-aae0-6e823d8305f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK >>>> Define a VarianceThreshold() object, specify the parameter threshold = 0.01 and store it in variable 'selector'\n",
    "selector = VarianceThreshold(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446481aa-4874-484e-bd85-fea4369dd47a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK >>>> Fit X_training with 'selector'\n",
    "selector.fit(X_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1946d745-e27c-4a02-8ad9-78efdebbb478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a mask of those features we want to keep in the dataset and store it in the variable 'features_we_keep'\n",
    "features_we_keep = X_training.columns[selector.get_support()]\n",
    "# Print the length of the variable features_we_keep\n",
    "print('The number of features that will be kept: {}'.format(len(features_we_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cfcafb3-18c5-40f2-92d4-a0836005c3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the quasi-constant features that we are meant to drop using a for loop \n",
    "for column in X_training.columns:\n",
    "    if column not in features_we_keep:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f609ad-4054-47c4-baec-fccccabf9ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK >>>> Transform X_training \n",
    "\n",
    "X_training = selector.transform(X_training)\n",
    "\n",
    "# TASK >>>> Transform X_testing\n",
    "X_testing = selector.transform(X_testing)\n",
    "\n",
    "# Get the shape of X_training and X_testing\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64df65ea-976a-4ec0-8d33-1c1d56d88828",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_training to Pandas DataFrame\n",
    "X_training = pd.DataFrame(X_training)\n",
    "# Using the '.columns' attribute assign column names\n",
    "X_training.columns = features_we_keep\n",
    "\n",
    "# Convert X_testing to Pandas DataFrame\n",
    "X_testing = pd.DataFrame(X_training)\n",
    "# Using the '.columns' attribute assign column names\n",
    "X_testing.columns = features_we_keep\n",
    "\n",
    "# Get the first 10 rows of X_train\n",
    "X_training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5cba6fb-246b-42c5-800b-8b63aeae69fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find whether our dataset contains duplicated features. You can copy-paste the `for` loop we've already used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6cf98e-345b-448d-820e-9a8948ea4626",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find duplicated features in X_training set\n",
    "features_duplicates = []\n",
    "for col in range(len(X_training.columns)):\n",
    "     \n",
    "    column_1 = X_training.columns[col]\n",
    "    \n",
    "    for column_2 in X_training.columns[col + 1:]:\n",
    "        if X_training[column_1].equals(X_training[column_2]):\n",
    "            features_duplicates.append(column_2)\n",
    "            \n",
    "len(features_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4954e82-efbe-4b66-8c1f-0f3d29f92faa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the features names\n",
    "features_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37439022-acad-46ca-b63b-394873a91efe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK >>>> Drop these duplicated features from X_training and X_testing\n",
    "\n",
    "X_training.drop(labels = features_duplicates, axis = 1, inplace = True)\n",
    "X_testing.drop(labels = features_duplicates, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00e3bce-6dc5-4ffc-94c1-bef4ecd0d4a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the shape of X_training and X_testing\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911c51c1-08a1-4b46-828c-48d07c472383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Apendix\n",
    "\n",
    "Data sources:\n",
    "\n",
    "\\\\(^{1}\\\\) Santander dataset: https://www.kaggle.com/c/santander-customer-satisfaction/data\n",
    "\n",
    "\\\\(^{2}\\\\) Wine quality dataset: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "[Answer]5_Feature_Selection_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
