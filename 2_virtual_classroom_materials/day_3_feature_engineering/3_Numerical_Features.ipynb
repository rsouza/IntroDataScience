{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library \n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import MinMaxScaler to scale the features\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load avocado dataset and store it to variable dataframe\n",
    "dataframe = pd.read_csv('Data/avocado.csv')\n",
    "# Get the first 5 rows\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of dataframe\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numerical variables to create DataFrame data\n",
    "data = dataframe.select_dtypes(include = ['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Variable scale / Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print descriptive statistics of these variables to see variable's magnitudes\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our variables have different magnitudes/scales. The minimum and maximum values of the variables are different. For example, the minimum value and maximum value of average price for avocado are 0.44 and 3.25, respectively. For small bags of avocados sold, the minimum and maximum values are 0 and 5.719097e+06, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range of numerical variables\n",
    "for col in['AveragePrice', 'Total Volume', 'Small Hass Avocado','Large Hass Avocado', 'Extra Large Hass Avocado', 'Total Bags',\n",
    "            'Small Bags', 'Large Bags', 'XLarge Bags']:\n",
    "    print(col, 'range is: ', data[col].max() - data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranges of our variables are different. \n",
    "\n",
    "# 2. Feature Scaling\n",
    "\n",
    "Models such as logistic regression, linear regression, or other models that involve a matrix are very sensitive to different scales of input variables. If we use such data for model fitting, the result might end up creating a bias. Therefore feature scaling techniques are used before model fitting.\n",
    "\n",
    "As you can guess, feature scaling techniques change the scale of the variables. There are several ways how you can scale your features. In this notebook, we'll demonstrate **MinMaxScaling** technique that scales variables to their minimum and maximum values. `scikit learn` offers `MinMaxScaler` class for this purpose. You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
    "\n",
    "The formula for min-max scaling is: \n",
    "\n",
    "**X_std = (X - X.min(axis = 0)) / (X.max(axis = 0) - X.min(axis = 0))**\n",
    "\n",
    "**X_scaled = X_std * (max - min) + min**\n",
    "\n",
    "- our Scaler subtracts the minimum value from all observations in our dataset and divide it by the range of values\n",
    "- it will transform each feature individually between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split our dataset to training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Total Volume', 'Small Hass Avocado','Large Hass Avocado', \n",
    "                                                          'Extra Large Hass Avocado', 'Total Bags', \n",
    "                                                          'Small Bags', 'Large Bags', 'XLarge Bags']],\n",
    "                                                    data['AveragePrice'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MinMaxScaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X_train data with scaler: this computes and saves the minimum and maximum values \n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the maximum values using .data_max attribute\n",
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the minimum values using .data_min attribute\n",
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and X_test with scaler and store it in variables X_train_scaled and X_test_scaled\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the scaled training dataset\n",
    "print('Mean: ', X_train_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_train_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_train_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the scaled testing dataset\n",
    "print('Mean: ', X_test_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_test_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_test_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_test_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this rescaling, all of the features have the range between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "Imagine you've normalized the data using MinMaxScaler and delivered your work to the Senior Data scientist. He/she proposed you to scale the data using different scaling technique. The technique should transform data such that its distribution will have a mean value 0 and standard deviation of 1. Find the right method [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Import selected Scaling class\n",
    "\n",
    "# TASK >>>> Create scaler object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X_train using scaler_technique\n",
    "scaler_technique.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scaled values\n",
    "scaler_technique.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using scaler_technique and store it in variable X_training_scaled\n",
    "X_training_scaled = scaler_technique.transform(X_train)\n",
    "# Print X_training_scaled\n",
    "X_training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the scaling also for X_test and store it in variable X_testing_scaled\n",
    "X_testing_scaled = scaler_technique.transform(X_test)\n",
    "# Print X_testing_scaled\n",
    "X_testing_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean and standard deviations of X_training_scaled\n",
    "print('Mean: ', X_training_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_training_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Data source: \n",
    "\n",
    "Avocado dataset: https://www.kaggle.com/neuromusic/avocado-prices\n",
    "\n",
    "Data license: Database: Open Database\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
