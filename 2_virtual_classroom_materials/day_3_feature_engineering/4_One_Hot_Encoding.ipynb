{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and numpy libraries\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import OneHotEncoder for one hot encoding \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Import LabelEncoder for target feature encoding\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should we use Encoding ? \n",
    "\n",
    "As we already know, we can't throw the data right away into machine learning models. We need to treat them in a specific way, so our model's algorithm can work with them. **Machine learning algorithm work with vectors of numbers**, so when it comes to values represented as a string, there is an issue. `scikit learn`, an industry-standard library using for machine learning, does not accept categorical values represented as strings as well. \n",
    "\n",
    "Imagine we have categorical variables stored as string in the dataset. For understanding how the encoding looks like, here's a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "dataframe = pd.DataFrame({'id': range(8), 'amount': [15,85,17,22,56,84,15,48],\n",
    "                          'color':['black','white','black','black','white','white','black','black'],\n",
    "                          })\n",
    "mapping = {'black': 1,\n",
    "          'white':0}\n",
    "# Mapping values\n",
    "mapped_df = dataframe['color'].map(mapping)\n",
    "# Comparison\n",
    "map_dataframe = pd.concat([dataframe, mapped_df], axis = 1)\n",
    "map_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique categories of 'color' column have been converted into numerical form as 1 when the 'black' category is present and 0 otherwise. Of course, encoding categorical features using mapping or replacing can be very tedious and not effective if we have many categorical features and corresponding categories. Fortunately, you can find several encoding methods that serve for different encoding challenges. Let's move on...\n",
    "\n",
    "-------\n",
    "\n",
    "Categorical variables take only limited numbers of possible values/categories and must be converted into a numerical form. We should perform this converting over **the training data** and propagate them to the unseen data (for example holdout data). \n",
    "\n",
    "**This approach's main reason is that we do not know whether the future data will have all the categories present in the training data**. There could also be fewer or more categories. Therefore the encoders must learn patterns from the training data and use those learned categories in both training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use Titanic and Mushrooms datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset using columns 'Survived','Sex','Embarked','Cabin' and store it in 'data'\n",
    "data = pd.read_csv('Data/titanic_data.csv', usecols = ['Survived','Sex','Embarked','Cabin'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for this demonstration, let's capture only the first letter of Cabin because there are many categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture only first letter of Cabin using .str[0] \n",
    "data['Cabin'] = data['Cabin'].str[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DataFrame into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Sex', 'Embarked','Cabin']],  \n",
    "                                                    data['Survived'],  \n",
    "                                                    test_size = 0.3,  \n",
    "                                                    random_state = 42)\n",
    "# Get the shape of training and testing set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardinality the of categorical features\n",
    "\n",
    "Let's explore how many unique values has each of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values of categorical features\n",
    "for column in X_train.columns:\n",
    "    print(column)\n",
    "    print(X_train[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the methods for encoding these categories and how these methods handle missing values present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. One-Hot Encoding with Pandas\n",
    "\n",
    "We can use Pandas method `pd.get_dummies()` to encode the categorical features. In the real world, this encoding method shouldn't be used in ML pipelines (computationally and memory ineffective), however in case of some simple data analysis, you should be able to use it. We'll look at how it works and what are the advantages and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy variables of feature 'Sex' using pd.get_dummies() \n",
    "dummies = pd.get_dummies(X_train['Sex'])\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantages are that `get_dummies()` returns a DataFrame and preserved feature names for dummy variables. Also, we can use this method even if our data contain missing values. \n",
    "\n",
    "Here it was created 1 column for the female category and 1 column for the male category according to its presence. We can compare created dummy variables to the original 'Sex' variable using concatenation to see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the original Series 'Sex' from X_train with created dummy variables Series\n",
    "result = pd.concat([X_train['Sex'], pd.get_dummies(X_train['Sex'])], axis = 1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 >>>> Get dummy variables for column 'Embarked'\n",
    "#             Concat the original 'Embarked' Series with created dummy variables Series\n",
    "#             Store it in variable result_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding into *k*-1 dummy variables**\n",
    "\n",
    "Categorical variables should be encoded by creating *k*-1 binary variables. What does it mean, and why should we use it? \n",
    "\n",
    "Here *k* represents the number of distinct categories. In the feature 'Sex' there are 2 categories of gender: male or female, so *k* = 2. We only need to create 1 binary variable (*k*-1 = 1) and still have all the information we need. In other words, if the value is 0 in all the binary variables, then it must be 1 in the final (not present) binary variable.\n",
    "For example, if we have the variable with 5 categories (*k* = 5), we would create 4 binary variables (*k* - 1 = 4). \n",
    "\n",
    "This approach helps to eliminate the redundancy of the information. \n",
    "\n",
    "To create *k*-1 dummy variables we specify parameter `drop_first = True` to drop first binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_2 = pd.get_dummies(X_train['Sex'], drop_first = True)\n",
    "dummies_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create dummy variables for the entire dataset, the prefixes (variables names) will be generated automatically. It doesn't return only 'male', but also the variable's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy variable for entire train set\n",
    "dummy_data = pd.get_dummies(X_train, drop_first = True)\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>> Get dummy variable for entire test set and store it in variable dummy_data_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KEY LEARNING** We can notice that training and testing sets have a different number of dummy variables. In the testing set, there is no category Cabin T. Therefore dummy variables for this category can not be created. As the training set and the testing set must be of the same shape, `scikit learn's` models won't accept these as inputs. **Our entire modeling pipeline can fail because of this! We did not save the \"state\" of how many dummies should leave this part.** The pipeline fails, model does not predict, money are lost, people scream in panic, senior engineers debug over night and protesters burn the cars in the streets. I think you get the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding with Scikit-learn\n",
    "\n",
    "`sklearn.preprocessing` module offers `OneHotEncoder()` class that encodes categorical features by creating binary columns for each unique category of variables using a one-hot encoding scheme. The output is not a DataFrame, but a NumPy array. You can find the documentation of OneHotEncoder [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "----\n",
    "Firstly we need to create the encoder object, where we can specify a set of parameters.\n",
    "Then we'll fit OneHotEncoder to X_train set, where we have to fill in missing values as OneHotEncoder doesn't except those. Using `.categories_` attribute, we'll find all of the determined categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with scikit, don't forget that we need to get rid of missing values. Let's just replace them with a string \"missing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna('Missing')\n",
    "X_test = X_test.fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to scikit. If you are confused over the word \"sparse\", don't worry. It is just a cool concept of how we can store a matrix in a more memory efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "# Set parameter categories = 'auto' to determine categories automatically from training set\n",
    "# Set parameter sparse = False to return dense array \n",
    "# Set parameter handle_unknown = 'error' to raise an error if an unknown categorical feature is present during transform\n",
    "encoder = OneHotEncoder(categories='auto', sparse=False, handle_unknown='error')\n",
    "\n",
    "#  Fit the encoder \n",
    "encoder.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect the categories used with the .categories_ attribute\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform X_train using our encoder, we need to fill in missing values again. Since the output will be NumPy array, we'll convert it to pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using encoder \n",
    "training_set = encoder.transform(X_train)\n",
    "# Convert X_train to a DataFrame\n",
    "pd.DataFrame(training_set).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after transforming the data, the names of the features are not returned, which is inconvenient for feature exploration. There is the method for retrieving these names `.get_feature_names()` that we apply on the columns. Let's repeat the entire process of transforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using one-hot encoding and return feature names\n",
    "training_set = encoder.transform(X_train)\n",
    "training_set = pd.DataFrame(training_set)\n",
    "training_set.columns = encoder.get_feature_names()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>> Transform X_test using one-hot encoding in the same way as we did with X_train and store it in variable testing_set\n",
    "#             Inspect the first 5 rows to see the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after encoding, the training set and testing set have the same number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoding target variable\n",
    "\n",
    "For encoding the target variable stored as a string datatype, we can use `LabelEncoder` class from `scikit learn` module. Label Encoder normalizes labels to have values between 0 and n_classes-1. You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder).\n",
    "\n",
    "Let's look at the simple example of using this class on dog breeds. Firstly we create LabelEncoder object, and then we fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LabelEncoder object\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data using label_encoder\n",
    "label_encoder.fit(['Border Collie','Dachshund','Irish Setter','Papillon','Pug',\n",
    "                   'Pembroke Welsh Corgi','Dachshund','Hokkaido','Pug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we fitted our data we can access the used categories\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "encoded_labels = label_encoder.transform(['Border Collie','Dachshund','Irish Setter','Papillon','Pug',\n",
    "                                          'Pembroke Welsh Corgi','Dachshund','Hokkaido','Pug'])\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of two binary values (0 and 1), we now have the sequence of the numbers that have not ascended order. The reason for it is that the numbering is assigned in alphabetical order.\n",
    "\n",
    "-------\n",
    "\n",
    "### TASK\n",
    "Now it's your turn to encode categorical variables in the Mushrooms classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to create a list of selected features\n",
    "cols_to_use = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "               'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "               'stalk-shape', 'stalk-root', 'stalk-surface-above-ring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset 'Data/mushrooms.csv' and store it in mushrooms\n",
    "# Specify parameter usecols = cols_to_use\n",
    "mushrooms = pd.read_csv('Data/mushrooms.csv', usecols = cols_to_use)\n",
    "# Get the first 5 rows\n",
    "mushrooms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values for all of the features in mushrooms that will be encoded\n",
    "for column in mushrooms.columns:\n",
    "    print(column)\n",
    "    print(mushrooms[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that one of the unique values there is '?' in column stalk-root. Replace this incorrectly stored value with 'Missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .replace() method to replace '?' with 'Missing'\n",
    "mushrooms['stalk-root'] = mushrooms['stalk-root'].replace('?','Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mushrooms into training and testing set\n",
    "# Set test_size = 0.3\n",
    "# Set random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(mushrooms[['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "                                                               'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "                                                               'stalk-shape', 'stalk-root', 'stalk-surface-above-ring']], \n",
    "                                                    mushrooms['class'], \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Create OneHotEncoder object where the categories will be automatically determined, the result will be dense array and \n",
    "# if an unknown categorical feature will be present during transform it will raise 'error'\n",
    "# Store it in variable encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit X_train set using encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Get the use categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform X_train set and convert it to pandas DataFrame\n",
    "# You can assign it to X_train\n",
    "# Get the feature names and inspect the changes after transforming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform X_test set and convert it to pandas DataFrame\n",
    "# You can assign it to X_test\n",
    "# Get the feature names and inspect the changes after transforming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target feature 'class' needs to be also encoded. To do so, use LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Create LabelEncoder object and store it in variable labels_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit y_train using labels_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print used categories\n",
    "labels_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform y_train data and assign to y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit and transform also y_test data in the same way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
