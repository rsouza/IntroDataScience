{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc65022-4897-4cb7-b377-d398b2cf8878",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"../data_titanic/train.csv\")\n",
    "train.Pclass = train.Pclass.astype(float) # to avoid DataConversionWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8034942-2c3a-432c-9714-601c47a6e53b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ca8fea-6359-47f0-a2e7-d9727c227bad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brief Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a75b43f5-95e8-4ce4-bfb4-dbbd29206d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "train.describe(include = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860d7860-0b56-4963-9471-8a5b2d146859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "458d5241-0568-48fd-bc0a-d64294ca2b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's work only with the following features for simplicity:   \n",
    "\n",
    "**Categorical**   \n",
    "- Sex\n",
    "- Embarked\n",
    "\n",
    "**Numerical**  \n",
    "- Survived: *our target feature* (0 = No, 1 = Yes)\n",
    "- Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- Age: Age in years\n",
    " \n",
    "More detailed info: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f8111b5-58f5-4e77-a370-3ee7553c978f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's keep only the desired columns\n",
    "train = train[['Sex','Embarked','Pclass', 'Age','Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be2459d6-d73c-49bd-9bf2-15d88f5fa698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eef9536-b454-4eca-a216-22918b426533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "818cce3b-4a1e-428b-8eac-8ddb5356eb70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For simplicity, we drop any row containing missing values. \n",
    "\n",
    "**Note**   \n",
    "If you later want to experiment with composite transformers, comment out this cell and include also missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31a58306-c5f9-4e5d-ac8d-b2acf5b9bc52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.dropna(axis=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f3ab98-1fbd-4498-9d1a-58fb84cc7ef0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "With our current knowledge, we can try to individually implement various transformers from Scikit Learn. Let's not forget to create a holdout set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97cd3eed-cc4d-4b1f-9483-b45b63d6b8da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[['Pclass', 'Age', 'Sex', 'Embarked']],\n",
    "                                                    train['Survived'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7ef250-8a88-4a27-9785-9652446a83a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Numerical Features\n",
    "The only numerical features we have are 'Pclass' and 'Age'.  \n",
    "Let's scale these two features using `MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3151e2d-568d-4b50-9e18-11eba4873e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train[['Pclass', 'Age']])\n",
    "X_train_transformed_numerical = scaler.transform(X_train[['Pclass', 'Age']])\n",
    "X_test_transformed_numerical = scaler.transform(X_test[['Pclass', 'Age']])\n",
    "\n",
    "print(X_train_transformed_numerical.shape)\n",
    "print(X_test_transformed_numerical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3112ab-7aa1-4299-bf0d-fc1fa9ec3bad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Categorical Features\n",
    "The categorical features we have are 'Sex' and 'Embarked'.   \n",
    "We can simply one-hot encode these using `OneHotEncoder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0a456c2-dc9c-4b73-9df0-edab2ced98c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "encoder.fit(X_train[['Sex', 'Embarked']])\n",
    "X_train_transformed_categorical = encoder.transform(X_train[['Sex', 'Embarked']])\n",
    "X_test_transformed_categorical = encoder.transform(X_test[['Sex', 'Embarked']])\n",
    "\n",
    "print(X_train_transformed_categorical.shape)\n",
    "print(X_test_transformed_categorical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "501b3c53-3f45-45b4-af09-171f170e6ba6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## HANDS-ON 1: Baseline Model & Model Evaluation\n",
    "It's time for our first exercise! \n",
    "Before, let's concatenate the transformed numerical and categorical features into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b7ca97e-317d-430a-8c87-f086c4bd7bce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_transformed = np.concatenate((X_train_transformed_numerical,X_train_transformed_categorical), axis = 1)\n",
    "X_test_transformed = np.concatenate((X_test_transformed_numerical,X_test_transformed_categorical), axis = 1)\n",
    "\n",
    "print(X_train_transformed.shape)\n",
    "print(X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd3e1ab6-2fbb-40f7-9fed-a41873082241",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1: Fit sklearn.DummyClassifier to the transformed training set.  \n",
    "# Then, let the model predict for train (X_train_transformed) and holdout set (X_test_transformed).\n",
    "# Store the prediction as y_pred_TRAIN_DUMMY (training set) and as y_pred_HOLDOUT_DUMMY (holdout set).\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_TRAIN_DUMMY = dummy_clf.predict(X_train_transformed)\n",
    "y_pred_HOLDOUT_DUMMY = dummy_clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "634a9d6c-3776-4deb-80ae-76acc53a385f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL TASK 1: Think about a simple heuristic that can be used as a baseline. \n",
    "# One possibility is to use gender and for example predict that every men or every woman has survived.\n",
    "# You can store the result as y_pred_TRAIN_HEURISTIC and as y_pred_HOLDOUT_HEURISTIC.\n",
    "\n",
    "y_pred_TRAIN_HEURISTIC =   np.array([1 if idx==0 else 0 for idx in X_train_transformed[:,3]])\n",
    "y_pred_HOLDOUT_HEURISTIC = np.array([1 if idx==0 else 0 for idx in X_test_transformed[:,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98845b5a-dca0-42bf-b43f-2a4f9622e87e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Great! We have our first prediction! It is time to evaluate how good our model is using the [*sklearn.metrics* module.](   \n",
    "https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ccbb9c6-569d-4764-83fb-0895ff826698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#TASK 2A: Display ACCURACY on TRAIN set.\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_DUMMY))\n",
    "print(metrics.accuracy_score(y_train,y_pred_TRAIN_HEURISTIC))  #Optional Task 1\n",
    "print()\n",
    "\n",
    "#TASK 2B: Display ACCURACY on HOLDOUT set.\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_DUMMY))\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_HEURISTIC))  #Optional Task 1\n",
    "\n",
    "#OPTIONAL TASK 2C: Can you think of a better measure than accuracy based on the domain problem? If yes, use it the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a489954f-f00b-451c-bf93-2c52753d8899",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Great! Now we would also like to see the confusion matrix as it is always a good idea to visually confirm the quality of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9849be95-7e15-4dfe-b9a4-2dd09eb9f10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TASK 3: Display a CONFUSION MATRIX on HOLDOUT set. Hint: do not use plot_confusion_matrix but confusion_matrix only.\n",
    "metrics.confusion_matrix(y_test, y_pred_HOLDOUT_DUMMY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9053c102-37ed-45f6-8e06-e675a2468b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_HOLDOUT_HEURISTIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99d8dd0-2d02-4db9-aa40-ed9c022e4b12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## HANDS-ON 2: Composite Estimators\n",
    "Let's nicely wrap our feature engineering and model fitting into a nice composite estimator. We will be very simplistic and only use two steps. \n",
    "They will not nest into each other at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf93868-31ed-47c2-9ede-73c248c11cfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering wrapped into ColumnTransformer\n",
    "The two feature transformations can be easily wrapped up into a single `ColumnTransformer()` object. This will ensure that our Feature Engineering is a bit **more robust and nicely encapsulated**. Section 6.1.4 [here](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data) showcases the exact application that we intend to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69fd614b-e727-4eab-aac6-ce40e041f5ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 3: Wrap MinMaxScaler and OneHotEncoder into a single ColumnTransformer. \n",
    "# The transformers should be applied to the respective numerical or categorical columns only.\n",
    "# Store the resulting composite as feature_engineering\n",
    "# Hint: Use the argument remainder='passthrough'\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "feature_engineering = ColumnTransformer([('numerical_scaler', preprocessing.MinMaxScaler(),['Pclass', 'Age']),\n",
    "                                         ('ohe', preprocessing.OneHotEncoder(sparse=False), ['Sex', 'Embarked'])\n",
    "                                        ],\n",
    "                                        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd645ab7-db67-41ae-ad22-c213032de07d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predictive Model Wrapped into Pipeline\n",
    "Let's now wrap the feature engineering and the model into a single Pipeline Composite estimator. Here is some pseudocode for this:\n",
    "``` \n",
    "entire_pipeline = feature_engineering -> model  \n",
    "``` \n",
    "\n",
    "Both components are already available. From the step above we can directly reuse the object `feature_engineering`. As model, we just call a new `DummyClassifier`, just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ced75f6-6f86-45e4-bd30-d55a88dc6c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 4: Wrap the feature engineering and the predictive model (dummy) into a single Pipeline composite estimator. \n",
    "# Store the result as entire_pipeline.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "entire_pipeline = Pipeline([('feature_engineering', feature_engineering), ('dummy', DummyClassifier(strategy=\"most_frequent\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b433cf7-e7cf-4734-bf73-e8930b7b90a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK: Uncomment the line and try to train the pipeline.\n",
    "# Notice that we are using untransformed data again (X_train) as the pipeline contains all necessary transformers.\n",
    "\n",
    "entire_pipeline.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c898ef-a310-4f05-9c91-91c4a61bb70d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict for training data\n",
    "y_pred_TRAIN_DUMMY = entire_pipeline.predict(X_train)\n",
    "\n",
    "# Predict for holdout data\n",
    "y_pred_HOLDOUT_DUMMY = entire_pipeline.predict(X_test)\n",
    "\n",
    "# Results should be the same as before\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_DUMMY))\n",
    "\n",
    "# Display accuracy on holdout set.\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_DUMMY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62afb9ee-e044-46e6-8e6e-6cfcb7734e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**OPTIONAL TASK**   \n",
    "The notebook 'nice_pipeline' was made to exemplify some examples of more complex pipelines. Feel free to scroll through it and learn what the process of preparing a complex composite looks like. You can then come back here and try to implement various components. For example, if I would not drop rows with missing values at the beginning of this notebook, constructing a composite would get a bit trickier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a6f19c9-3c90-4913-a026-fc75063201f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## HANDS-ON 3: Tree-based Models & Hyperparameter Tuning\n",
    "Hold your constructed pipeline firmly! The only thing that we need to do now is to replace `DummyClassifier` with a proper learning model.   \n",
    "We can start with a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74bd6d0-7dd1-4950-b3f7-172cdb75a8f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fitting a Learning Model – Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57c1ad1f-d1a5-458d-b05e-95e4fb07ec27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 5: Reuse your composite and instead of a dummy, fit a decision tree with default parameters.\n",
    "# Store the result as dt_pipeline.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_pipeline = Pipeline([('feature_engineering', feature_engineering), ('decision_tree', DecisionTreeClassifier())])\n",
    "\n",
    "# Train the pipeline\n",
    "dt_pipeline.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61a5f2b-72ec-48ad-bf49-bca909d8e73a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 5B: Let the pipeline predict for the training set. \n",
    "# Store the result as y_pred_TRAIN_DT.\n",
    "# Also, display accuracy.\n",
    "\n",
    "y_pred_TRAIN_DT = dt_pipeline.predict(X_train)\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c0403e-f6b7-4aa7-9325-52d03fe0afd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 5C: Let the pipeline predict for the holdout set. \n",
    "# Store the result as y_pred_HOLDOUT_DT.\n",
    "# Also, display accuracy.\n",
    "\n",
    "y_pred_HOLDOUT_DT = dt_pipeline.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b806a4c0-cc5a-4754-bcc9-d90ea0fdad89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Looking at the accuracy on training and holdout set, what can you infer about over model? Will it generalize well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcfc8b59-dcf4-455d-8fd5-57324e89f5b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL TASK 6: Do the same steps with RandomForest with default parameters. \n",
    "# Does the RandomForest display similar results as decision tree? If not, why?\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipeline = Pipeline([('feature_engineering', feature_engineering), ('random_forest', RandomForestClassifier())])\n",
    "\n",
    "# Train the pipeline\n",
    "rf_pipeline.fit(X = X_train, y = y_train)\n",
    "\n",
    "#Predict and show accuracy TRAIN\n",
    "y_pred_TRAIN_RF = rf_pipeline.predict(X_train)\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_RF))\n",
    "\n",
    "#Predict and show accuracy HOLDOUT\n",
    "y_pred_HOLDOUT_RF = rf_pipeline.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dc3b6bb-502c-45eb-aa49-eff547cbc8ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tuning Hyperparameters of our Decision Tree\n",
    "Time to improve the performance of our learning model by finding its optimal set of hyperparameters.  \n",
    "We start by examining **which hyperparameters are available** in our decision tree pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bc2fbf-b324-4c6b-a21e-abf0c13f1d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e6f02d-62d1-4f3a-b04d-5367d21b3948",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We would like to tune `max_depth` and `min_samples_split`.  \n",
    "Notice that to access them, we also need to navigate within the composite and call them as **`decision_tree`**`__max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c39349be-6c59-40e3-8e2c-f4ca902abfea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 7: Define a grid through which we should search. \n",
    "# Tune parameters: max_depth and min_samples_split.\n",
    "# The values which you pick as parameters are up to you. You can think about them intuitively.\n",
    "\n",
    "param_grid = {'decision_tree__max_depth':[3, 4, 5, 6, 7, 8, 9], \n",
    "              'decision_tree__min_samples_split':[ 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92b453c0-7d0b-4938-bf04-8d660af65b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model\n",
    "dt_pipeline\n",
    "\n",
    "# Searching strategy, providing grid\n",
    "tuning = GridSearchCV(dt_pipeline, param_grid)\n",
    "\n",
    "# Train\n",
    "tuning.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b552f0-0664-44cd-bb9f-9cbaba0e8d57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the best parameters\n",
    "tuning.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2496ae0a-c384-4b2a-8999-c6e386a823f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 8: Use the best setting of the two hyperparameters and fit a optimized decision tree. \n",
    "# Hint: Reuse the pipeline and when declaring it, specify the params.\n",
    "# Store it as dt_pipeline_tuned.\n",
    "\n",
    "dt_pipeline_tuned = Pipeline([('feature_engineering', feature_engineering), \n",
    "                              ('decision_tree', DecisionTreeClassifier(max_depth=6, min_samples_split=5))])\n",
    "\n",
    "# Train\n",
    "dt_pipeline_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b398789d-0f21-4a20-ab5e-47e9f45277dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 8B: Display accuracy on the training set of the optimized decision tree.\n",
    "\n",
    "print(metrics.accuracy_score(y_train, dt_pipeline_tuned.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fc1991f-dcb5-4c71-9a96-ea9a8ae8e214",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 8C: Display accuracy on the holdout set of the optimized decision tree.\n",
    "print(metrics.accuracy_score(y_test, dt_pipeline_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a771e4d-4b54-4888-823f-44177bf5dca4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Does the optimized decision tree perform better then the one with default parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a904a3b-3dad-4e7f-862d-9d7603c307f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Optional Advanced TASK: Tuning Random Forest\n",
    "When you are tuning a more complex model, it is good practice to search available literature on which hyperparameters should be tuned. Below I have predefined some. You can play around with the grid, for example expand or narrow it. Keep in mind that as our feature set is extremely limited, its hard for hyperparameter tuning to arrive at something meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc04cafc-30b8-4126-a042-a6be0697a8b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL TASK 9\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a pipeline\n",
    "rf_pipeline = Pipeline([('feature_engineering', feature_engineering), ('random_forest', RandomForestClassifier())])\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid_rf = {\n",
    "    'random_forest__bootstrap': [True, False],\n",
    "    'random_forest__max_depth': [3, 5, 10, 15],\n",
    "    'random_forest__max_features': [2, 3],\n",
    "    'random_forest__min_samples_leaf': [3, 4, 5],\n",
    "    'random_forest__min_samples_split': [5, 8, 10, 12],\n",
    "    'random_forest__n_estimators': [5, 10, 15, 20, 25]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf_pipeline, \n",
    "                           param_grid = param_grid_rf, \n",
    "                           cv = 3, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 2)\n",
    "\n",
    "# Searching strategy, providing grid\n",
    "tuning_rf = GridSearchCV(rf_pipeline, param_grid_rf)\n",
    "\n",
    "# Train\n",
    "tuning_rf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validated score (more robust than holdout set most likely)\n",
    "print(tuning_rf.best_score_)\n",
    "print(tuning_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2693b4f6-544d-4f5f-9de2-1adf2314301e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Optional Advanced TASK: Check Kaggle competitions and join one of them!  \n",
    "https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "949b9801-9521-416d-92d4-bbfe0101d3c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2_sup_learning_workflow_answers",
   "notebookOrigID": 706562957547390,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
