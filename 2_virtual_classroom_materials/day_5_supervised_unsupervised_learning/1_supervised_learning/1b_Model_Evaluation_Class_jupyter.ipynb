{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36721672-0d4f-476b-b101-376cdfda2aff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model Evaluation: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bd5a85-caf0-43b0-9914-cb4a02b77a88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating the performance of classification models is a critical aspect of building effective predictive models. In this notebook we are going to explore various metrics for binary classification tasks:\n",
    "\n",
    "* [**Accuracy**](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_classification)\n",
    "\n",
    "* [**F1 score**](https://en.wikipedia.org/wiki/F-score)\n",
    "\n",
    "* [**ROC AUC**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "* **PR AUC**\n",
    "\n",
    "\n",
    "Which of these metrics is better? When and how should we use them? What are they good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e02fa85c-b013-4f58-92e9-9af9f8f8fdfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy measures the proportion of correctly predicted instances among the total instances in a dataset. In other words, accuracy tells you how well the model's predictions match the actual outcomes.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "When does it make sense to use it?\n",
    "* You should not use accuracy on imbalanced datasets. In situations where the classes are imbalanced, meaning one class has significantly more instances than the others, accuracy can be misleading. A model might achieve high accuracy by simply predicting the majority class all the time.\n",
    "* You can use it when every class is equally important to you and errors are equally costly. In some applications, the cost of making a particular type of error might be much higher than the others. For instance, in medical diagnosis, a false negative (saying a person is healthy when they are not) can be more critical than a false positive. Accuracy doesn't take into account the severity of different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81aabee-035a-48df-a273-a39c874540c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## F1 Score\n",
    "\n",
    "The F1 score is the **harmonic mean of precision and recall**. \n",
    "\n",
    "Precision is the ratio of true positive predictions to the total predicted positives.\n",
    "In the formulas below we use TP for the amount of true positives, TN for true negatives, FP for false positives and FN for false negatives.\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "Recall is the ratio of true positive predictions to the total actual positives.\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "The formula for calculatng the F1 score is:\n",
    "\n",
    "$$\\text{F1} = 2 \\times\\frac{\\text{Precision}\\times\\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
    "\n",
    "When does it make sense to use it?\n",
    "* When the positive class is more important for you.\n",
    "* In scenarios where the costs of different types of errors (false positives and false negatives) are not equal, the F1 score provides a balanced assessment of the trade-off between precision and recall. For instance, in medical diagnoses, a false negative (missing a disease) could be more severe than a false positive (wrongly diagnosing a healthy patient).\n",
    "* In fraud detection or anomaly detection, the class of interest (fraudulent cases or anomalies) is often the minority class. The F1 score is better suited to evaluate the model's performance in correctly identifying these rare occurrences.\n",
    "* In tasks like sentiment analysis, where the classes might not be perfectly balanced, the F1 score helps assess the model's performance when dealing with different levels of sentiment expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1e43a2-f291-4e13-b53d-ef96561f04c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ROC AUC Score\n",
    "\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve and the Area Under the ROC Curve (ROC AUC) are popular evaluation metrics used in binary classification tasks. The ROC AUC score is particularly useful for assessing a model's ability to discriminate between positive and negative classes across different probability thresholds. \n",
    "\n",
    "The ROC AUC score measures the area under the ROC curve. The ROC curve is a graphical representation that illustrates the trade-off between:\n",
    "\n",
    "* true positive rate (TPR, recall)\n",
    "\n",
    "* false positive rate (FPR)\n",
    "\n",
    "$$\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$$\n",
    "\n",
    "as the classification threshold changes.\n",
    "\n",
    "AUC is the probability that the model ranks a random positive example more highly than a random negative example. \n",
    "\n",
    "*More about ROC AUC: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc*\n",
    "\n",
    "When does it make sense to use it?\n",
    "* You should not use it with heavily imbalanced dataset. False positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n",
    "* You should use it when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cabc2154-9cdb-4ff1-9ebb-89e1f2caf924",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PR AUC Score\n",
    "\n",
    "Precision-recall curve combines Positive Predictive Value (PPV) and Recall (TPR) in a single visualization. The higher on y-axis your curve is the better your model performance. The higher the recall, the lower the precision. The PR AUC score measures the area under this curve. It quantifies the balance between precision and recall across various thresholds and provides a single value that ranges between 0 and 1. \n",
    "\n",
    "When does it make sense to use it?\n",
    "* The PR AUC score is especially relevant for imbalanced datasets, where the positive class (minority class) is significantly outnumbered by the negative class. \n",
    "* When the positive class is of particular interest, such as detecting rare diseases, fraud cases, anomalies, or relevant search results, the PR AUC score becomes essential. It emphasizes the model's ability to accurately identify positive instances while keeping false positives low.\n",
    "* When you want to choose the threshold that fits the business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd0f3db0-ef9c-45f1-9f6d-7937805d3b85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Example: Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ec7d5c-2a3a-408b-91d0-6e3fe7093700",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will work with a dataset https://www.kaggle.com/competitions/ieee-fraud-detection/data to detect fraudulent transactions. The dataset was transformed (https://github.com/neptune-ai/blog-binary-classification-metrics), so that it has 43 features, 66000 observations and the fraction of the positive class is 0.09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c5d192-5767-4724-a0fa-ece9fd72022a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fe1f67-be47-46cb-bb28-8030daadd85b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../../Data/fraud_train.csv')\n",
    "test = pd.read_csv('../../../Data/fraud_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bab155e-8abd-4c58-a6e0-6f0d2733a6ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Brief exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f4b56a-8b9a-4145-8ae9-f9d856c2bff0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa531deb-56c1-4f4d-aa21-4817ad521acd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00f60250-352b-48cd-a501-59e465cafb91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will check if our dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3caf923-a342-4268-99a6-c3d7d122ae7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class_counts = train['isFraud'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ef077a-fec3-4e2c-a909-0dd8b3326a40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that our data is **imbalanced**: fraudulent transactions (class = 1) account for one tenth of non-fraudulent transactions (class = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6344987-03c0-48a3-a5c4-03e1307d96f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a8f7d4-c500-4e53-802a-e694b3500164",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc6aa87-5c69-401e-82c6-991e0058aec3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_names = [col for col in train.columns if col not in ['isFraud']]\n",
    "\n",
    "# Split dataset\n",
    "X_train, y_train = train[feature_names], train['isFraud']\n",
    "X_test, y_test = test[feature_names], test['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c01cdfc-266a-4ddb-8dbf-9c69b12be4a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of different hyperparameters\n",
    "parameters = [\n",
    "  {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 10\n",
    "  },\n",
    "  {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100\n",
    "  },\n",
    "  {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 300\n",
    "  },\n",
    "  {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 600\n",
    "  },\n",
    "  {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 1500\n",
    "  },\n",
    "  {\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1500\n",
    "  },\n",
    "  {\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 3000\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd9a521-9272-4c1b-b786-4cbc4ad44a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "id = 0\n",
    "\n",
    "# Create models for different hyperparameters\n",
    "for config in parameters:\n",
    "  model = lightgbm.LGBMClassifier(random_state = 42, learning_rate = config['learning_rate'], n_estimators = config['n_estimators'])\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  roc_auc = roc_auc_score(y_test, y_prob)\n",
    "  pr_auc = average_precision_score(y_test, y_prob)\n",
    "  \n",
    "  # Store results\n",
    "  result = {\n",
    "      'ID': id,\n",
    "      'Learning rate': config['learning_rate'],\n",
    "      'N_estimators': config['n_estimators'],\n",
    "      'Accuracy': accuracy,\n",
    "      'F1 Score': f1,\n",
    "      'ROC AUC': roc_auc,\n",
    "      'PR AUC': pr_auc,\n",
    "  }\n",
    "  results.append(result)\n",
    "  id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd8051a4-a8f1-4562-87c9-47d3011f629d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let us take a look at the results and make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef08c57-da18-46c1-8504-e72e3fa93517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fe1850-840c-4e18-9d97-f0a1dcc69bc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort experiments by accuracy\n",
    "sorted(results, key=lambda x: x.get(\"Accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5162128d-1697-449c-9f6f-7c4dd0886d99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that our models always have high accuracy score (>90%). The worst model has the accuracy of 0.93. But remember that we have an imbalanced dataset. It means that even if all transactions will be classified as non-fraudulent, we will get an accuracy of 0.9. **You should always take an imbalance into consideration when looking at accuracy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990323da-22da-40a6-bcf9-1ebec4b7e4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted(results, key=lambda x: x.get(\"F1 Score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1741d127-9b63-4a65-ac38-3908314ebd91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With the imbalance we have, even the worst model has very high accuracy and the improvements as we go to the end of the table are not as clear on accuracy as they are on F1 score. Hence, between accuracy and F1 Score one should choose **F1 Score** for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eb7ae0e-cb16-4daa-9c71-74a712650751",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As a next step we will compare ROC AUC and PR AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8234c545-94b1-4ffb-84b1-376842aada44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted(results, key=lambda x: x.get(\"ROC AUC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ac6687-5581-4d71-9d67-122a81a2dc3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted(results, key=lambda x: x.get(\"PR AUC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d355dcc6-8cc0-407b-896c-1ff2b7eecbf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ROC AUC and PR AUC both assess the performance of classification models based on prediction scores rather than fixed class assignments. However, they differ in the metrics they focus on. ROC AUC examines the True Positive Rate (TPR) and False Positive Rate (FPR), whereas PR AUC considers the Positive Predictive Value (PPV) and TPR.\n",
    "\n",
    "If your primary concern is the positive class, PR AUC is often a superior choice. It is more sensitive to improvements in the positive class, making it particularly valuable in scenarios with highly imbalanced datasets. For instance, in cases like fraud detection, where the positive class (i.e., instances of fraud) is rare compared to the negative class, PR AUC provides a more informative evaluation of model performance.\n",
    "\n",
    "In our case we can see, that although ROC AUC and PR AUC rank models in the same way, the improvements calculated in **PR AUC** are larger and clearer. We get from 0.69 to 0.87 when at the same time ROC AUC goes from 0.92 to 0.96."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ec56cd-a6aa-40eb-810a-228a6cea7fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we should decide between F1 Score and PR AUC.\n",
    "\n",
    "One significant distinction between the F1 score and PR AUC is that the F1 score operates on predicted classes, while PR AUC relies on predicted scores. Consequently, when using the F1 score, you must select a threshold for class assignment, a decision that can substantially impact model performance.\n",
    "\n",
    "If your objective is to rank predictions, without the need for well-calibrated probabilities, and your dataset maintains a reasonable balance between classes, then ROC AUC is a good choice.\n",
    "\n",
    "However, in scenarios characterized by a heavily imbalanced dataset, or when your primary concern centers on the positive class, considering the F1 score or Precision-Recall curve with PR AUC is advisable. Additionally, using F1 may be advantageous because this metric is more straightforward to interpret and convey to business stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d09a51-de86-473e-b15f-a061b75f10b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*This notebook is based on the article: https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc.*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1b_Model_Evaluation_Class_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
