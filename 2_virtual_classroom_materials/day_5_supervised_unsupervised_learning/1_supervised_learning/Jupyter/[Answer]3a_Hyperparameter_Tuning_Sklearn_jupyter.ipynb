{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24842c29-643f-4599-a087-8922a8179b97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Supervised Learning Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8ad45c-fbfb-4823-b2ed-96a18970d12a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../../../../Data/data_titanic/train.csv\")\n",
    "train.Pclass = train.Pclass.astype(float) # to avoid DataConversionWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5ef396-2dd6-4570-b470-f554f5e5aed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.dropna(axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[['Pclass', 'Age', 'Sex', 'Embarked']],\n",
    "                                                    train['Survived'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd2dabc-be07-4acb-a2f4-0b73f40b9031",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 3: Tree-based Models & Hyperparameter Tuning\n",
    "In the previous notebook we constructed our first pipeline:\n",
    "```\n",
    "entire_pipeline = Pipeline([('feature_engineering', feature_engineering), ('dummy', DummyClassifier(strategy=\"most_frequent\"))])\n",
    "```\n",
    "Hold your constructed pipeline firmly! The only thing that we need to do now is to replace `DummyClassifier` with a proper learning model.   \n",
    "We can start with a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3767f440-076c-4587-8b9b-fbe3347a5e02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_engineering = ColumnTransformer([('numerical_scaler', preprocessing.MinMaxScaler(),['Pclass', 'Age']),\n",
    "                                         ('ohe', preprocessing.OneHotEncoder(sparse=False), ['Sex', 'Embarked'])\n",
    "                                        ],\n",
    "                                        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093c218f-15a1-4749-810b-263d00705bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fitting a Learning Model â€“ Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff2b0c8-5323-45cd-ae5b-dff287ed0772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1A: Reuse your composite and instead of a dummy, fit a decision tree with default parameters.\n",
    "# Store the result as dt_pipeline.\n",
    "\n",
    "dt_pipeline = Pipeline([('feature_engineering', feature_engineering), ('decision_tree', DecisionTreeClassifier())])\n",
    "\n",
    "# Train the pipeline\n",
    "dt_pipeline.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f66f99-538f-4845-a966-bbe3e9f586ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1B: Let the pipeline predict for the training set. \n",
    "# Store the result as y_pred_TRAIN_DT.\n",
    "# Also, display accuracy.\n",
    "\n",
    "y_pred_TRAIN_DT = dt_pipeline.predict(X_train)\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f8dd36-4e9d-460d-b079-375e6a627763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1C: Let the pipeline predict for the holdout set. \n",
    "# Store the result as y_pred_HOLDOUT_DT.\n",
    "# Also, display accuracy.\n",
    "\n",
    "y_pred_HOLDOUT_DT = dt_pipeline.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5832d2-e918-4439-91f6-2b31f12e2762",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Looking at the accuracy on training and holdout set, what can you infer about over model? Will it generalize well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b024cbf-d2ae-4756-aa48-2ef28264e88c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL TASK 2: Do the same steps with RandomForest with default parameters. \n",
    "# Does the RandomForest display similar results as decision tree? If not, why?\n",
    "\n",
    "# Reuse your composite and fit a random forest with default parameters.\n",
    "# Store the result as rf_pipeline.\n",
    "rf_pipeline = Pipeline([('feature_engineering', feature_engineering), ('random_forest', RandomForestClassifier())])\n",
    "\n",
    "# Train the pipeline\n",
    "rf_pipeline.fit(X = X_train, y = y_train)\n",
    "\n",
    "#Predict and show accuracy TRAIN\n",
    "y_pred_TRAIN_RF = rf_pipeline.predict(X_train)\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_RF))\n",
    "\n",
    "#Predict and show accuracy HOLDOUT\n",
    "y_pred_HOLDOUT_RF = rf_pipeline.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5fedf2-a839-4d31-bbd1-1343c03d810f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tuning Hyperparameters of our Decision Tree\n",
    "Time to improve the performance of our learning model by finding its optimal set of hyperparameters.  \n",
    "We start by examining **which hyperparameters are available** in our decision tree pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25df3637-5fdf-499b-8a2a-35ab43f77450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d332d3-0da9-4770-8d07-db4daf5dc172",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We would like to tune `max_depth` and `min_samples_split`.  \n",
    "Notice that to access them, we also need to navigate within the composite and call them as **`decision_tree`**`__max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70929f9d-5eb3-40ec-a58f-7bd3c772295b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 3: Define a grid through which we should search. \n",
    "# Tune parameters: max_depth and min_samples_split.\n",
    "# The values which you pick as parameters are up to you. You can think about them intuitively.\n",
    "\n",
    "param_grid = {'decision_tree__max_depth':[3, 4, 5, 6, 7, 8, 9], \n",
    "              'decision_tree__min_samples_split':[ 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d527bfe-eb90-4cc7-b86b-ad627c9c24ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model\n",
    "dt_pipeline\n",
    "\n",
    "# Searching strategy, providing grid\n",
    "tuning = GridSearchCV(dt_pipeline, param_grid)\n",
    "\n",
    "# Train\n",
    "tuning.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214bc114-a4c2-4482-b0bf-5d70a23b7a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the best parameters\n",
    "tuning.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a6ac8c-43d6-4e8f-9a2d-030026d34274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 4A: Use the best setting of the two hyperparameters and fit a optimized decision tree. \n",
    "# Hint: Reuse the pipeline and when declaring it, specify the params.\n",
    "# Store it as dt_pipeline_tuned.\n",
    "\n",
    "dt_pipeline_tuned = Pipeline([('feature_engineering', feature_engineering), \n",
    "                              ('decision_tree', DecisionTreeClassifier(max_depth=6, min_samples_split=5))])\n",
    "\n",
    "# Train\n",
    "dt_pipeline_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e06b94f-388c-4b4a-99a1-46f6df24cf86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 4B: Display accuracy on the training set of the optimized decision tree.\n",
    "\n",
    "print(metrics.accuracy_score(y_train, dt_pipeline_tuned.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1921f3b9-c0d0-4e35-b58c-61eac5456c72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 4C: Display accuracy on the holdout set of the optimized decision tree.\n",
    "print(metrics.accuracy_score(y_test, dt_pipeline_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51376345-2d88-435a-8892-30365f51f6f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Does the optimized decision tree perform better then the one with default parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ece052-1a59-4f33-9fba-22a2f225b5e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Optional Advanced TASK: Tuning Random Forest\n",
    "When you are tuning a more complex model, it is good practice to search available literature on which hyperparameters should be tuned. Below I have predefined some. You can play around with the grid, for example expand or narrow it. Keep in mind that as our feature set is extremely limited, its hard for hyperparameter tuning to arrive at something meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f29154-f7a5-4eb8-8d68-c5598a4994ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL TASK 5\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a pipeline\n",
    "rf_pipeline = Pipeline([('feature_engineering', feature_engineering), ('random_forest', RandomForestClassifier())])\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid_rf = {\n",
    "    'random_forest__bootstrap': [True, False],\n",
    "    'random_forest__max_depth': [3, 5, 10, 15],\n",
    "    'random_forest__max_features': [2, 3],\n",
    "    'random_forest__min_samples_leaf': [3, 4, 5],\n",
    "    'random_forest__min_samples_split': [5, 8, 10, 12],\n",
    "    'random_forest__n_estimators': [5, 10, 15, 20, 25]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf_pipeline, \n",
    "                           param_grid = param_grid_rf, \n",
    "                           cv = 3, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 2)\n",
    "\n",
    "# Searching strategy, providing grid\n",
    "tuning_rf = GridSearchCV(rf_pipeline, param_grid_rf)\n",
    "\n",
    "# Train\n",
    "tuning_rf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validated score (more robust than holdout set most likely)\n",
    "print(tuning_rf.best_score_)\n",
    "print(tuning_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d66a14d-8c64-45cb-8663-ff85f2f417cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Optional Advanced TASK: Check Kaggle competitions and join one of them!  \n",
    "https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "[Answer]1d_sup_learning_workflow_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
