{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f2f6564-a571-485e-bdf7-563e28cb0dbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# AutoML tools: Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe7fbd1-b44b-4e2c-b1ff-dd2a13023544",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook we will be using [Optuna](https://optuna.readthedocs.io/en/stable/index.html) for **hyperparameter optimization** in machine learning. \n",
    "Hyperparameter optimization is a critical step in improving the performance of machine learning models.\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/index.html)\n",
    "provides an efficient and automated way to search for the best hyperparameters.\n",
    "\n",
    "Before we dive into the specifics of [Optuna](https://optuna.readthedocs.io/en/stable/index.html), \n",
    "let's take a moment to understand what **hyperparameters** are. \n",
    "Hyperparameters are the parameters of a machine learning model that are **not learned** from the data during training. They are **set prior** to training and can have a significant influence on a model's performance and generalization ability.\n",
    "Examples of hyperparameters include the learning rate of an optimizer, the number of hidden layers in a neural network, and the regularization strength in a regression model.\n",
    "\n",
    "Selecting appropriate hyperparameters is a crucial aspect of developing effective machine learning models. Poorly chosen hyperparameters can lead to bad performance, including overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "364a06c6-2cf4-4d18-8ea3-654a525c8a5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's explore [Optuna](https://optuna.readthedocs.io/en/stable/index.html) in greater detail to better understand its features and functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4231cdae-a082-4c5c-b0d5-09c0b31ca77b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ddc43b-b373-4e58-8775-760b62b26ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e7f9dd-2619-433d-8a0d-35416b981562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e88bb7-47ba-4881-93f6-d3936d454edc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../../Data/Boston.csv')\n",
    "\n",
    "X = data.iloc[:, 1:14]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "563f568f-22a3-494d-b024-4c5af92601c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a baseline\n",
    "\n",
    "For this regression task we will optimize the hyperparameters of a [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html).\n",
    "The performance of the models will be compared by the [**\\\\(r^2\\\\)-score**](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n",
    "\n",
    "Let's start out by fitting a\n",
    "[GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1fad1fe-a017-466c-a76a-7ca483e54e33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "baseline_regressor = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "baseline_regressor.fit(X_train, y_train)\n",
    "baseline_r2 = baseline_regressor.score(X_test, y_test)\n",
    "\n",
    "print(f\"The baseline r2-score is {baseline_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b300412d-7d6c-450a-9326-b9a227f7068b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We find that on the test set the \\\\(r^2\\\\)-score for a\n",
    "[GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "without any hyperparameter tuning is about 0.79.\n",
    "In the following sections we will improve this score by optimizing the model with\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6221e50-9098-4f72-be51-c479395809b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define the objective function\n",
    "We start with **defining objective function**. The objective function is a crucial component of hyperparameter optimization. It defines the metric you want to optimize (e.g., accuracy, loss). This function takes hyperparameters as input, builds and trains a model, and evaluates its performance on a validation set.\n",
    "\n",
    "Let's take a look at GradientBoostingRegressor documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html.\n",
    "\n",
    "As we can see, GradientBoostingRegressor has many parameters. Which of them should we optimize?\n",
    "\n",
    "The overall parameters can be divided into 3 categories:\n",
    "\n",
    "- **Tree-Specific Parameters** min_samples_split, min_samples_leaf, max_leaf_nodes etc.\n",
    "- **Boosting Parameters**: learning_rate, n_estimators, subsample\n",
    "- **Miscellaneous Parameters**: loss, random_state etc.\n",
    "\n",
    "We will tune only first two types of parameters. *Let's follow the general approach for parameter tuning, explained in this article: https://luminousdata.wordpress.com/2017/07/27/complete-guide-to-parameter-tuning-in-gradient-boosting-gbm-in-python/.*\n",
    "\n",
    "First, we take a default **learning rate** (0.1). Now we should determine the **optimum number of trees** (n_estimators) for this learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537ba516-466c-45e4-8bc8-50ba27a25745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Objective function\n",
    "def objective_1(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  # Suggest the number of trees in range [10, 300]\n",
    "  n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators = n_estimators,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate r2\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee4324b7-15f4-4a25-9be9-4eb125c16182",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create and run the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc443f86-9c19-49c4-8725-a14817709931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new study (set of trials)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize an objective function\n",
    "study.optimize(objective_1, n_trials=50)\n",
    "\n",
    "# Print the results\n",
    "print('Study #1')\n",
    "# Attribute 'trials' returns the list of all trials\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "# Attribute 'best_trial' returns the best trial in the study\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "# 'value' returns the r2-score of the best trial in the study\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46431248-3754-44a6-ac8b-b28f97e73871",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our study showed **the optimum number of trees**.\n",
    "\n",
    "Now we will use this number of trees in our model and tune tree-specific parameters. We should choose the order of tuning variables wisely, i.e. start with the ones that have a bigger effect on the outcome. For example, we need to focus on variables max_depth and min_samples_split first, as they have a strong impact.\n",
    "\n",
    "Let's tune **max_depth** and **min_samples_split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d60c9bc6-cec0-418a-a146-18ed8abd8b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_2(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "  min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators = 178,\n",
    "        max_depth = max_depth,\n",
    "        min_samples_split = min_samples_split,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_2, n_trials=100)\n",
    "\n",
    "print('Study #2')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff421a1-57a9-40c6-aaca-aa6218ee5c60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We got the best values for max_depth and min_samples_split. At this point, we can notice that there is a big impovement in r2-score compared to the untuned model.\n",
    "\n",
    "Now, let's keep max_depth in our model and tune **min_samples_split** and **min_samples_leaf** together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a1eed98b-a75c-42bb-84c6-bf677f06d6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_3(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 71, step=10)\n",
    "  min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        max_depth=24,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_3, n_trials=50)\n",
    "\n",
    "print('Study #3')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fde9def-1893-4b32-b168-06c7d73ba586",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have the last tree-specific parameter we need to tune - **max_features**. We will try values from 1 to 13 in steps of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ec4fb94d-9dff-40ba-ae04-d2da0e09fc04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_4(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  max_features = trial.suggest_int('max_features', 1, 13, 2)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        max_depth=24,\n",
    "        min_samples_split=0.31220765553286495,\n",
    "        min_samples_leaf=1,\n",
    "        max_features = max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_4, n_trials=50)\n",
    "\n",
    "print('Study #4')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "918a37e6-6fa4-4126-97da-d31575dd0826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will tune boosting parameter **subsample**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ad79e085-3e24-49f1-a5d1-46e9de574cff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_5(trial):\n",
    "  # Define hyperparameters to optimize\n",
    "  subsample = trial.suggest_float('subsample', 0.6, 1, step=0.05)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        max_depth=24,\n",
    "        min_samples_split=0.31220765553286495,\n",
    "        min_samples_leaf=1,\n",
    "        max_features = 11,\n",
    "        subsample=subsample,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_5, n_trials=50)\n",
    "\n",
    "print('Study #5')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06405b7d-fc7f-40c7-af1d-68f9d2bc9b82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It can be seen, that default value of subsample is optimal.\n",
    "\n",
    "Now we will create a final model with all the parameters we tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6d8e46bb-4d25-4c95-9a62-a9cb6672c52f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_model = GradientBoostingRegressor(\n",
    "    n_estimators=178,\n",
    "    max_depth=24,\n",
    "    min_samples_split=0.31220765553286495,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=11,\n",
    "    subsample=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "final_predictions = final_model.predict(X_test)\n",
    "final_score = r2_score(y_test, final_predictions)\n",
    "print('Trained and evaluated the final model using the best hyperparameters.\\n')\n",
    "print('Final model score:', final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ddbfd5-320a-4a9d-bc9b-145f6dba1f26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Perfect! Using the Optuna library, we tuned the hyperparameters of the model and got an improvement in the r2-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e13ed1b-0129-4fd1-a955-e8d9bb59a995",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Your turn!\n",
    "Now it's your turn to put what you've learned about the Optuna library into practice! You will try to optimize the model hyperparameters for a classification problem.\n",
    "\n",
    "In the code chunk below we load the data for this task.\n",
    "The goal is to predict if a patient is obese.\n",
    "There are 4 classes  in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4f3c3f-a21b-498e-a9cb-5cb5908c4056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import obesity_data.csv dataset\n",
    "obesity_df = pd.read_csv(\"../../../../Data/obesity_data.csv\")\n",
    "\n",
    "X = obesity_df.iloc[:,:16]\n",
    "y = obesity_df[['NObeyesdad']]\n",
    "\n",
    "# Encode the categorical variables\n",
    "obesity_preprocessing = OneHotEncoder(drop=\"if_binary\")\n",
    "\n",
    "X = obesity_preprocessing.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63291a25-c2b1-480d-8eb1-c8a72ab0984f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "obesity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e77f79b-7466-4e3e-a0ed-0ddca97c504a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task: split the dataset into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032de1ae-8b71-4c00-8fde-4c40cc5666cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Again we create a baseline with default parameters.\n",
    "In this task we use a\n",
    "[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ea4f86-8f4f-46e7-acf9-85aec1df4312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "baseline_classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "baseline_classifier.fit(X_train, y_train)\n",
    "baseline_accuracy = baseline_classifier.score(X_test, y_test)\n",
    "\n",
    "print(f\"The baseline accuracy-score is {baseline_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f729061-5287-4fe7-91e1-50c7f838bd97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose one of the best untuned model based on lazypredict results (see notebook AutoML tools: LazyPredict & PyCaret)\n",
    "# Find documentation for this model and check which hyperparameters you can tune\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    # Suggest the number of trees in range [50, 200]\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "    # Suggest the function to measure the quality of a split\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    # Suggest the maximum depth of the tree in range [1, 100]\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 100)\n",
    "    # Suggest the number of samples required to split an internal node  in range [2, 20]\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "\n",
    "    # Create the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Note: use classification score function!\n",
    "\n",
    "# create a new study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize an objective function\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the results\n",
    "print('Study #1')\n",
    "# Attribute 'trials' returns the list of all trials\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "# Attribute 'best_trial' returns the best trial in the study\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "# 'value' returns the accuracy score of the best trial in the study\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60eb98ce-0386-43d6-b732-a92dfcda7644",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Congratulations! :) You finished the notebook about hyperparameter optimization with Optuna.\n",
    "\n",
    "This notebook has provided an introduction to the Optuna library and its significance in automating hyperparameter tuning for machine learning models. By utilizing its functionalities, we efficiently tuned the hyperparameters for a regression and classification problems.\n",
    "\n",
    "We encourage you to explore the Optuna documentation further.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/reference/index.html.\n",
    "\n",
    "Keep up the excellent work!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Answer]3b_Hyperparameter_Optimization_Optuna_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
