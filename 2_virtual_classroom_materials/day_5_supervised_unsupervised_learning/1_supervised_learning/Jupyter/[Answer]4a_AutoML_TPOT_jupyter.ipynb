{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4038f1ed-35f8-4703-80a4-3838a073d8bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# AutoML tools: TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b45265f-370d-43f1-aca4-328c71d584b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook, we will explore how to use [**TPOT**](https://epistasislab.github.io/tpot/) to automatically optimize machine learning pipelines.\n",
    "\n",
    "TPOT, which stands for **Tree-based Pipeline Optimization Tool**, is an open-source AutoML library in Python. It utilizes **genetic programming** to automate the process of feature engineering, model selection, and hyperparameter tuning. TPOT generates and evaluates a population of pipelines, evolving them over generations to identify the most effective combination of data preprocessing steps and machine learning models.\n",
    "\n",
    "Before we preceed, let's install TPOT library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f05395-5166-4ed5-aa34-97a4a525e664",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddec5faf-9861-4f4f-9f8a-99f1b1cdcc19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You only need to run this cell after installing the optuna package on Databricks\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1d5402-f309-4360-b69b-23264af18e02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by importing the necessary libraries and loading the Boston dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e198e9e-ab34-4c56-8c5a-c68f69d62eb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tpot\n",
    "from tpot import TPOTRegressor, TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5b595ff-7843-4463-bc92-7b805779aa54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Regression with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4753025d-9cbe-4645-97eb-dd1edd19d003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Boston dataset\n",
    "boston_df = pd.read_csv(\"../../../../Data/Boston.csv\")\n",
    "\n",
    "X = boston_df.iloc[:, 1:14]\n",
    "y = boston_df.iloc[:, -1]\n",
    "\n",
    "# Split the Boston data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f006282-a2db-4d53-83bb-c27c543708ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The TPOTRegressor performs an intelligent search over machine learning pipelines that can contain supervised regression models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the scikit-learn API. The TPOTRegressor will also search over the hyperparameters of all objects in the pipeline.\n",
    "\n",
    "[TPOT Regressor provides various parameters](https://epistasislab.github.io/tpot/api/#regression) to control the optimization process, including:\n",
    "\n",
    "* generations: The number of generations (iterations) for the genetic\n",
    "optimization process.\n",
    "* population_size: The number of pipelines to maintain in each generation.\n",
    "* max_time_mins: The maximum time (in minutes) that TPOT should run for optimization.\n",
    "* scoring: The performance metric used to evaluate the pipelines (e.g., 'neg_mean_squared_error', 'r2', etc.).\n",
    "* cv: The number of cross-validation folds to use during pipeline evaluation.\n",
    "* verbosity: The level of verbosity for output during optimization (higher values provide more details).\n",
    "\n",
    "Now, let's create an instance of TPOTRegressor and let it search for the best regression pipeline on the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204f56ac-f1f2-4449-bd53-232742f4d9bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a TPOTRegressor instance\n",
    "tpot_reg = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "\n",
    "# Fit TPOT on the training data for regression\n",
    "tpot_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52036ef-5537-4296-ba0b-91d8e8481841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the best regression pipeline on the test set\n",
    "best_pipeline_reg = tpot_reg.fitted_pipeline_\n",
    "test_score_reg = best_pipeline_reg.score(X_test, y_test)\n",
    "print(f'Test Set R^2 Score (Regression): {test_score_reg:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47d4048-f37b-4645-8171-728fe27a3c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classification with TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9030b1f-865a-4055-a084-f2e8cade086d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*You may use the documentation as a guide:*\n",
    "\n",
    "*http://epistasislab.github.io/tpot/api/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325ff5be-55a4-4805-8d52-75205e9cf134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task: Import titanic.csv dataset\n",
    "\n",
    "titanic_df = pd.read_csv(\"../../../../Data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d4ed23-da03-4ee9-9fc3-832b0f8de87a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = titanic_df[['Sex', 'Embarked', 'Pclass', 'Age']]\n",
    "y = titanic_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81676a09-b7d6-48d1-9e99-5d05ea1f4929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task: split the dataset into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65a6433-ff50-46e9-98cd-d495a9553bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to deal with the missing data and transform categorical attributes\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, [\"Sex\", \"Embarked\"]),\n",
    "        (\"num\", numerical_transformer, [\"Age\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b75cdff-9f3a-4386-95f1-060178a7e19e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task: Create a TPOTClassifier instance\n",
    "tpot_clas = TPOTClassifier(generations=5, population_size=20, verbosity=2, random_state=42)\n",
    "\n",
    "# Task: Fit TPOT on the training data for classification\n",
    "tpot_clas.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82516898-0d24-414b-b683-584800be0ee8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task: Evaluate the best classification pipeline on the test set\n",
    "best_pipeline_clas = tpot_clas.fitted_pipeline_\n",
    "test_score_clas = best_pipeline_clas.score(X_test, y_test)\n",
    "print(f'Test Set Accuracy (Classification): {test_score_clas*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aeff98a-9661-4e98-a1b4-0ff3e77a82fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook, we used TPOT, a powerful AutoML library, to automate the process of optimizing machine learning pipelines for both regression and classification tasks. We applied TPOT on two different datasets: the Boston dataset for regression and the Titanic dataset for classification.\n",
    "\n",
    "Using TPOT, we significantly reduced the manual effort of hyperparameter tuning and pipeline construction while achieving competitive performance in both tasks. TPOT is a valuable tool for automating the machine learning workflow and is worth exploring further for various datasets and tasks.\n",
    "\n",
    "We highly recommend delving deeper into the documentation of TPOT to gain a comprehensive understanding of its functionalities and capabilities.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "http://epistasislab.github.io/tpot/\n",
    "\n",
    "\n",
    "\n",
    "Happy automating!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Answer]4a_AutoML_TPOT_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
