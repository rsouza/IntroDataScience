{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c133f4f-54f7-4c34-8068-02ccbcc76ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fe21f1-e008-4f1c-9427-220e8b637239",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Supervised Learning Workflow\n",
    "Let's continue with our previous example and see how we can use composite estimators for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d33f40-f8bd-4ca6-9f05-70f7bc75142b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../../../../Data/data_titanic/train.csv\")\n",
    "train.Pclass = train.Pclass.astype(float) # to avoid DataConversionWarning\n",
    "train = train[['Sex','Embarked','Pclass', 'Age','Survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0437128-7849-4c3e-b104-00bc5d3eaa53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note**   \n",
    "If you later want to experiment with the composite transformers, comment out this cell and include also missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da6c821-99ee-4f6e-a161-9f28fb70eb98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.dropna(axis=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23ab413-9aad-4411-8007-60ad766b2a97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2: Composite Estimators\n",
    "Let's nicely wrap our feature engineering and model fitting into a nice composite estimator. We will be very simplistic and only use two steps. \n",
    "They will not nest into each other at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2dc4f55-2de0-4aa9-9771-e5439df07af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[['Pclass', 'Age', 'Sex', 'Embarked']],\n",
    "                                                    train['Survived'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e83149-4921-4d77-b5a1-6ed9ad55cdb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering wrapped into ColumnTransformer\n",
    "The two feature transformations can be easily wrapped up into a single\n",
    "[`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "object.\n",
    "This will ensure that our Feature Engineering is a bit **more robust and nicely encapsulated**.\n",
    "Section 6.1.4 [here](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "showcases the exact application that we intend to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa7d8e8-3441-4a1f-b68a-9b3d32e78fe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1: Wrap MinMaxScaler and OneHotEncoder into a single ColumnTransformer. \n",
    "# The transformers should be applied to the respective numerical or categorical columns only.\n",
    "# Store the resulting composite as feature_engineering\n",
    "# Hint: Use the argument remainder='passthrough'\n",
    "\n",
    "feature_engineering = ColumnTransformer([('numerical_scaler', preprocessing.MinMaxScaler(),['Pclass', 'Age']),\n",
    "                                         ('ohe', preprocessing.OneHotEncoder(sparse_output=False), ['Sex', 'Embarked'])\n",
    "                                        ],\n",
    "                                        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22206a9c-b770-4c69-a6bf-42cb18c00b03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predictive Model Wrapped into Pipeline\n",
    "Let's now wrap the feature engineering and the model into a single Pipeline Composite estimator. Here is some pseudocode for this:\n",
    "``` \n",
    "entire_pipeline = feature_engineering -> model  \n",
    "``` \n",
    "\n",
    "Both components are already available. From the step above we can directly reuse the object `feature_engineering`.\n",
    "As model, we just call a new\n",
    "[`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html),\n",
    "just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4e424c-3e11-461f-9888-ae7ce26e99e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 2: Wrap the feature engineering and the predictive model (dummy) into a single Pipeline composite estimator. \n",
    "# Store the result as entire_pipeline.\n",
    "\n",
    "entire_pipeline = Pipeline([('feature_engineering', feature_engineering), ('dummy', DummyClassifier(strategy=\"most_frequent\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0d5548-7186-4ad0-a6bb-bc9a5afa00e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 3: Uncomment the line and try to train the pipeline.\n",
    "# Notice that we are using untransformed data again (X_train) as the pipeline contains all necessary transformers.\n",
    "\n",
    "entire_pipeline.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8cbb31-33f2-41a9-ba62-ae512c5665ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict for training data\n",
    "y_pred_TRAIN_DUMMY = entire_pipeline.predict(X_train)\n",
    "\n",
    "# Predict for holdout data\n",
    "y_pred_HOLDOUT_DUMMY = entire_pipeline.predict(X_test)\n",
    "\n",
    "# Results should be the same as before\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_DUMMY))\n",
    "\n",
    "# Display accuracy on holdout set.\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_DUMMY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a61c7f-51f6-4e3a-abc9-89602df90a21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**OPTIONAL TASK**   \n",
    "The notebook <a href=\"$./2b_Example_Pipelines\">``2b_Example_Pipelines``</a> was made to exemplify some examples of more complex pipelines. Feel free to scroll through it and learn what the process of preparing a complex composite looks like. You can then come back here and try to implement various components. For example, if I would not drop rows with missing values at the beginning of this notebook, constructing a composite would get a bit trickier. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Answer]2a_Composite_Estimators_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
