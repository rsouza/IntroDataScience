{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e2c9c8-8364-41e5-8cb5-06e8d9d93fca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Using Pipelines\n",
    "\n",
    "In this notebook we present a nice example of a pipeline which we can use for training purposes. At first glance it looks messy and hard to read.  \n",
    "But if you take a moment to really understand it you will notice the beauty of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc11907-b26a-4b76-a564-a461381de5ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d8c211-31b6-41f9-8946-f36670de6b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to import some transformers which are going to be inside of the pipeline.  \n",
    "**This is not operational code, just an example of longer pipelines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8c4d09-2fb6-4465-b652-f43523478307",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Dimensionality reduction\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Other\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65b4e902-1385-4703-a9fb-2a28bf873c56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The pipeline below will be a bit overwelming at first. \n",
    "Because of this we recommend the following steps to ease comprehension:\n",
    "\n",
    "## Step 1: Take a quick glance\n",
    "Please take a quick look at the pipeline below and come back here.\n",
    "\n",
    "## Step 2: Slow walkthrough\n",
    "Get a **high level view** of the pipeline:\n",
    "- Look at the top. \n",
    "There is a \n",
    "[`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) \n",
    "object, which is a wrapper for the entire feature engineering process.\n",
    "- Look at the bottom. \n",
    "There is a \n",
    "[`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) \n",
    "object, which is our predictive model.\n",
    "\n",
    "Now we can go deeper into the \n",
    "[`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) \n",
    "object we have instantiated, which is where the **feature engineering** is happening.\n",
    "- [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) \n",
    "splits into three parts, depending on which features we are attempting to process:\n",
    "    - On top we have numerical features.\n",
    "    - In the middle we have categorical features.\n",
    "    - At the bottom we have textual features.\n",
    "- Now zoom out again and realize that this is wrapped under \n",
    "[`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html), \n",
    "which means that **these features will be transformed in a parallel way and appended next to each other**.\n",
    "\n",
    "## Step 3: Zooming in\n",
    "Only now let's **zoom into one part of our feature engineering**, for example into `numerical_features`, on the top:\n",
    "- Inside of it, we right away use \n",
    "[`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) \n",
    "as we want to specify for which columns certain transformation will be applied to based on name or by type.\n",
    "- Now we can already apply the transformers. \n",
    "But remember that \n",
    "[`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "by default drops all untransformed columns, which would mean that if we want to apply some transformations sequentially we would not be able to.\n",
    "\n",
    "## Step 4: Indentation\n",
    "Finally, **get used to the indentation** (the whitespacing). \n",
    "Your code editor helps with this. \n",
    "Get used to this by clicking just behind the last visible character on the line where you are. \n",
    "For example go behing the last bracket on the line of\n",
    "[`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). \n",
    "Now if you hit Enter, it will land where a code should continue on the next line it you still want to stay within the element, which is a\n",
    "[`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f67178-240c-42d8-b2e0-3ec3facf5e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Source 1: https://www.codementor.io/@bruce3557/beautiful-machine-learning-pipeline-with-scikit-learn-uiqapbxuj   \n",
    "Source 2: http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f48898-f9f8-4ad8-bdf6-5512abc3a8ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"numerical_features\",\n",
    "         ColumnTransformer([\n",
    "             (\"numerical\",\n",
    "              Pipeline(steps=[(\n",
    "                  \"impute_stage\",\n",
    "                  SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "              )]),\n",
    "              [\"feature_1\"]\n",
    "             )\n",
    "         ])\n",
    "        ), \n",
    "        (\"categorical_features\",\n",
    "            ColumnTransformer([\n",
    "                (\"country_encoding\",\n",
    "                 Pipeline(steps=[\n",
    "                     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "                     (\"reduction\", NMF(n_components=8)),\n",
    "                 ]),\n",
    "                 [\"country\"],\n",
    "                ),\n",
    "            ])\n",
    "        ), \n",
    "        (\"text_features\",\n",
    "         ColumnTransformer([\n",
    "             (\"title_vec\",\n",
    "              Pipeline(steps=[\n",
    "                  (\"tfidf\", TfidfVectorizer()),\n",
    "                  (\"reduction\", NMF(n_components=50)),\n",
    "              ]),\n",
    "              \"title\"\n",
    "             )\n",
    "         ])\n",
    "        )\n",
    "    ])\n",
    "    ),\n",
    "    (\"classifiers\", RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ed3c49-3978-4268-b3d2-19541a156f98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have created a pipeline and know how it is structured, we can work with easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836d7dd3-69f7-41a9-8348-c25ba5e64f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_pipeline.fit(train_data, train_labels.values)\n",
    "predictions = model_pipeline.predict(predict_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1a9d09-b3d5-4f7c-b22e-f79de0e9fa8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. How to write that?\n",
    "Alright, we are now slightly more comfortable with understanding how pipelines work. But how do we write them ourselves?   \n",
    "The answer is: **from the outside inwards**. \n",
    "\n",
    "Let's walk through an example. Of course you can write things differently.    \n",
    "At first, lay out a simple structure which separates your feature engineering (inside of \n",
    "[`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)\n",
    ") and your predictive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7e3fc0-5c92-4b5a-a1f2-f345a8fdc566",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model_pipeline = Pipeline(steps=[\n",
    "#     (\"features\", FeatureUnion([#all feature engineering goes here])),\n",
    "#     (\"classifiers\", RandomForestClassifier())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faf9884a-1311-4e5e-9915-70127660b79c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Secondly, depending on your features, split the feature engineering into various parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4981a8-2ddb-4c69-9b15-72e9ec090983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"features\", FeatureUnion([(\"numerical_features\", #numerical transformations), \n",
    "                               (\"categorical_features\", #categorical transformations), \n",
    "                               (\"text_features\", #textual transformations)\n",
    "                              ])\n",
    "    ),\n",
    "    (\"classifiers\", RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "718e2030-3432-4a91-a882-0238d4135804",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now you want to insert a\n",
    "[`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "as the transformations will be applied only to specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52a2804-191c-491d-8470-c2d36f3a151d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"features\", FeatureUnion([(\"numerical_features\", ColumnTransformer([#numerical transformations])),\n",
    "                               (\"categorical_features\", ColumnTransformer([#categorical transformations])),\n",
    "                               (\"text_features\", ColumnTransformer([#textual transformations]))\n",
    "                              ])\n",
    "    ),\n",
    "    (\"classifiers\", RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a240ee-1de4-45fc-aaa7-695d1fb6893b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can put a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "inside of the feature engineering, for example, in case you have transformers which need to be applied sequentially (such as numeric scaling and feature selection).  \n",
    "\n",
    "At this point you can start inserting your individual transformations from before into the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6a598c-31da-4d51-9d8b-b246abc0bd13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Reflect\n",
    "Continue with this point only once you went through the pipeline above.  \n",
    "\n",
    "Usually we think that nicely written code costs significantly more effort than code scraped together in whichever way. Now that we went through the composite estimators properly, you know that it might be even simpler in many cases, not to mention more robust.  \n",
    "\n",
    "At this point, you are hopefully able to tell apart two things:  \n",
    "- Data preprocessing and wrangling.\n",
    "- Data preparation for ML (Feature Engineering)  \n",
    "\n",
    "Always try to separate these things in your use case (code). That is why we present these topics separatedely. It will be of tremendous help in the long run to write code in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6722c17-79e2-4cd4-ad9f-1097c2780011",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Working Example  \n",
    "[Source](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a065c2c-193b-4449-b909-b969288a4306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41759475-47d7-4670-8725-9d635901d4fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../../../Data/data_titanic/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea99a659-ad81-4a09-85b8-5ff0152a9505",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will use \n",
    "[`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "where we select the columns by their names.   \n",
    "We will then train our classifier with the following features:\n",
    "\n",
    "**Numeric Features**\n",
    "\n",
    "* 'Age': float;\n",
    "* 'Fare': float.\n",
    "\n",
    "**Categorical Features**\n",
    "\n",
    "* 'Embarked': categories encoded as strings: ``{'C', 'S', 'Q'}``;\n",
    "* 'Sex': categories encoded as strings: ``{'female', 'male'}``;\n",
    "* 'Pclass': ordinal integers: ``{1, 2, 3}``.\n",
    "\n",
    "We first create the preprocessing pipelines for both numeric and categorical data.\n",
    "Note that 'Pclass' could either be treated as a categorical or a numeric\n",
    "feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a7542d-6fb6-418b-b21e-24e40003fba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2abe43-c4a1-4523-84c2-a0f2c28c6881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"Age\", \"Fare\"]\n",
    "numeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "                                      (\"scaler\", StandardScaler())]\n",
    "                              )\n",
    "\n",
    "categorical_features = [\"Embarked\", \"Sex\", \"Pclass\"]\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[(\"num\", numeric_transformer, numeric_features),\n",
    "                                               (\"cat\", categorical_transformer, categorical_features),\n",
    "                                              ]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b567d43f-511f-40f0-8318-f36460bc8304",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Append a classifier, in this case a logistic regression, to preprocessing pipeline to arrive at a full prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffb4cd8-de75-4be2-99e8-24357c90fcc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                      (\"classifier\", LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6961a4db-72a4-4969-bfd2-47056046778e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ea548e-50fd-456d-8bf5-08acf68d345e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**We can also select columns by data types when using\n",
    "[`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).**   \n",
    "When dealing with a cleaned dataset, the preprocessing can be automatic by\n",
    "using the data types of the column to decide whether to treat a column as a numerical or categorical feature.\n",
    "\n",
    "The function [`sklearn.compose.make_column_selector`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html) gives this possibility.\n",
    "\n",
    "In practice, you will have to handle the column data type yourself.\n",
    "If you want some columns to be considered as `category`, you will have to convert them into categorical columns. If you are using pandas, you can refer to their documentation regarding [Categorical data](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html).</p>\n",
    "\n",
    "\n",
    "+ First, we will transform the object columns into categorical.  \n",
    "+ Then we will only select a subset of columns to simplify our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1822d8d5-f768-4b52-a7d2-47e082420be1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X[\"Embarked\"] = X[\"Embarked\"].astype(\"category\")\n",
    "X[\"Sex\"] = X[\"Sex\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d3fb7b-fef0-4e2e-badf-c6015a9afd41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "subset_feature = [\"Embarked\", \"Sex\", \"Pclass\", \"Age\", \"Fare\"]\n",
    "X_train, X_test = X_train[subset_feature], X_test[subset_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2d9a1e-2c42-4e5f-bfdc-9b16145d2a9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d15a31-f40e-4528-83ac-680a2a879276",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can observe that the 'Embarked' and 'Sex' columns were tagged as `category` columns.  \n",
    "Therefore, we can use this information to dispatch the categorical columns to the ``categorical_transformer`` and the remaining columns to the ``numerical_transformer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd800c42-4e36-46b1-ad85-6eb3fa415838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d57983bc-4269-4cbb-9542-0fd19c71d612",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The resulting score is not exactly the same as the one from the previous pipeline because the dtype-based selector treats the 'Pclass' column as a numeric feature instead of a categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d615224-9919-48ae-9219-05d6524e3994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selector(dtype_exclude=\"category\")(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "493070d9-519f-4729-ab15-6d92ea76cdc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selector(dtype_include=\"category\")(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7aa8242-966f-429b-b186-11d8e5d41e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Using the prediction pipeline in a grid search**   \n",
    "\n",
    "A grid search can also be performed on the different preprocessing steps which make up the\n",
    "[`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "object.\n",
    "So you can optimize the classifier's hyperparameters as part of the pipeline.  \n",
    "We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using\n",
    "[`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51398068-185e-4a2f-a827-d5ebdfb6cee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "              \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97a15db4-44c9-468d-9e39-9ee4598499ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Calling\n",
    "[`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit)\n",
    "triggers the cross-validated search for the best hyper-parameters combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46360b96-0141-4a54-a9d5-1e8408e1ff86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde103df-e4ab-4970-9f6c-0f154b40438b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The internal cross-validation score obtained by those parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d06e44-5f7b-4aac-a54b-9ef3caaa2cf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Internal CV score: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27bcdf53-c03e-4a9c-b9f9-0d3f8c05a6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also extract the top grid search results as a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86937c26-2165-4588-906c-614cd5be46da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\n",
    "cv_results[[\"mean_test_score\",\n",
    "            \"std_test_score\",\n",
    "            \"param_preprocessor__num__imputer__strategy\",\n",
    "            \"param_classifier__C\",\n",
    "           ]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083721f2-3852-46d9-8e55-ee53ebb3b340",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The best hyper-parameters have to be used to re-fit the final model on the full training set.  \n",
    "We can evaluate that final model on held out test data that was not used for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c9ea8e-76b1-4c40-a315-8118c3cb31de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"best logistic regression from grid search: {grid_search.score(X_test, y_test):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2b_Example_Pipelines_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
