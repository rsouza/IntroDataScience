{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebfccdfa-b73c-4723-a44f-ef176c9c8730",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# AutoML tools: TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9134e5d9-0919-4721-875c-168f9d3ca2c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook, we will explore how to use [**TPOT**](https://epistasislab.github.io/tpot/) to automatically optimize machine learning pipelines.\n",
    "\n",
    "TPOT, which stands for **Tree-based Pipeline Optimization Tool**, is an open-source AutoML library in Python. It utilizes **genetic programming** to automate the process of feature engineering, model selection, and hyperparameter tuning. TPOT generates and evaluates a population of pipelines, evolving them over generations to identify the most effective combination of data preprocessing steps and machine learning models.\n",
    "\n",
    "Before we preceed, let's install TPOT library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e107cc5e-a1ce-460f-bfe9-7f1d57687956",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q tpot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f34871cd-d1c9-4dca-b773-7c5de7f700c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by importing the necessary libraries and loading the Boston dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a915d97-1739-45b7-8883-91756020a287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tpot\n",
    "from tpot import TPOTRegressor, TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9df37ad-e147-47b8-9b76-17db66d6f33d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Regression with TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f40cd36-c04b-462e-b1a9-5f7262cf661f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will use the Boston dataset for the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac1787e9-2d49-4578-adca-5a4f86d33add",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Boston dataset\n",
    "boston_df = pd.read_csv(\"../../../../Data/Boston.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ee0990-cb3f-4fd8-8613-be86786ce906",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is important to preprocess your data before using TPOT. We need to take care of missing values and categorical variables. Let's look at the summary of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b5b4e7-17ec-42a7-87bc-20a16f88cf5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c557a64-a9e2-43ba-a6d6-39f735bfb288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "boston_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325abed1-7d99-4248-afa9-917e30e2a893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see here that all entries have non-null values and float data type for all columns. It means, the only thing we need to do is splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a61d7ec-ed98-42f7-9cc1-c5158b65507f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = boston_df.iloc[:, 1:14]\n",
    "y = boston_df.iloc[:, -1]\n",
    "\n",
    "# Split the Boston data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb571ea-2b8f-4928-8137-a35f24a53b71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The TPOTRegressor performs an intelligent search over machine learning pipelines that can contain supervised regression models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the scikit-learn API. The TPOTRegressor will also search over the hyperparameters of all objects in the pipeline.\n",
    "\n",
    "[TPOT Regressor provides various parameters](https://epistasislab.github.io/tpot/api/#regression) to control the optimization process, including:\n",
    "\n",
    "* generations: The number of generations (iterations) for the genetic\n",
    "optimization process.\n",
    "* population_size: The number of pipelines to maintain in each generation.\n",
    "* max_time_mins: The maximum time (in minutes) that TPOT should run for optimization.\n",
    "* scoring: The performance metric used to evaluate the pipelines (e.g., 'neg_mean_squared_error', 'r2', etc.).\n",
    "* cv: The number of cross-validation folds to use during pipeline evaluation.\n",
    "* verbosity: The level of verbosity for output during optimization (higher values provide more details).\n",
    "\n",
    "TPOT effectiveness improves with more generations, but the trade-off is longer processing time.\n",
    "\n",
    "**How can we control the execution time?**\n",
    "\n",
    "We can adjust certain parameters to control TPOT execution time:\n",
    "\n",
    "* max_time_mins: overrides the generations parameter, specifying the time TPOT runs.\n",
    "* max_eval_time_mins: how many minutes TPOT spends evaluating a single pipeline.\n",
    "* early_stop: determines when TPOT ends optimization if no improvement occurs.\n",
    "* n_jobs: specifies the number of procedures used in parallel during optimization.\n",
    "* subsample: fraction of training samples used during optimization.\n",
    "\n",
    "\n",
    "But keep in mind that constraining execution time limits TPOT's ability to explore all potential pipelines thoroughly. Consequently, the model suggested within this timeframe may be not the best fit for the dataset. However, if time is sufficient, TPOT can offer something very close to the best model.\n",
    "\n",
    "Now, let's create an instance of TPOTRegressor and let it search for the best regression pipeline on the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e889f3a-a851-43c9-b996-de543ddc5b85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a TPOTRegressor instance\n",
    "tpot_reg = TPOTRegressor(generations=5, population_size=20, verbosity=2, random_state=42, max_time_mins=5, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Fit TPOT on the training data for regression\n",
    "tpot_reg.fit(X_train.values, y_train)\n",
    "print(\"Negative MSE on the test set: \", tpot_reg.score(X_test.values, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ef099a-95ca-4f72-bf47-7be872f24ff2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Note that the default scoring function for regression is negative mean squared error. It is recommended to use the negative version of mean squared error and related metrics so TPOT will minimize (instead of maximize) the metric. You can also create and use your [custom metric](https://epistasislab.github.io/tpot/using/#scoring-functions).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "568ddcb6-5e12-44c1-b7be-b56ec2dff296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 4 main functions we can use:\n",
    "\n",
    "* fit - Run the TPOT optimization process on the given training data.\n",
    "* predict - Use the optimized pipeline to predict the target values for a feature set.\n",
    "* score - Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.\n",
    "* export - Export the optimized pipeline as Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12c11cc-c900-47a7-b304-8fbd86a29df2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at the resulting pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5f223a-b395-451e-9296-787cfb459471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tpot_reg.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "129347c9-51cf-46d1-bcb9-fa220e67ddb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classification with TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad35751-3d3b-4095-868c-396bb7dc4d8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Scikit-learn includes some [popular datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html). We will load [one of them](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) to illustrate how [**TPOTClassifier**](https://epistasislab.github.io/tpot/api/#classification) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42014b90-9795-42f0-85ef-d30c1ff8c96b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits(as_frame=True) \n",
    "#if as_frame parameter is set to True, the data is a pandas DataFrame\n",
    "digits.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25fd993-957f-4c0a-9b4a-909c40b6eaaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d532352-b138-40da-89ff-f9e411e9b2b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tpot_class = TPOTClassifier(generations=5, population_size=20, cv=5, random_state=42, verbosity=2, max_time_mins=5)\n",
    "\n",
    "tpot_class.fit(X_train.values, y_train)\n",
    "\n",
    "print(\"Accuracy on the test set: \", tpot_class.score(X_test.values, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ff4d167-67f4-44e5-818f-aa4497754aea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Note that the default scoring function for TPOTClassifier is accuracy. We can also use other built-in function such as 'balanced_accuracy', 'f1', 'precision', 'recall' etc.*\n",
    "\n",
    "To export the optimized pipeline as Python code we can use the export function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a315e5cb-fa01-481a-9cd4-dddb17f028fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tpot.export('tpot_class_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eed4990-5e13-467f-9b52-9842691c0c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's get predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee5d680-2021-4a7c-9e8e-006330d8c821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = tpot_class.fitted_pipeline_.predict(X_test.values)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e201f6c-a79f-43b6-8b93-849a5e599cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook, we used TPOT, a powerful AutoML library, to automate the process of optimizing machine learning pipelines for both regression and classification tasks. We applied TPOT on two different datasets: the Boston dataset for regression and the Digits dataset for classification.\n",
    "\n",
    "Using TPOT, we significantly reduced the manual effort of hyperparameter tuning and pipeline construction while achieving competitive performance in both tasks. TPOT is a valuable tool for automating the machine learning workflow and is worth exploring further for various datasets and tasks.\n",
    "\n",
    "We highly recommend delving deeper into the documentation of TPOT to gain a comprehensive understanding of its functionalities and capabilities.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "http://epistasislab.github.io/tpot/\n",
    "\n",
    "\n",
    "\n",
    "Happy automating!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "4a_AutoML_TPOT_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
