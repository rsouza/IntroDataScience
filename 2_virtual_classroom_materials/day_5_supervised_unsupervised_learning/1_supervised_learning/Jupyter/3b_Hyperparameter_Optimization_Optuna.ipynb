{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e71731a-65ad-4d4d-afdc-174049daed79",
     "showTitle": false,
     "title": ""
    },
    "id": "feQCPxrbtKvH"
   },
   "source": [
    "# AutoML tools: Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c3d41f2-dbd6-44fe-8ca0-09fa67b434bf",
     "showTitle": false,
     "title": ""
    },
    "id": "7tpgN5s7tKvK"
   },
   "source": [
    "In this notebook we will be using Optuna for **hyperparameter optimization** in machine learning. Hyperparameter optimization is a critical step in improving the performance of machine learning models. Optuna provides an efficient and automated way to search for the best hyperparameters.\n",
    "\n",
    "Before we dive into the specifics of Optuna, let's take a moment to understand what **hyperparameters** are. Hyperparameters are the parameters of a machine learning model that are **not learned** from the data during training. They are **set prior** to training and can have a significant influence on a model's performance and generalization ability. Examples of hyperparameters include the learning rate of an optimizer, the number of hidden layers in a neural network, and the regularization strength in a regression model.\n",
    "\n",
    "Selecting appropriate hyperparameters is a crucial aspect of developing effective machine learning models. Poorly chosen hyperparameters can lead to bad performance, including overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73833b6-e821-4928-95e3-a0fcf9c54529",
     "showTitle": false,
     "title": ""
    },
    "id": "AQUEqBPLtKvK"
   },
   "source": [
    "Let's explore Optuna in greater detail to better understand its features and functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54d125b-6c1e-424d-a45c-5e57cc63e228",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5goLuoItKvL",
    "outputId": "1dab20ca-8cd3-4999-ee5b-cf16eb6ddb98"
   },
   "outputs": [],
   "source": [
    "pip install -q optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1afee7-f744-4d56-b8a4-ca97b6052895",
     "showTitle": false,
     "title": ""
    },
    "id": "RhwQ_Dt_tKvM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f99825-995e-4e37-afd1-cfa0b0d8bca5",
     "showTitle": false,
     "title": ""
    },
    "id": "ndFylVf9tKvN"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../Data/Boston.csv')\n",
    "\n",
    "X = data.iloc[:, 1:14]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32b24294-96fd-4cc7-bc94-39cd2b67228b",
     "showTitle": false,
     "title": ""
    },
    "id": "9BKy44zutKvN"
   },
   "source": [
    "#### Define the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee351a55-8725-4f48-b7f5-34fc33e56eba",
     "showTitle": false,
     "title": ""
    },
    "id": "bbl_8rfTtKvN"
   },
   "source": [
    "As we found out before, according to LazyRegressor, the best performing model without hyperparameter tuning is GradientBoostingRegressor with **0.79 r2-score** (see notebook 'AutoML tools: LazyPredict & PyCaret'). Let's optimize this model with Optuna.\n",
    "\n",
    "We start with **defining objective function**. The objective function is a crucial component of hyperparameter optimization. It defines the metric you want to optimize (e.g., accuracy, loss). This function takes hyperparameters as input, builds and trains a model, and evaluates its performance on a validation set.\n",
    "\n",
    "Let's take a look at GradientBoostingRegressor documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html.\n",
    "\n",
    "As we can see, GradientBoostingRegressor has many parameters. Which of them should we optimize?\n",
    "\n",
    "The overall parameters can be divided into 3 categories:\n",
    "\n",
    "- **Tree-Specific Parameters** min_samples_split, min_samples_leaf, max_leaf_nodes etc.\n",
    "- **Boosting Parameters**: learning_rate, n_estimators, subsample\n",
    "- **Miscellaneous Parameters**: loss, random_state etc.\n",
    "\n",
    "We will tune only first two types of parameters. *Let's follow the general approach for parameter tuning, explained in this article: https://luminousdata.wordpress.com/2017/07/27/complete-guide-to-parameter-tuning-in-gradient-boosting-gbm-in-python/.*\n",
    "\n",
    "First, we take a default **learning rate** (0.1). Now we should determine the **optimum number of trees** (n_estimators) for this learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be2208b-919d-4b14-89ca-3f908279900e",
     "showTitle": false,
     "title": ""
    },
    "id": "8iO1GTMftKvO"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Objective function\n",
    "def objective_1(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  # Suggest the number of trees in range [10, 300]\n",
    "  n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators = n_estimators,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate r2\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f556b56c-71e9-47c2-a4cb-0d544fbcd93c",
     "showTitle": false,
     "title": ""
    },
    "id": "HdFyB00lBHqn"
   },
   "source": [
    "#### Create and run the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0442cf-3d2a-4cc2-973b-775851909ea5",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvluEFbwA62M",
    "outputId": "5925f212-7b32-4130-d917-dcba1201b243"
   },
   "outputs": [],
   "source": [
    "# Create a new study (set of trials)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize an objective function\n",
    "study.optimize(objective_1, n_trials=50)\n",
    "\n",
    "# Print the results\n",
    "print('Study #1')\n",
    "# Attribute 'trials' returns the list of all trials\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "# Attribute 'best_trial' returns the best trial in the study\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "# 'value' returns the r2-score of the best trial in the study\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8e3686d-33a7-4264-9e35-8916808c5989",
     "showTitle": false,
     "title": ""
    },
    "id": "pOl4ncRLtKvO"
   },
   "source": [
    "Our study showed **the optimum number of trees**.\n",
    "\n",
    "Now we will use this number of trees in our model and tune tree-specific parameters. We should choose the order of tuning variables wisely, i.e. start with the ones that have a bigger effect on the outcome. For example, we need to focus on variables max_depth and min_samples_split first, as they have a strong impact.\n",
    "\n",
    "Let's tune **max_depth** and **min_samples_split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0481eda-79b9-4a8e-acb4-9def0856a58c",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUN8FAn0tKvO",
    "outputId": "01d5d629-6034-4c75-ff39-c42fd29a1677"
   },
   "outputs": [],
   "source": [
    "def objective_2(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "  min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators = 178,\n",
    "        max_depth = max_depth,\n",
    "        min_samples_split = min_samples_split,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_2, n_trials=100)\n",
    "\n",
    "print('Study #2')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e84cc2-d90d-475e-8d1a-90b03ad6bd1d",
     "showTitle": false,
     "title": ""
    },
    "id": "jtL9Wmy1tKvO"
   },
   "source": [
    "We got the best values for max_depth and min_samples_split. At this point, we can notice that there is a big impovement in r2-score compared to the untuned model.\n",
    "\n",
    "Now, let's keep max_depth in our model and tune **min_samples_split** and **min_samples_leaf** together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bebbcaa-2b20-4788-90d3-e1cc004ef0e3",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvVIzyr61WTW",
    "outputId": "ee0d241c-17e3-44c1-c7ed-f7f8c95ba38f"
   },
   "outputs": [],
   "source": [
    "def objective_3(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 70, 10)\n",
    "  min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        max_depth=24,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_3, n_trials=50)\n",
    "\n",
    "print('Study #3')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3d2c35-a712-4c77-86c1-f34eb90231ae",
     "showTitle": false,
     "title": ""
    },
    "id": "MObxIvQl2qdX"
   },
   "source": [
    "We have the last tree-specific parameter we need to tune - **max_features**. We will try values from 1 to 13 in steps of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d1a035f-7d28-4c47-931a-ed233feac1db",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-fG36J6tKvP",
    "outputId": "d3d8b711-8915-4ea5-8757-1dd9b3d8208f"
   },
   "outputs": [],
   "source": [
    "def objective_4(trial):\n",
    "\n",
    "  # Define hyperparameters to optimize\n",
    "  max_features = trial.suggest_int('max_features', 1, 13, 2)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        max_depth=24,\n",
    "        min_samples_split=0.31220765553286495,\n",
    "        min_samples_leaf=1,\n",
    "        max_features = max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_4, n_trials=50)\n",
    "\n",
    "print('Study #4')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f7537b-b4e8-4809-8caf-b6122685c672",
     "showTitle": false,
     "title": ""
    },
    "id": "oUKYTaaaD1nT"
   },
   "source": [
    "Now we will tune boosting parameter **subsample**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b73da89-64f4-421f-985b-661deb2ad241",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAqga1yltKvP",
    "outputId": "9ca9e0fc-2237-4e7a-8225-98ee7e7c2338"
   },
   "outputs": [],
   "source": [
    "def objective_5(trial):\n",
    "  # Define hyperparameters to optimize\n",
    "  subsample = trial.suggest_float('subsample', 0.6, 1, step=0.05)\n",
    "\n",
    "  model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        max_depth=24,\n",
    "        min_samples_split=0.31220765553286495,\n",
    "        min_samples_leaf=1,\n",
    "        max_features = 11,\n",
    "        subsample=subsample,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  return r2\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective_5, n_trials=50)\n",
    "\n",
    "print('Study #5')\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4036e559-910f-456b-ae10-0272b8a11e86",
     "showTitle": false,
     "title": ""
    },
    "id": "13tVoK2iEgj9"
   },
   "source": [
    "It can be seen, that default value of subsample is optimal.\n",
    "\n",
    "Now we will create a final model with all the parameters we tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0956f79-211a-4149-af53-ccd0d1279c3d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1soinvpstKvP",
    "outputId": "46b58072-b90a-4d63-c858-4c7fc9ad8a4a"
   },
   "outputs": [],
   "source": [
    "final_model = GradientBoostingRegressor(\n",
    "    n_estimators=178,\n",
    "    max_depth=24,\n",
    "    min_samples_split=0.31220765553286495,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=11,\n",
    "    subsample=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "final_predictions = final_model.predict(X_test)\n",
    "final_score = r2_score(y_test, final_predictions)\n",
    "print('Trained and evaluated the final model using the best hyperparameters.\\n')\n",
    "print('Final model score:', final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dce1279-2062-4651-8e76-1aa049bd628e",
     "showTitle": false,
     "title": ""
    },
    "id": "4uKo84NAE4wU"
   },
   "source": [
    "Perfect! Using the Optuna library, we tuned the hyperparameters of the model and got an improvement in the r2-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3540ab88-8d20-413b-82bf-7a0b6e443dec",
     "showTitle": false,
     "title": ""
    },
    "id": "UtKMWiVjFvph"
   },
   "source": [
    "## Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d983029-3c9f-4970-bf1e-315fc3f0faf7",
     "showTitle": false,
     "title": ""
    },
    "id": "M3_icy0ZFyv1"
   },
   "source": [
    "Now it's your turn to put what you've learned about the Optuna library into practice! You will try to optimize the model hyperparameters for a classification problem. Select one of the best untuned models based on the results of LazyPredict, create an objective function and run the study. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27dde39-dfbb-4148-af8a-1541773d4107",
     "showTitle": false,
     "title": ""
    },
    "id": "biRWBSFNHV4d"
   },
   "outputs": [],
   "source": [
    "# Task: Import titanic.csv dataset\n",
    "\n",
    "titanic_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfeb1fa-6712-46eb-abd0-2092ebf8bfc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = titanic_df[['Sex', 'Embarked', 'Pclass', 'Age', 'Survived']]\n",
    "y = titanic_df[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c07bf1-6cee-4dc6-8a76-35d9fb0faa6b",
     "showTitle": false,
     "title": ""
    },
    "id": "f_-dJEpxIE9a"
   },
   "outputs": [],
   "source": [
    "# Task: split the dataset into train and test sets\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1c79ad-6154-4554-9758-61dcd76d60a9",
     "showTitle": false,
     "title": ""
    },
    "id": "IWl5fCAKIUY-"
   },
   "outputs": [],
   "source": [
    "# Choose one of the best untuned model based on lazypredict results (see notebook AutoML tools: LazyPredict & PyCaret)\n",
    "# Find documentation for this model and check which hyperparameters you can tune\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "  ...\n",
    "# Note: use classification score function!\n",
    "\n",
    "# create a new study\n",
    "study = ...\n",
    "\n",
    "# optimize an objective function\n",
    "...\n",
    "\n",
    "# print the results\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4231a756-87fc-4393-84f8-4c7b2d016376",
     "showTitle": false,
     "title": ""
    },
    "id": "h1H0Vlg6IT73"
   },
   "source": [
    "Congratulations! :) You finished the notebook about hyperparameter optimization with Optuna.\n",
    "\n",
    "This notebook has provided an introduction to the Optuna library and its significance in automating hyperparameter tuning for machine learning models. By utilizing its functionalities, we efficiently tuned the hyperparameters for a regression and classification problems.\n",
    "\n",
    "We encourage you to explore the Optuna documentation further.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/reference/index.html.\n",
    "\n",
    "Keep up the excellent work!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3_Hyperparameter_Optimization_with_Optuna",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
