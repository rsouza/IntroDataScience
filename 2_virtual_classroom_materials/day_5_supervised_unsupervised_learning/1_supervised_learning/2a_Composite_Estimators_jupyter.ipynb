{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34cb28d-7d05-41ee-b656-d9b268140885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df791325-5429-4416-b34b-584527a80961",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Supervised Learning Workflow\n",
    "Let's continue with our previous example and see how we can use composite estimators for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05cf511-e4b2-4438-84ce-a5b5f671188f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train = pd.read_csv(\"../../../Data/data_titanic/train.csv\")\n",
    "train.Pclass = train.Pclass.astype(float) # to avoid DataConversionWarning\n",
    "train = train[['Sex','Embarked','Pclass', 'Age','Survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "810dc301-79ce-4722-8a1f-9ea39d45debd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note**   \n",
    "If you later want to experiment with the composite transformers, comment out this cell and include also missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f499b5bc-380b-488f-879a-fbfe40ae59c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.dropna(axis=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bdea879-27b5-4203-885a-6d0a6e7afb88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2: Composite Estimators\n",
    "Let's nicely wrap our feature engineering and model fitting into a nice composite estimator. We will be very simplistic and only use two steps. \n",
    "They will not nest into each other at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d81ff27-d559-48ec-87c9-dc222d6cab0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[['Pclass', 'Age', 'Sex', 'Embarked']],\n",
    "                                                    train['Survived'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec249de2-d085-4aa0-beb2-5f5c39a1c6d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering wrapped into ColumnTransformer\n",
    "The two feature transformations can be easily wrapped up into a single\n",
    "[`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "object.\n",
    "This will ensure that our Feature Engineering is a bit **more robust and nicely encapsulated**.\n",
    "Section 6.1.4 [here](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "showcases the exact application that we intend to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7576010a-06fa-4250-aeb8-12cfc8700157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1: Wrap MinMaxScaler and OneHotEncoder into a single ColumnTransformer. \n",
    "# The transformers should be applied to the respective numerical or categorical columns only.\n",
    "# Store the resulting composite as feature_engineering\n",
    "# Hint: Use the argument remainder='passthrough'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "828b054d-bfcd-4657-bad6-709b7c0fd862",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predictive Model Wrapped into Pipeline\n",
    "Let's now wrap the feature engineering and the model into a single Pipeline Composite estimator. Here is some pseudocode for this:\n",
    "``` \n",
    "entire_pipeline = feature_engineering -> model  \n",
    "``` \n",
    "\n",
    "Both components are already available. From the step above we can directly reuse the object `feature_engineering`.\n",
    "As model, we just call a new\n",
    "[`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html),\n",
    "just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c66608-5f86-494e-9295-9cba11ae0243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 2: Wrap the feature engineering and the predictive model (dummy) into a single Pipeline composite estimator. \n",
    "# Store the result as entire_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08f8399-76ca-4371-aa2d-4b1c8cbeb741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 3: Uncomment the line and try to train the pipeline.\n",
    "# Notice that we are using untransformed data again (X_train) as the pipeline contains all necessary transformers.\n",
    "\n",
    "# entire_pipeline.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931001c1-761b-4d79-9251-3224e7c997a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict for training data\n",
    "y_pred_TRAIN_DUMMY = entire_pipeline.predict(X_train)\n",
    "\n",
    "# Predict for holdout data\n",
    "y_pred_HOLDOUT_DUMMY = entire_pipeline.predict(X_test)\n",
    "\n",
    "# Results should be the same as before\n",
    "print(metrics.accuracy_score(y_train, y_pred_TRAIN_DUMMY))\n",
    "\n",
    "# Display accuracy on holdout set.\n",
    "print(metrics.accuracy_score(y_test, y_pred_HOLDOUT_DUMMY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c955de-c213-498e-be4b-05086e30af83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**OPTIONAL TASK**   \n",
    "The notebook <a href=\"$./2b_Example_Pipelines_jupyter\">``2b_Example_Pipelines``</a> was made to exemplify some examples of more complex pipelines. Feel free to scroll through it and learn what the process of preparing a complex composite looks like. You can then come back here and try to implement various components. For example, if I would not drop rows with missing values at the beginning of this notebook, constructing a composite would get a bit trickier. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2a_Composite_Estimators_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
