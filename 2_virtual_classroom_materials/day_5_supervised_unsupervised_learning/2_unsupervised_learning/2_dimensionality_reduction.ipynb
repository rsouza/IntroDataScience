{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Principal component analysis is an unsupervised learning method that tries to detect the directions in which the vector formed data varies most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data of digits in black and white colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "x_digits, y_digits = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by visualizing our data. Fetch the first 10 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(16, 5))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: return the shape of x_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1797 numbers represented by 8 x 8 matrices with the color intensity for each pixel. These are flattened a vector of 64 numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters.\n",
    "\n",
    "__TO DO__: \n",
    "- import PCA from scikit-learns decomposition module\n",
    "- fit_transform PCA() model with n_components = 2 on x_digits and assign the result to the variable x_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we plot the reduced 2D feature space we see that similar numbers are close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we would choose the number of principal components such that we can explain e.g. 90% of the initial data dispersion (via the explained_variance_ratio_). \n",
    "\n",
    "__TO DO__: \n",
    "- return the explained variance ratio with PCA()'s attribute explained_variance_ratio_.  \n",
    "Each value returns the percentage of variance explained by each of the selected components. \n",
    "- How much variance did we explain with the first two components?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the returned array is equal to n_components. If n_components is not set, then all components are stored and the sum of the ratios is equal to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get 90% explained variance we would have to retain 20 principal components. Run the code below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(x_digits)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, x_digits.shape[1])\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "min_variance = 0.9\n",
    "min_components = np.min(np.where(np.cumsum(pca.explained_variance_ratio_) > min_variance))\n",
    "print(min_components)\n",
    "plt.axvline(min_components, c='b')\n",
    "plt.axhline(min_variance, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__scaling__\n",
    "\n",
    "In the MNIST dataset all the features have the same scale so the PCA algorithm is not impacted by different scales. \n",
    "\n",
    "While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "x_wine = wine.data\n",
    "y_wine = wine.target\n",
    "cols_wine = wine.feature_names\n",
    "df_wine = pd.DataFrame(x_wine, columns = cols_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: use a describe() method on df_wine to see standard deviations, quartiles and other descriptive statistics of the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that the scales of the features are different e.g. proline is in magnitude of hundreds, magnesium in tens, other are even smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: \n",
    "- fit_tranform the PCA with 2 components on the x_wine\n",
    "- return the reduced data set with 2 features and assign to the variable _x_reduced_\n",
    "- print the components of the pca with components_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: \n",
    "- use StandardScaler() to scale (fit_transform) the x_wine and store it to x_wine_scaled variable\n",
    "- fit_transform the PCA with 2 components on the scaled data\n",
    "- return the reduced data set with 2 features and assign it to the variable x_reduced_scaled\n",
    "- print the components of the pca with components_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe above how the last value of the components in the unscaled case was by cca 2 magnitudes larger than the other values of the components. In the scaled case the components have similar weights. You can see the results of the two PCAs in the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15,6))\n",
    "\n",
    "ax1.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_wine, \n",
    "            edgecolor='none', alpha=0.7, s=40)\n",
    "ax1.set_title('wine PCA projection');\n",
    "\n",
    "ax2.scatter(x_reduced_scaled[:, 0], x_reduced_scaled[:, 1], c=y_wine, \n",
    "            edgecolor='none', alpha=0.7, s=40)\n",
    "ax2.set_title(f'wine PCA projection scaled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling helped to visualize the similar results close to each other. As a consequence scaling improves the performance of classifiers after dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: use a train_test_split to split the x_wine, y_wine data into x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will make use of make_pipeline which constructs a pipeline from the given estimators. First we will apply PCA with 2 components to reduce dimensionality of the training data x_train and then apply RandomForestClassifier() with default parameters. We initialize PCA and RF in the pipeline and fit it to the training data.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_unscaled = make_pipeline(PCA(2), RandomForestClassifier()) \n",
    "clf_unscaled.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will predict the classes with the pipeline on a test dataset x_test and evaluate by comparing the predictions to true values from y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_unscaled = clf_unscaled.predict(x_test)\n",
    "accuracy_score(y_test, pred_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__:  repeat the same steps as in the previous two cells \n",
    "- make pipeline but now include additional estimator _StandardScaler()_ at the beginning of the pipeline. Assign the pipeline to the variable clf_scaled. In the example above you had 2 estimators (PCA(2) and RandomForestClassifier()). Now you will have 3 estimators. \n",
    "- fit the pipeline to the training data x_train and y_train\n",
    "- use the predict() method of clf_scaled on the test data x_test and store the predictions to a variable pred_test_scaled\n",
    "- print the accuracy of the pred_test_scaled compared to y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that scaling improved the accuracy of the model significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE\n",
    "\n",
    "TSNE is another dimensionality reduction method that works well for visualizing high-dimensional data. With t-sne you can get different results with different initializations. If the number of features is high t-sne can get quite slow. Here we select 2000 random samples from the mnist digits dataset and still t-sne takes much more time to complete than PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: use TSNE with n_components = 2 and fit_transform the random 2000 examples from mnist data set that is stored in x_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "t1 = dt.now()\n",
    "\n",
    "x_embedded = # your code here\n",
    "\n",
    "plt.scatter(x_embedded[:, 0], x_embedded[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. TSNE projection');\n",
    "print(f'Timerun: {dt.now()-t1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. Details of the algorithm are for further reading. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/work/2_virtual_classroom_materials/day_5_supervised_unsupervised_learning/2_unsupervised_learning/data/mnist.npz ~/.keras/datasets/mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_digits, y_digits), (_, _) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# if you have problems with RAM memory run the 3 lines below that will select just a subset of the data\n",
    "# idxs_ = random.sample(range(x_digits.shape[0]), 10000)\n",
    "# x_digits = x_digits[idxs_]\n",
    "# y_digits = y_digits[idxs_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by visualizing our data. Fetch the first 10 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(x_digits[i,:].reshape([28,28]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60.000 numbers represented by 28 x 28 matrices with the color intensity for each pixel. We need to flatten every matrix into a vector of 28x28 numbers. We will use .reshape() function that gives a new shape to an array without changing its data\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.reshape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_digits_flat = x_digits.reshape(x_digits.shape[0], x_digits.shape[1]*x_digits.shape[2]) # reshaping matrices into vectors\n",
    "x_digits_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the encoder decoder architecture stacked sequentially (autoencoder) using keras. if it is not installed you will have to install keras either with pip or conda. just execute the code and read comments. learning keras is out of scope of this lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Reshape\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import seaborn as sns\n",
    "\n",
    "X_train = x_digits/255.0 # digits matrices are scaled to values between 0 and 1\n",
    " \n",
    "### Encoder\n",
    "encoder = Sequential()\n",
    "encoder.add(Flatten(input_shape=[28,28]))   # flatten the input matrices into vectors of 28x28\n",
    "encoder.add(Dense(400,activation=\"relu\"))   # add a dense layer with 400 neurons and relu activation\n",
    "encoder.add(Dense(200,activation=\"relu\"))\n",
    "encoder.add(Dense(100,activation=\"relu\"))\n",
    "encoder.add(Dense(50,activation=\"relu\"))\n",
    "encoder.add(Dense(2,activation=\"relu\"))     # add a dense layer with 2 neurons and relu activation\n",
    " \n",
    "### Decoder\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(50,input_shape=[2],activation='sigmoid'))  # decoder will start with an input of dimension 2 from an encoder\n",
    "decoder.add(Dense(100,activation='sigmoid'))\n",
    "decoder.add(Dense(200,activation='sigmoid'))\n",
    "decoder.add(Dense(400,activation='sigmoid'))\n",
    "decoder.add(Dense(28 * 28, activation=\"sigmoid\"))\n",
    "decoder.add(Reshape([28, 28]))           # reshape the output from a flat vector to matrix of 28x28 \n",
    " \n",
    "### Autoencoder\n",
    "autoencoder = Sequential([encoder,decoder]) # stack encoder and decoder sequentially\n",
    "autoencoder.compile(loss=\"mse\", optimizer='Adam') # compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: fit the compiled autoencoder model as you are used from scikit learn. as x use X_train and as y use X_train again, specify number of epochs with parameter epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: use a predict() method of the encoder on X_train and assign the result to x_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('Encoder PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training with 5 epochs the result does not look very promising. However, if you let the model learn for 50 epochs you should get the result as shown on image below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/encoder.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
