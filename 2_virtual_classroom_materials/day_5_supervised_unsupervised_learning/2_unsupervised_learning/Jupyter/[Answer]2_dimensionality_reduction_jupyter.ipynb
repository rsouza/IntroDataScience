{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0296bc32-ff88-4a56-bd19-6ffe985ef866",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ff26e7-986c-453e-9fe2-894212ef3de2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588b8f9d-35c4-4ea7-bead-80e953169986",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "Principal component analysis is an unsupervised learning method that tries to detect the directions in which the vector formed by the data varies most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7743f102-f9de-40b0-a6e8-32733b048d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we import the data of grayscale digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816f64fd-03b5-428b-8ccc-1f36bf0213fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "x_digits, y_digits = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174358bb-9b40-492f-b17b-1365bff513c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by visualizing our data. Fetch the first 10 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9534450-0f16-4660-9067-2fab6628a552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(16, 5))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d10bf431-cc89-44b1-9d47-e06b7d14985a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: Return the shape of `x_digits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28cc7afa-c9ee-4446-90da-432dda12791a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "x_digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec385fd6-ba6d-46b9-b2f8-538023addc73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 1797 numbers represented by 8 x 8 matrices, each entry encoding the color intensity of a pixel. These matrices are flattened to a vector of 64 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf6227c-5b9f-415b-95d1-b080954ded22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our data has 64 dimensions, but we are going to reduce it to only 2. We will see that, even with just 2 dimensions, the digits are already separating into clusters.\n",
    "\n",
    "__TO DO__: \n",
    "- Import `PCA` from the `scikit-learns.decomposition` module.\n",
    "- `fit_transform` a `PCA()`-model with `n_components = 2` on `x_digits` and assign the result to the variable `x_reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2013727b-11f2-46e2-9f11-0e81140bd03f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_reduced = pca.fit_transform(x_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fab8dc1-5bd1-4dd5-843c-778f3dec6065",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If we plot the reduced 2D feature space we see that similar numbers are close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ea8bb5-5fd2-497a-bde2-9832682b9262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66193380-f771-44f0-80d5-947dbaef5b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In practice, we would choose the number of principal components such that we can explain e.g. 90% of the initial data dispersion (via the `explained_variance_ratio_`). \n",
    "\n",
    "__TO DO__: \n",
    "- Return the explained variance ratio with `PCA()`'s attribute `explained_variance_ratio_`.  \n",
    "Each value returns the percentage of variance explained by each of the selected components. \n",
    "- How much variance did we explain with the first two components?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67303766-8113-45d3-b6ea-b73ea6431a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3212b6-edb7-422f-84ea-bdece1a15a28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The length of the returned array is equal to `n_components`. If the `n_components` parameter is not set, then all components are stored and the sum of the ratios will be equal to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31567e0e-3d14-497e-b7b0-97818be61ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In order to get explain 90% of the variance we would have to retain 20 principal components. Run the code below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d77f570-f3b9-46f8-b737-4773b9c7f90c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(x_digits)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, x_digits.shape[1])\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "min_variance = 0.9\n",
    "min_components = np.min(np.where(np.cumsum(pca.explained_variance_ratio_) > min_variance))\n",
    "print(min_components)\n",
    "plt.axvline(min_components, c='b')\n",
    "plt.axhline(min_variance, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253cd630-b743-45f2-9b03-a298d9a47f94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Scaling__\n",
    "\n",
    "In the MNIST dataset all features have the same scale out of the box. Thus the PCA algorithm is not impacted by different scales. \n",
    "\n",
    "While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, Principle Component Analysis (PCA) especially is a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds to the 'weight' axis if those features are not scaled. As a change in height of one meter can be considered to be much more important than a change in weight of one kilogram, this is clearly incorrect.\n",
    "\n",
    "\n",
    "Import the wine data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8bf0df-c66c-4b7b-9e1d-2ea3cddcb060",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "x_wine = wine.data\n",
    "y_wine = wine.target\n",
    "cols_wine = wine.feature_names\n",
    "df_wine = pd.DataFrame(x_wine, columns = cols_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9abe0cc-df17-442b-a3c7-5600f45b2f42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: Use the `describe()` method on `df_wine` to see standard deviations, quartiles and other descriptive statistics of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac1892e-3975-4de3-8dba-58ead78fb35f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "df_wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62504ec1-2a5c-4ece-8539-14fa732affbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the scales of the features are different. E.g. 'proline' is in the magnitude of hundreds, magnesium in tens and others are even smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6281cca7-eda2-43be-87aa-ceb12fc9ed2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: \n",
    "- `fit_tranform` the PCA with 2 components on `x_wine`.\n",
    "- Return the reduced data set with 2 features and assign it to the variable `x_reduced`.\n",
    "- Print the components of the PCA using the `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0827c94-7774-4a3a-869c-2552917f831d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "pca_unscaled = PCA(n_components=2)\n",
    "x_reduced = pca_unscaled.fit_transform(x_wine)\n",
    "pca_unscaled.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea696d1-f758-4da7-85b6-e383126a8505",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: \n",
    "- Use `StandardScaler()` to scale (`fit_transform`) the `x_wine` and store it in the `x_wine_scaled` variable.\n",
    "- `fit_transform` the PCA with 2 components on the scaled data.\n",
    "- Return the reduced data set with 2 features and assign it to the variable `x_reduced_scaled`.\n",
    "- Print the components of the PCA using the `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da0c28a-664a-44f1-8a7b-ac385e403863",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "x_wine_scaled = StandardScaler().fit_transform(x_wine)\n",
    "pca_scaled = PCA(n_components=2)\n",
    "x_reduced_scaled = pca_scaled.fit_transform(x_wine_scaled)\n",
    "print(pca_scaled.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc688012-ccda-4701-814e-fa0ffbee5ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Observe how above the last value of the components in the unscaled case was 2 magnitudes larger than the other values of the components. In the scaled case the components have similar weights. You can see the results of the two PCAs in the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac848cd-db1b-4d14-9867-10e793838e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15,6))\n",
    "\n",
    "ax1.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_wine, \n",
    "            edgecolor='none', alpha=0.7, s=40)\n",
    "ax1.set_title('wine PCA projection (unscaled)');\n",
    "\n",
    "ax2.scatter(x_reduced_scaled[:, 0], x_reduced_scaled[:, 1], c=y_wine, \n",
    "            edgecolor='none', alpha=0.7, s=40)\n",
    "ax2.set_title('wine PCA projection (scaled)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e740991b-6c95-484a-83b2-5480a343e6b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To summerize:   \n",
    "Scaling helped us to visualize similar results close to each other. As a consequence scaling improves the performance of classifiers after dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e871a642-7320-401a-b25f-e23f15371dda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9b5673-4c3c-41cf-9738-ae4e54464258",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Optional part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2193e1f4-452b-40d4-ba2c-9c48c5f41050",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: Use the `train_test_split()` function to split the `x_wine`, `y_wine` data into `x_train`, `x_test`, `y_train`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76e62aa-6932-400a-9abc-5016346d3878",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_wine, y_wine, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3265433d-7325-47df-b21c-1f063762bac8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we will make use of [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) which constructs a pipeline from the given estimators. First we will apply a PCA with 2 components to reduce the dimensionality of the training data `x_train` and then apply `RandomForestClassifier()` with default parameters. We initialize a PCA and a RF in the pipeline and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d83c90-8d62-4388-bbc7-1dc7c323ee30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clf_unscaled = make_pipeline(PCA(2), RandomForestClassifier()) \n",
    "clf_unscaled.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f81228a-d21f-4ebc-97e5-e94851a4dabd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will predict the classes with the pipeline on a test dataset x_test and evaluate by comparing the predictions to true values from y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a659d73-08c1-4758-97e0-3e9acfc5c85f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_test_unscaled = clf_unscaled.predict(x_test)\n",
    "accuracy_score(y_test, pred_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9664897b-ff80-4213-b9ff-2a166be42834",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__:  Repeat the same steps as in the previous two cells.\n",
    "- Create a new pipeline but now include the additional estimator `StandardScaler()` at the beginning of the pipeline.   \n",
    "Assign the pipeline to the variable `clf_scaled`.   \n",
    "In the example above you had 2 estimators (`PCA(2)` and `RandomForestClassifier()`). Now you will have 3 estimators. \n",
    "- Fit the pipeline to the training data `x_train` and `y_train`.\n",
    "- Use the `predict()` method of `clf_scaled` on the test data `x_test` and store the predictions to a variable `pred_test_scaled`.\n",
    "- Print the accuracy of the `pred_test_scaled` as compared to `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b3fc8c-d965-4ccd-bb55-9f6eb1ccac01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "clf_scaled = make_pipeline(StandardScaler(), PCA(2), RandomForestClassifier())\n",
    "clf_scaled.fit(x_train, y_train)\n",
    "pred_test_scaled = clf_scaled.predict(x_test)\n",
    "accuracy_score(y_test, pred_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d8f932-0616-47a6-8714-bdfa92eaeb7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can see that scaling improved the accuracy of the model significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948de199-d09b-45a0-a032-6c08d5d57075",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## t-SNE\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) is another dimensionality reduction method that works well for visualizing high-dimensional data.   \n",
    "With t-SNE you can get **different results with different initializations**.   \n",
    "If the number of features is high then t-SNE can get quite slow. \n",
    "\n",
    "Here we select 2000 random samples from the MNIST digits dataset and still t-SNE takes much more time to complete than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1946f00c-1b2d-4445-83cd-c7d9a2d1986e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064cc753-d1b3-4f6c-a4ce-43d66b82ece0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: \n",
    "Use t-SNE with `n_components = 2` and then apply `fit_transform` to the random 2000 examples from the MNIST data set which are stored in `x_digits`. Store the result in the variable `x_embedded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ab17bf-9622-4478-931e-16309a9a7cd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "t1 = dt.now()\n",
    "x_embedded = TSNE(n_components=2).fit_transform(x_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2095d589-e6cc-4039-8c38-c8f110d6dbb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(x_embedded[:, 0], x_embedded[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. TSNE projection');\n",
    "print(f'Timerun: {dt.now()-t1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76901a54-a693-4baf-8d41-8f6ed64087b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. Details of the algorithm are for further reading. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d3315c-0e72-42cc-ae25-cfc4419ca59a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60d2bfb-8b11-47b1-b51e-501e96183182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir = os.path.join(current_dir, \"../../../../Data\")\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc70ea51-7c7b-434c-821d-68df17ba714c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(x_digits, y_digits), (_, _) = tf.keras.datasets.mnist.load_data(path=current_dir+'/mnist.npz')\n",
    "\n",
    "# if you have problems with RAM memory run the 3 lines below that will select just a subset of the data\n",
    "\n",
    "idxs_ = random.sample(range(x_digits.shape[0]), 10000)\n",
    "x_digits = x_digits[idxs_]\n",
    "y_digits = y_digits[idxs_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0886bc48-4061-4656-9930-1f13323eb27b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by visualizing our data. Fetch the first 10 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed6e9fc-7bc6-4ffa-8295-32ecfb6ec53c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(x_digits[i,:].reshape([28,28]), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc41383f-2acd-4153-9326-d2c03a21d854",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: Return the shape of `x_digits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11644152-a366-484c-8216-adc2744a48f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "x_digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd44267-4d89-439f-b89e-581c73f4c106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 60.000 numbers represented by 28 x 28 matrices with each entry encoding the color intensity for a pixel. We need to flatten every matrix into a vector of 28 x 28 = 784 numbers. We will use the numpy `.reshape()` method which gives a new shape to an array without changing its data.\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.reshape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4b32fc-531f-4dd6-b58d-9a60272f60b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x_digits_flat = x_digits.reshape(x_digits.shape[0], x_digits.shape[1]*x_digits.shape[2]) # reshaping matrices into vectors\n",
    "x_digits_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f055a3f3-a36c-4128-a722-f6c772e60ee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we compile the encoder-decoder architecture stacked sequentially (autoencoder) using keras. If it is not installed you will have to install keras either with pip or conda.   \n",
    "Just execute the code and read the comments. \n",
    "Learning keras is out of scope of this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd7e690-ac38-4494-95e2-64bf1ae63867",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Reshape\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import seaborn as sns\n",
    "\n",
    "X_train = x_digits/255.0 # digits matrices are scaled to values between 0 and 1\n",
    " \n",
    "### Encoder\n",
    "encoder = Sequential()\n",
    "encoder.add(Flatten(input_shape=[28,28]))   # flatten the input matrices into vectors of 28x28\n",
    "encoder.add(Dense(400,activation=\"relu\"))   # add a dense layer with 400 neurons and relu activation\n",
    "encoder.add(Dense(200,activation=\"relu\"))\n",
    "encoder.add(Dense(100,activation=\"relu\"))\n",
    "encoder.add(Dense(50,activation=\"relu\"))\n",
    "encoder.add(Dense(2,activation=\"relu\"))     # add a dense layer with 2 neurons and relu activation\n",
    " \n",
    "### Decoder\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(50,input_shape=[2],activation='sigmoid'))  # decoder will start with an input of dimension 2 from an encoder\n",
    "decoder.add(Dense(100,activation='sigmoid'))\n",
    "decoder.add(Dense(200,activation='sigmoid'))\n",
    "decoder.add(Dense(400,activation='sigmoid'))\n",
    "decoder.add(Dense(28 * 28, activation=\"sigmoid\"))\n",
    "decoder.add(Reshape([28, 28]))           # reshape the output from a flat vector to matrix of 28x28 \n",
    " \n",
    "### Autoencoder\n",
    "autoencoder = Sequential([encoder,decoder]) # stack encoder and decoder sequentially\n",
    "autoencoder.compile(loss=\"mse\", optimizer='Adam') # compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69aa809a-5072-4aa0-87b1-6ac9f9a9a4f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: Fit the compiled autoencoder model as you are used from Scikit-learn. As`x` use `X_train` and as `y` use `X_train` again and specify number of epochs with the parameter `epochs=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2da7ec1-6fce-4c52-9a2a-928ff3f38179",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "autoencoder.fit(X_train,X_train,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f3dbe7-7fd7-4f29-926d-1944dd7e0162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__TO DO__: Use the `predict()` method of the encoder on `X_train` and assign the result to `x_reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85cde4c-4ae6-49bd-834f-2cbf690ee1bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "x_reduced = encoder.predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047cd18b-a7b1-421e-a1f5-e12c7fb54423",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('Encoder PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91b81de-9632-4bfd-83d4-5df6e545b71d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After training with 5 epochs the result does not look very promising. However, if you let the model learn for 50 epochs you should get a result similar to the one shown in the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c944642b-9e75-4b46-93af-850e7e0e403b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train,X_train,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b64bde-adf7-4acb-b6da-b6de7e668b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x_reduced = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110995f7-86a1-4b88-8487-bde5ffc3c05f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y_digits, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('Encoder PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4205e04b-5376-44ac-bb7a-2fbe70efb9f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "[Answer]2_dimensionality_reduction_jupyter",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
