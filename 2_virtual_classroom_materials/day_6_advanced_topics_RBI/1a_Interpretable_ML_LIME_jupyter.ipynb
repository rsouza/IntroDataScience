{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d09bff-9075-4e17-9234-89e156004887",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explaining your Machine Learning model using [LIME](https://ema.drwhy.ai/LIME.html)  \n",
    "[Github](https://github.com/marcotcr/lime)  \n",
    "\n",
    "\n",
    "### [Why should you trust your model?](https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5)\n",
    "\n",
    "Shapley values are most suitable for models with a small or moderate number of explanatory variables. For models with a very large number of explanatory variables, sparse explanations with a small number of variables offer a useful alternative. The most popular example of such sparse explainers is the Local Interpretable Model-agnostic Explanations (LIME) method and its modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f390962e-776e-4ff9-9f61-3aebabe88d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1 - [Intuition behind LIME](https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-lime/notebook)   \n",
    "\n",
    "![](https://miro.medium.com/max/1165/1*k-rxjnvUDTwk8Jfg6IYBkQ.png)\n",
    "\n",
    "The intuition behind LIME is very simple. First, forget the training data and imagine we have only the black box model where we supply the input data. The black box model generate the predictions for the model. We can enquire the box as many times as we like. Our objective is to understand why the machine learning model made a certain prediction.\n",
    "\n",
    "Now, LIME comes into play. LIME tests what happens to the predictions when we provide variations in the data which is being fed into the machine learning model.\n",
    "\n",
    "LIME generates a new dataset consisting of permuted samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model. It is weighted by the proximity of the sampled instances to the instance of interest. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called **local fidelity**. There is no dependency on the type of original model for LIME to provide explanations (model agnostic).\n",
    "\n",
    "![https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5](https://miro.medium.com/max/720/1*vE3PUuhG6RRgK1J9oxg0nA.webp)\n",
    "\n",
    "What does LIME offer for model interpretability?\n",
    "1. A consistent model agnostic explainer – LIME.  \n",
    "2. A method to select a representative set with explanations – SP-LIME – to make sure the model behaves consistently while replicating human logic. This representative set would provide an intuitive global understanding of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0562acc-6161-416a-91ee-600951998267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U -q lime\n",
    "!pip install -U lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138938f3-eab4-40c0-a483-d099e2509b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dfdc1e2-c79c-4c26-9f6e-6ba7fabf8490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e1767b-008e-4340-a59d-79cfb120b613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ddcfae-0754-46db-8009-b5fa5fcd6a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading the titanic data\n",
    "df_titanic = pd.read_csv(\"../Data/titanic_data.csv\")\n",
    "\n",
    "\n",
    "# data preparation\n",
    "df_titanic.fillna(0,inplace=True)\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a344b582-375d-4595-bf8a-a719b77cf91f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "feat = ['PassengerId', 'Pclass_le', 'Sex_le','SibSp_le', 'Parch','Fare']\n",
    "\n",
    "# label encoding textual data\n",
    "df_titanic['Pclass_le'] = le.fit_transform(df_titanic['Pclass'])\n",
    "df_titanic['SibSp_le'] = le.fit_transform(df_titanic['SibSp'])\n",
    "df_titanic['Sex_le'] = le.fit_transform(df_titanic['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94fe80e-3769-4c2e-8177-2f95cd38868b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Using train test split to create validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80e54a8-d1a4-4fca-81ec-a09e99ff1006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_titanic[feat],df_titanic[['Survived']],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38604287-788c-469e-8804-19be10f85a97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'goss',\n",
    "    'objective': 'binary',\n",
    "    'metric':'binary_logloss',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbose': 0,\n",
    "    'num_iteration':100,\n",
    "    'num_threads':7,\n",
    "    'max_depth':12,\n",
    "    'min_data_in_leaf':100,\n",
    "    'alpha':0.5}\n",
    "\n",
    "\n",
    "# def lgb_model(X_train,y_train,X_test,y_test,lgb_params):\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "\n",
    "# training the lightgbm model\n",
    "model = lgb.train(lgb_params,\n",
    "                  lgb_train,\n",
    "                  num_boost_round=20,\n",
    "                  valid_sets=lgb_eval,\n",
    "                  early_stopping_rounds=5\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e474254-042e-4568-803f-31bc123a02d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3 - Model agnostic explainer (_LIME_).\n",
    "**LIME requires class probabilities in case of classification example.**   \n",
    "LightGBM directly returns probability for class 1 by default, so we will use it as a model here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd036055-3a0c-44e2-bbf7-9291708daba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a03f4d-ea8f-4499-8cfb-749cbab85895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prob(data):\n",
    "    return np.array(list(zip(1-model.predict(data),model.predict(data))))\n",
    "    \n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_titanic[model.feature_name()].astype(int).values,\n",
    "                                                   mode='classification',\n",
    "                                                   training_labels=df_titanic['Survived'],\n",
    "                                                   feature_names=model.feature_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b43d1e-b652-42b3-9705-d90696ec1476",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.1 - Asking for explanation for LIME model\n",
    "\n",
    "There are three parts to the explanation :\n",
    "\n",
    "+ Left most section displays prediction probabilities.\n",
    "+ The middle section returns the 5 most important features. For the binary classification task, it would be in 2 colors. orange/blue. Attributes in orange support class 1 and those in blue support class 0. `Sex_le` ≤0 supports class 1. \n",
    "+ Float point numbers on the horizontal bars represent the relative importance of these features.\n",
    "+ The color-coding is consistent across sections. It contains the actual values of the top 5 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ad7ad0-7285-4de8-a5d2-48fae7291f44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "exp = explainer.explain_instance(df_titanic.loc[i,feat].astype(int).values, prob, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bf29ed-c8d0-4bab-a89d-a06bb53d130e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = 6\n",
    "exp = explainer.explain_instance(df_titanic.loc[i,feat].astype(int).values, prob, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c65cb1-8d80-45e0-8e63-8455cdf15971",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2 - Submodular pick (*SP-LIME*) for explaining models\n",
    "\n",
    "LIME aims to attribute a model’s prediction to human-understandable features. In order to do this, we need to run the explanation model on a diverse but representative set of instances to return a nonredundant explanation set that is a global representation of the model.  \n",
    "\n",
    "**Note:** Running `SubmodularPick` can take some time, so you might want to run the cells below and return at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f73482b-c4a7-453f-a910-d924ef623f95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feac5b1e-edb6-47c0-a32c-d4c171ac9309",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "# Remember to convert the dataframe to matrix values\n",
    "# SP-LIME returns explanations on a sample set to provide a non redundant global decision boundary of original model\n",
    "sp_obj = submodular_pick.SubmodularPick(explainer, \n",
    "                                        df_titanic[model.feature_name()].values, \n",
    "                                        prob, \n",
    "                                        num_features=3,\n",
    "                                        num_exps_desired=5)\n",
    "\n",
    "#[exp.as_pyplot_figure(label=1) for exp in sp_obj.sp_explanations]\n",
    "[exp.show_in_notebook() for exp in sp_obj.sp_explanations]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1b_Interpretable_ML_LIME",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
