{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "714e8a1c-2190-4680-8590-b7290768b837",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Introduction to Time Series Analysis and Forecast\n",
    "\n",
    "Inspired by [this](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/), [this](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/) and [this](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/) blog posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f282a842-1dec-4c59-9683-1f6f6c8d0650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58238677-f742-4929-a9a8-a4bf5bbbd3d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Time Series (referred as TS from now) is considered to be one of the less known skills in the analytics space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0050cd-c405-4aa6-a9c6-c45a8136bfe1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Out journey would go through the following steps:  \n",
    "\n",
    "+ What makes a time series special?  \n",
    "+ Loading and handling time series in Pandas  \n",
    "+ How to check the stationarity of a time Series?  \n",
    "+ How to make a time series stationary?  \n",
    "+ Forecasting a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1b50aff-b5ae-48bc-b753-b9542707f4b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What makes Time Series Special?\n",
    "\n",
    "As the name suggests, a TS is a collection of data points collected at constant time intervals. These are analyzed to determine the long term trend so as to forecast the future or perform some other form of analysis. But what makes a TS different from a regular regression problem? There are 2 things:\n",
    "\n",
    "+ It is time dependent. So the basic assumption of a linear regression model that the observations are independent doesn’t hold in this case.  \n",
    "+ Along with an increasing or decreasing trend, most TS have some form of seasonality trends, i.e. variations specific to a particular time frame. For example, if you see the sales of a woolen jacket over time, you will invariably find higher sales in winter seasons.  \n",
    "\n",
    "Because of the inherent properties of a TS, there are various steps involved in analyzing it. These are discussed in detail below. Lets start by loading a TS object in Python. We’ll be using the popular **AirPassengers data set** which can be downloaded [here](https://www.analyticsvidhya.com/wp-content/uploads/2016/02/AirPassengers.csv).  \n",
    "\n",
    "Please note that the aim of this notebook is to familiarize you with the various techniques used for TS in general. The example considered here is just for illustration and I will focus on covering a breadth of topics and not making a very accurate forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b62091-bf44-416d-b09a-a437bc1af68f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loading and Handling Time Series in Pandas  \n",
    "\n",
    "Pandas has dedicated libraries for handling TS objects, particularly the `datatime64[ns]` class which stores time information and allows us to perform some operations really fast. Lets start by firing up the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22851eec-2bec-42a1-8349-0846f955e6d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('AirPassengers.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e02a2526-7b63-42b9-a4b5-5cc212fbe129",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec1a0ac-a9ad-47d7-86a6-23c0a83f6542",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The data contains a particular month and number of passengers travelling in that month. But this is still not read as a TS object as the data types are ‘object’ and ‘int’. In order to read the data as a time series, we have to pass special arguments to the `read_csv` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5e714e-70b6-4a8d-88e4-29fdb93b3fc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\n",
    "data['Month'] = pd.to_datetime(data['Month'], format='%Y-%m')\n",
    "data.Month[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33807f8b-bdd0-415f-8726-5291a556f709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "408af345-645d-47c9-88f7-26b9a7ef6748",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.set_index(['Month'], drop=True, inplace=True)\n",
    "data.index = pd.DatetimeIndex(data.index.values, freq=data.index.inferred_freq)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528739eb-759c-4f61-8fac-ffd85051eac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1886440-ddcf-4209-8f08-b9aff34af78b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting the dataframe into a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd341a1-0fd8-4983-8abb-a70c51fb3269",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts = data['#Passengers']\n",
    "ts.sort_index(inplace=True)\n",
    "ts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb8e30e-8394-4590-bbf1-b322a3a09890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using the index as a string constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07273052-c5ec-422d-a5dd-693185addb84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts['1949-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44427e72-46dc-4bdd-ba94-cf81fdcd07d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using the datetime library and use 'datetime' function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827347c6-520e-4216-984a-7880c2ae2384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts[datetime(1949,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df720b6-08f0-4c68-bcd2-bc97469102e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Specifying a time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac36ef1-a91d-43c4-b10d-f75369699871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts['1949-01-01':'1949-05-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa79020-0d02-477f-8f54-f97d1a5ce4e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts[:'1949-10-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20bb73d2-1ddf-4756-9740-d937839a4ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 2 things to note here: \n",
    "+ Unlike numeric indexing, the end index is included here.  \n",
    "For instance, if we index a list as a[:5] then it would return the values at indices – [0,1,2,3,4].  \n",
    "But here the index ‘1949-05-01’ was included in the output.  \n",
    "+ The indices have to be sorted for ranges to work. If you randomly shuffle the index, this won’t work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38f76d8-4fb7-4862-bb63-afb8f2a414ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts['1949']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bc4dc3-401b-4ac0-be00-cf887b864488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### How to Check Stationarity of a Time Series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "439c771e-fc5d-4f73-a9e2-cea8ae2345d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A TS is said to be stationary if its statistical properties such as mean, variance remain constant over time. But why is it important? Most of the TS models work on the assumption that the TS is stationary. Intuitively, we can say that if a TS has a particular behaviour over time, there is a very high probability that it will follow the same in the future. Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series.\n",
    "\n",
    "Stationarity is defined using a very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n",
    "\n",
    "1. constant mean  \n",
    "2. constant variance  \n",
    "3. an autocovariance that does not depend on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de23ca8b-07e2-4dc2-80b6-2bd7603da874",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "+ The mean of the series should not be a function of time, rather it should be a constant. The first graph in the image below has the left hand graph satisfying the condition whereas the graph in red has a time dependent mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a44db0f-ec43-4b22-ae0a-e3566deebb63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "+ The variance of the series should not a be a function of time. This property is known as homoscedasticity. The second graph below depicts what is and what is not a stationary series. (Notice the varying spread of the distribution in the right hand graph.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c98066b-7b28-4eb4-895c-2c86b683fc4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "+ The covariance of the \\\\(i^{th}\\\\) term and the \\\\((i+m)^{th}\\\\) term should not be a function of time. In the third graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the ‘red series’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc719354-e2a9-4303-8e56-6e5ec795970a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(Image('img/nonstationary.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8d6e259-2056-433e-80af-c660efc29a21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets move to testing stationarity. The first step is to simply plot the data and analyze it visually. The data can be plotted using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f48a1f43-1ecd-41d7-a9ac-7413400dad04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "115e023e-ae30-451f-a66d-dd545e38a7b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is clearly evident that there is an overall increasing trend in the data along with some seasonal variations. However, it might not always be possible to make such visual inferences (we’ll see such cases later). So, more formally, we can check stationarity using the following:\n",
    "\n",
    "+ **Plotting Rolling Statistics**   \n",
    "We can plot the moving average or moving variance and see if it varies with time. By moving average/variance I mean that at any instant ‘t’, we’ll take the average/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.\n",
    "\n",
    "+ **Dickey-Fuller Test**  \n",
    "This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a test statistic and some critical values for different confidence levels. If the test statistic is less than the critical value we can reject the null hypothesis and say that the series is stationary. Refer to [this article](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/) for more details.\n",
    "\n",
    "These concepts might not sound very intuitive at this point. I recommend going through the article linked at the beginning of this notebook. If you’re interested in some theoretical statistics, you can refer to [Introduction to Time Series and Forecasting by Brockwell and Davis](https://www.amazon.com/Introduction-Forecasting-Springer-Texts-Statistics/dp/0387953515). The book is a bit stats-heavy, but if you have the skill to read between the lines you can understand the concepts and tangentially touch the statistics.\n",
    "\n",
    "Back to checking stationarity, we’ll be using the rolling statistics plots along with the Dickey-Fuller test results a lot. Thus we have defined a function which takes a TS as input and generates both of them for us. Please note that we are plotting standard deviation instead of variance to keep the unit the same as for the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ae3849-d232-4358-ba95-07fdee1eac8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12,center=False).mean()\n",
    "    rolstd = timeseries.rolling(window=12,center=False).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daccfa6a-f2b2-41e1-916c-785c7fc7ed6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_stationarity(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307eadf4-aeb7-4594-a88d-1318738ec02d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Though the variation in standard deviation is small, the mean is clearly increasing with time and this is not a stationary series. Also, the test statistic is way higher than the critical values. Note that the signed values should be compared and not the absolute values.\n",
    "\n",
    "Next, we’ll discuss the techniques that can be used to take this TS towards stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f112a1-e33a-4bdf-bb04-f03652929475",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### How to make a Time Series Stationary?\n",
    "\n",
    "Though the assumption of stationarity is taken in many TS models, almost none of practical time series are stationary. So statisticians have figured out ways to make series stationary, which we’ll discuss now. Actually, its almost impossible to make a series perfectly stationary, but we try to take it as close as possible.\n",
    "\n",
    "Lets understand what is making a TS non-stationary. There are 2 major reasons non-stationarity of a TS:\n",
    "+ Trend – varying mean over time. In this case we saw that on average the number of passengers was growing over time.\n",
    "+ Seasonality – variations at specific time-frames. E.g. people might have a tendency to buy cars in a particular month because of a pay increment or festivals.\n",
    "\n",
    "The underlying principle is to model or estimate the trend and seasonality in the series and remove them to get a stationary series. Then statistical forecasting techniques can be implemented on this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.\n",
    "\n",
    "Note: I’ll be discussing a number of methods. Some might work well in this case and others might not. But the idea is to get a hang of all the methods and not focus on just the problem at hand.\n",
    "\n",
    "Let’s start by working on the trend part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f562753-b32e-46eb-8e2e-138570b46247",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Estimating & Eliminating Trend\n",
    "\n",
    "One of the first tricks to reduce trend can be transformation. For example, in this case we can clearly see that the there is a significant positive trend. So we can apply transformation which penalize higher values more than smaller values. This can mean taking a log, square root, cube root, etc. Lets take a log transform here for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afb45ae4-5172-4a0c-8440-502a241bce22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log = np.log(ts)\n",
    "ts_log.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd6b15f4-45ba-4260-9a25-beb8bd9d01ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this simpler case, it is easy to see a forward trend in the data. But its not very intuitive in the presence of noise. So we can use some techniques to estimate or model this trend and then remove it from the series. There can be many ways of doing it and some of most commonly used are:\n",
    "\n",
    "+ Aggregation – taking the average of a time period like monthly/weekly averages\n",
    "+ Smoothing – taking rolling averages\n",
    "+ Polynomial Fitting – fit a regression model\n",
    "\n",
    "I will discuss smoothing here and you should try other techniques as well which might work out for other problems. Smoothing refers to taking rolling estimates, i.e. considering the past few instances. There are are various ways to implement smoothing but I will discuss only two of them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d6bcf3c-37dc-4221-8b5b-7d6ec9313661",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Moving average\n",
    "\n",
    "In this approach, we take average of \\\\(k\\\\) consecutive values depending on the frequency of the time series. Here we can take the average over the past 1 year, i.e. last 12 values. Pandas has specific functions defined for determining rolling statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412c11cb-e68a-4c9e-9af7-f0e9952b64da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "moving_avg = ts_log.rolling(window=12, center=False).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ts_log.plot(ax=ax)\n",
    "moving_avg.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62692ed3-ce1a-4fe0-95a5-c759d3081eb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The orange line shows the rolling mean. Lets subtract this from the original series. Note that since we are taking the average of the last 12 values, the rolling mean is not defined for first 11 values. This can be observed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ffdf12-ddf5-4ce0-8a2f-9160a1dbeb87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff = ts_log - moving_avg\n",
    "ts_log_moving_avg_diff.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "700131a8-ee7e-4667-a054-d53fed27f277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880e72a7-28ed-40b8-9771-1e792eb4c5a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This looks like a much better series. The rolling values appear to be varying slightly but there is no specific trend. Also, the test statistic is smaller than the 5% critical values so we can say with 95% confidence that this is a stationary series.\n",
    "\n",
    "However, a drawback in this particular approach is that the time-period has to be strictly defined. In this case we can take yearly averages but in complex situations like forecasting a stock price, its difficult to come up with a number. So we take a ‘weighted moving average’ where more recent values are given a higher weight. There can be many technique for assigning weights. A popular one is exponentially weighted moving averages where weights are assigned to all the previous values with a certain decay factor. You can find more details [here](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-moment-functions). Exponentially weighted moving averages can be implemented in Pandas as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28804e6b-fab5-473f-9cc1-38391ac187d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expwighted_avg = ts_log.ewm(ignore_na=False, adjust=True, halflife=12, min_periods=0).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ts_log.plot(ax=ax)\n",
    "expwighted_avg.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7fd9632-31a6-4914-98c5-804a8113bded",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that here the parameter ‘halflife’ is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain. Now, let’s remove this from series and check stationarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c1ab06-8951-417e-a659-9a74b0e1762e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log_ewma_diff = ts_log - expwighted_avg\n",
    "test_stationarity(ts_log_ewma_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530f60b5-0817-4c54-b02e-eaf4205b454f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This TS has even less variation of mean and standard deviation in its magnitude. Also, the test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values since all values are given weights from the start. So it will work even with no previous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d211c460-1fdd-4837-ba83-9c5e0848efac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Eliminating Trend and Seasonality\n",
    "\n",
    "The simple trend reduction techniques discussed before don’t work in all cases, particularly the ones with high seasonality. Lets discuss two ways of removing trend and seasonality:\n",
    "\n",
    "+ Differencing – taking the difference with a particular time lag\n",
    "+ Decomposition – modeling both trend and seasonality and removing them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af2563f3-f010-4766-897e-4cff50aeb64d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Differencing\n",
    "\n",
    "One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity.  \n",
    "First order differencing can be done in Pandas as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1c0537f-1b84-42d3-abec-f17c24305fab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log_diff = ts_log - ts_log.shift()\n",
    "ts_log_diff.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ac7672-fd83-4f6c-8407-e43b58590423",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This appears to have reduced the trend considerably. Let's verify using our plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d49283e3-5f60-4fb5-bd11-751f6051a609",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0adf8bd9-0cb6-4ad8-b7ac-02d387296a26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the mean and standard deviation have small variations with time. Also, the Dickey-Fuller test statistic is less than the 10% critical value, thus the TS is stationary with 90% confidence. We can also take second or third order differences which might get even better results in certain applications. I leave it to you to try them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352a666d-fa8e-47e9-86c0-71233a0c277d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Decomposing\n",
    "\n",
    "In this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned. I’ll skip the statistics and come to the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2836f0ba-f7f9-4bd4-b0fb-29d7b50ee73c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "f, ax = plt.subplots(2,2,figsize=(12,12))\n",
    "ax[0,0].plot(ts_log, label='Original')\n",
    "ax[0,0].legend(loc='best')\n",
    "ax[0,1].plot(trend, label='Trend')\n",
    "ax[0,1].legend(loc='best')\n",
    "ax[1,0].plot(seasonal,label='Seasonality')\n",
    "ax[1,0].legend(loc='best')\n",
    "ax[1,1].plot(residual, label='Residuals')\n",
    "ax[1,1].legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f05eca5-c042-458d-9f48-77145929645c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we can see that the trend and seasonality are separated out from data and we can model the residuals. Let's check the stationarity of residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb906ef5-92c5-4a38-afad-e1926df0c4f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ts_log_decompose = residual\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "test_stationarity(ts_log_decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cccc2b4f-5afb-4661-95e5-23b1bcace71d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Dickey-Fuller test statistic is significantly lower than the 1% critical value. So this TS is very close to stationary. You can try advanced decomposition techniques as well which can generate better results. Also, you should note that converting the residuals into original values for future data is not very intuitive in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f737ef3-3cdf-4895-82f6-c4c5bd2912a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Forecasting a Time Series\n",
    "\n",
    "We saw different techniques and all of them worked reasonably well for making the TS stationary. \n",
    "A very popular technique is to create a model on the TS after differncing, which we will try out next. It is relatively easy to add noise and seasonality back into the predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:\n",
    "\n",
    "+ A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\n",
    "\n",
    "+ A series with significant dependence among values. In this case we need to use some statistical models like **ARIMA** to forecast the data.\n",
    "\n",
    "Let me give you a brief introduction to ARIMA. I won’t go into the technical details but you should understand these concepts in detail if you wish to apply them more effectively. ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear equation (like a linear regression). The predictors depend on the parameters \\\\( (p,d,q)\\\\) of the ARIMA model:\n",
    "\n",
    "+ **Number of AR (Auto-Regressive) terms (\\\\(p\\\\))**   \n",
    "AR terms are just lags of the dependent variable. For instance, if \\\\( p =  5 \\\\), the predictors for \\\\( x(t) \\\\) will be \\\\( x(t-1) \\ldots x(t-5) \\\\).\n",
    "+ **Number of MA (Moving Average) terms (\\\\(q\\\\))**    \n",
    "MA terms are lagged forecast errors in the prediction equation. For instance, if \\\\( q = 5 \\\\), the predictors for \\\\(x(t) \\\\) will be \\\\( e(t-1) \\ldots e(t-5) \\\\) where \\\\( e(i) \\\\) is the difference between the moving average at \\\\(i^{th}\\\\) instant and the actual value.\n",
    "+ **Number of Differences (\\\\(d\\\\))**     \n",
    "These are the numbers of non-seasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put \\\\( d=0 \\\\) or pass the original variable and put \\\\(d=1 \\\\). Both will generate the same results.\n",
    "\n",
    "An importance concern here is how to determine the value of \\\\(p\\\\) and \\\\(q\\\\). We use two plots to determine these numbers. Lets discuss them first.\n",
    "\n",
    "+ **Autocorrelation Function (ACF)**   \n",
    "It is a measure of the correlation between the the TS with a lagged version of itself. For instance, at lag 5, ACF would compare the series at the time instant \\\\(t_1 \\ldots t_2\\\\) with the series at instant \\\\(t_1-5 \\ldots t_2-5\\\\) (\\\\(t_1-5\\\\) and \\\\(t_2\\\\) being the end points).\n",
    "\n",
    "+ **Partial Autocorrelation Function (PACF)**    \n",
    "This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. E.g. at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\n",
    "\n",
    "The ACF and PACF plots for the TS after differencing can be plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2271deb-db24-4ecc-a45f-6a7d738e7f42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ACF and PACF plots:\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a43721b-f74f-493e-bf7e-e4cd47d8eb94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lag_acf = acf(ts_log_diff, nlags=20, fft=False)\n",
    "lag_pacf = pacf(ts_log_diff, nlags=20, method='ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d312b423-db6d-479d-b1b9-991d73a251c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Plot ACF: \n",
    "f = plt.figure(figsize=(12,6))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "\n",
    "ax1.plot(lag_acf)\n",
    "ax1.axhline(y=0,linestyle='--',color='gray')\n",
    "ax1.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "ax1.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "ax1.set_title('Autocorrelation Function')\n",
    "\n",
    "#Plot PACF:\n",
    "ax2.plot(lag_pacf)\n",
    "ax2.axhline(y=0,linestyle='--',color='gray')\n",
    "ax2.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "ax2.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "ax2.set_title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e67ce872-6852-4c5d-9ec6-2b5832e2a06f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the \\\\(p\\\\) and \\\\(q\\\\) values as:\n",
    "\n",
    "+ \\\\(p\\\\) – The lag value where the PACF chart crosses the upper confidence interval for the first time. If you look closely, in this case \\\\(p=2\\\\).\n",
    "+ \\\\(q\\\\) – The lag value where the ACF chart crosses the upper confidence interval for the first time. If you look closely, in this case \\\\(q=2\\\\).\n",
    "\n",
    "Now, let's create 3 different ARIMA models considering individual as well as combined effects. We will also print the RSS for each. Please note that here RSS is for the values of residuals and not the actual series.\n",
    "\n",
    "We need to load the ARIMA model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99dd4464-d1f9-4a96-a8cb-3978c5781497",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as smapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533503ae-7e7f-41d1-ac86-caab3ce840f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The \\\\(p, q, d\\\\) values can be specified using the order argument of ARIMA which takes a tuple \\\\((p, q, d)\\\\). Let's model the 3 cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45d9217-71ee-41ba-b118-60bd2b286e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### AR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cb0f5cc-51b0-496b-9164-593bfc2773e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = smapi.tsa.arima.ARIMA(ts_log_diff, order=(2, 1, 0))  \n",
    "results_AR = model.fit()  \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a2350a0-a4e4-4698-9bd1-05327b434371",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecfd6461-810f-41cd-b8a5-216d0ca958b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = smapi.tsa.arima.ARIMA(ts_log_diff, order=(0, 1, 2))  \n",
    "results_MA = model.fit()  \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7500c052-4df2-4fb5-b8bc-5cce32fe603f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41558140-4f41-4a4f-a6c0-99f75fa2a97e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = smapi.tsa.arima.ARIMA(ts_log_diff, order=(2, 2, 2))  \n",
    "results_ARIMA = model.fit(method_kwargs={\"warn_convergence\": False})  \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_ARIMA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a031b0ce-8d84-46fd-8a5b-33ff65e2e48c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we can see that the AR and MA models have almost the same RSS but combined they are significantly better. Now, we are left with 1 last step, i.e. taking these values back to the original scale.\n",
    "\n",
    "### Taking it back to original scale\n",
    "\n",
    "Since the combined model gave best result, let's scale it back to the original values and see how well it performs there. The first step would be to store the predicted results as a separate series and observe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d45712e-b86a-4083-8dd7-c5be8e52d619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n",
    "predictions_ARIMA_diff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ef75d9-471f-4deb-97e0-93ec9b64e14c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that these start from ‘1949-02-01’ and not the first month. Why? This is because we took a lag of 1 and the first element doesn’t have anything before it to subtract from. The way to convert the differencing to log scale is to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at a given index and then add it to the base number. The cumulative sum can be found as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93538540-439d-460d-a166-b1fcf32d0992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a84b57dd-e7da-403b-b6b8-3ed6e36076b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can quickly do some calculations using the previous output to check if these are correct. Next, we have to add them to the base number. For this, lets create a series with all values as a base number and add the differences to it. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91aa7055-d5b7-474f-8ed6-a0056395fb0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02d49b35-4250-4ba8-be12-13e3ea62bd10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here the first element is the base number itself and from thereon the values are cumulatively added. The last step is to take the exponent and compare it with the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03a29e8-fc67-4d66-8407-780a7d0271e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "predictions_ARIMA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "724fbac9-2aec-4920-8b76-73572134ac41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ts)\n",
    "plt.plot(predictions_ARIMA)\n",
    "plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)/len(ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d47e1a-c094-4d23-b5b2-6a7ede8aaad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we have a forecast at the original scale. Not a very good forecast but enough to get the idea. \n",
    "\n",
    "Now, I leave it upto you to refine the methodology further and make a better solution."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3_Intro_Time_Series",
   "notebookOrigID": 706562957542558,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
