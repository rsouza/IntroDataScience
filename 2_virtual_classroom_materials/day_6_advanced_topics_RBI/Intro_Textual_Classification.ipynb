{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization Techniques\n",
    "#### Bag Of Words, Topic Modeling and Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial  \n",
    "https://github.com/wendykan/DeepLearningMovies  \n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/  \n",
    "\n",
    "\n",
    "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.\n",
    "\n",
    "Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's charge the batteries for our analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "os.environ[\"NLTK_DATA\"] = \"/home/jovyan/work/2_virtual_classroom_materials/day_6_advanced_topics_RBI/data/nltk_data\"\n",
    "import nltk\n",
    "#import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set  \n",
    "--\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "sampleSubmission - A comma-delimited sample submission file in the correct format.\n",
    "Data fields\n",
    "\n",
    "id - Unique ID of each review  \n",
    "sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews  \n",
    "review - Text of the review  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('./data/labeledTrainData.tsv.zip', 'r') as myzip:\n",
    "    with myzip.open('labeledTrainData.tsv') as myfile:\n",
    "        train = pd.read_csv(myfile, header=0, delimiter=\"\\t\", quoting=3)\n",
    "        \n",
    "with ZipFile('./data/testData.tsv.zip', 'r') as myzip:\n",
    "    with myzip.open('testData.tsv') as myfile:\n",
    "        test = pd.read_csv(myfile, header=0, delimiter=\"\\t\", quoting=3)\n",
    "        \n",
    "with ZipFile('./data/unlabeledTrainData.tsv.zip', 'r') as myzip:\n",
    "    with myzip.open('unlabeledTrainData.tsv') as myfile:\n",
    "        unlabeled_train = pd.read_csv(myfile, header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_dataset = 0.05\n",
    "\n",
    "train = train.sample(frac=fraction_dataset, replace=False, random_state=1, ignore_index=True)\n",
    "test = test.sample(frac=fraction_dataset, replace=False, random_state=1, ignore_index=True)\n",
    "unlabeled_train = unlabeled_train.sample(frac=fraction_dataset, replace=False, random_state=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1250 labeled train reviews, 1250 labeled test reviews, and 2500 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} labeled train reviews, \\\n",
    "{} labeled test reviews, and \\\n",
    "{} unlabeled reviews\\n\".format(train[\"review\"].size,\n",
    "                               test[\"review\"].size,\n",
    "                               unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"2161_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"How many of us wish that we could throw away ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"4950_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Knowing when to end a movie is just as import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"4942_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I have to admit that for the first half hour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"668_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I just watched \\\"The Last Wave\\\" in my school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"8689_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Perfect cast for a few-person drama. Simon is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sentiment                                             review\n",
       "0  \"2161_10\"          1  \"How many of us wish that we could throw away ...\n",
       "1   \"4950_8\"          1  \"Knowing when to end a movie is just as import...\n",
       "2   \"4942_7\"          1  \"I have to admit that for the first half hour ...\n",
       "3    \"668_7\"          1  \"I just watched \\\"The Last Wave\\\" in my school...\n",
       "4   \"8689_7\"          1  \"Perfect cast for a few-person drama. Simon is..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1250 entries, 0 to 1249\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         1250 non-null   object\n",
      " 1   sentiment  1250 non-null   int64 \n",
      " 2   review     1250 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 29.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  1250.000000\n",
       "mean      0.519200\n",
       "std       0.499831\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: For Beginners - Bag of Words\n",
    "--\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words  \n",
    "\n",
    "What is NLP?\n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. This page will help you get started with loading and cleaning the IMDB movie reviews, then applying a simple Bag of Words model to get surprisingly accurate predictions of whether a review is thumbs-up or thumbs-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Cleaning_Utilities(object):\n",
    "    \"\"\"Tools for processing text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_wordlist(text, \n",
    "                         remove_stopwords=False, \n",
    "                         remove_html=False, \n",
    "                         remove_non_letters=False, \n",
    "                         steeming=False):\n",
    "        '''Split a text into a list of words'''\n",
    "        #text = text.replace('-\\n','')\n",
    "        text = text.lower()\n",
    "        if remove_html:\n",
    "            text = BeautifulSoup(text, \"html5lib\").get_text()\n",
    "        if remove_non_letters:\n",
    "            text = re.sub(\"[^-A-Za-z0-9_]\", \" \", text)\n",
    "        list_words = word_tokenize(text)\n",
    "        list_words = [w.strip(string.punctuation) for w in list_words if w not in string.punctuation]\n",
    "        list_words = [w for w in list_words if len(w) > 1]\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            list_words = [w for w in list_words if w not in stops]\n",
    "        if steeming:\n",
    "            stemmer = PorterStemmer()\n",
    "            list_words = [stemmer.stem(item) for item in list_words]\n",
    "        return list_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_to_list_of_texts(dataframe, column, \n",
    "                            remove_stopwords=False, \n",
    "                            remove_html=False, \n",
    "                            remove_non_letters=False, \n",
    "                            steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(' '.join(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                                 remove_stopwords=remove_stopwords,\n",
    "                                                                                 remove_html=remove_html,\n",
    "                                                                                 remove_non_letters=remove_non_letters,\n",
    "                                                                                 steeming=steeming)))\n",
    "            \n",
    "        return clean_texts\n",
    "\n",
    "    @staticmethod\n",
    "    def df_to_list_of_tokens(dataframe, column, \n",
    "                             remove_stopwords=False, \n",
    "                             remove_html=False, \n",
    "                             remove_non_letters=False, \n",
    "                             steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                        remove_stopwords=remove_stopwords,\n",
    "                                                                        remove_html=remove_html,\n",
    "                                                                        remove_non_letters=remove_non_letters,\n",
    "                                                                        steeming=steeming))\n",
    "            \n",
    "        return clean_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all the datasets and getting word lists\n",
    "--\n",
    "first set is without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(train, \n",
    "                                                                  'review', \n",
    "                                                                  remove_stopwords=True,\n",
    "                                                                  remove_html=True,)\n",
    "clean_test_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(test, \n",
    "                                                                 'review', \n",
    "                                                                 remove_stopwords=True,\n",
    "                                                                 remove_html=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'many us wish could throw away social cultural obligations free us suspect shall '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"huge fan original operation delta force thought pick film figured could n't bad \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second set mantains stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(train, \n",
    "                                                                     'review',\n",
    "                                                                     remove_html=True,)\n",
    "\n",
    "clean_test_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(test, \n",
    "                                                                    'review',\n",
    "                                                                    remove_html=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how many of us wish that we could throw away social and cultural obligations and'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as huge fan of the original operation delta force thought pick this film up figu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Features from a Bag of Words (Using scikit-learn)\n",
    "--\n",
    "\n",
    "Now that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "Sentence 1: \"The cat sat on the hat\"  \n",
    "Sentence 2: \"The dog ate the cat and the hat\"  \n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Sentence 1: [ 2, 1, 1, 1, 1, 0, 0, 0 ]\n",
    "\n",
    "Similarly, the features for Sentence 2 are: [ 3, 1, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
    "\n",
    "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features.  \n",
    "We will test two strategies: CountVectorizer (term frequecies - TF) and TFIDF Vectorizer:  \n",
    "First we'll start with plain word counts (TF):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer_tf = CountVectorizer(input='content', \n",
    "                               encoding='utf-8', \n",
    "                               decode_error='strict', \n",
    "                               strip_accents=None, \n",
    "                               lowercase=True, \n",
    "                               preprocessor=None, \n",
    "                               tokenizer=None, \n",
    "                               stop_words=None, \n",
    "                               #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                               ngram_range=(1, 2),\n",
    "                               analyzer='word', \n",
    "                               max_df=1.0, \n",
    "                               min_df=1, \n",
    "                               max_features=5000, \n",
    "                               vocabulary=None, \n",
    "                               binary=False, \n",
    "                               dtype=np.int64,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "second, it transforms our training data into feature vectors. \n",
    "The input to fit_transform should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tf = vectorizer_tf.fit_transform(clean_train_reviews)\n",
    "train_data_features_tf = train_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tf = vectorizer_tf.fit_transform(clean_test_reviews)\n",
    "test_data_features_tf = test_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use TfIDf vectors and the train/test cleaned reviews with stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(input='content',\n",
    "                                  #encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents=None,\n",
    "                                  lowercase=True,\n",
    "                                  preprocessor=None,\n",
    "                                  tokenizer=None,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words=None,\n",
    "                                  #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=5000,\n",
    "                                  vocabulary=None, \n",
    "                                  binary=False, \n",
    "                                  dtype=np.int64,\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_train_reviews_sw)\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_test_reviews_sw)\n",
    "test_data_features_tfidf = test_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing Train set for Cross Validation  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zygmuntz/classifying-text/blob/master/bow_validate.py  \n",
    "Alternatively, we can use the indexes to divide the train samples  \n",
    "\n",
    "train_i, test_i = train_test_split(np.arange(len(train)), train_size = 0.8, random_state = 44)  \n",
    "\n",
    "After generating indexes, we can divide ou datasets:  \n",
    "traincv = train_data_features1[train_i]  \n",
    "testcv = train_data_features1[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plain Word Counts\n",
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(train_data_features_tf,\n",
    "                                                                                        train[\"sentiment\"],\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "(X_traincv_tfidf, \n",
    " X_testcv_tfidf, \n",
    " y_traincv_tfidf, \n",
    " y_testcv_tfidf) = model_selection.train_test_split(train_data_features_tfidf,\n",
    "                                                    train[\"sentiment\"],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training some Classifiers  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning! Here, we'll use some classifiers implementations included in  the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_tf = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=0, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tf_tts = clf_RF_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_RF_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40333333, 0.59666667],\n",
       "       [0.49      , 0.51      ],\n",
       "       [0.69666667, 0.30333333],\n",
       "       [0.38666667, 0.61333333],\n",
       "       [0.28333333, 0.71666667],\n",
       "       [0.77666667, 0.22333333],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.59333333, 0.40666667],\n",
       "       [0.37333333, 0.62666667],\n",
       "       [0.33666667, 0.66333333],\n",
       "       [0.43666667, 0.56333333],\n",
       "       [0.60333333, 0.39666667],\n",
       "       [0.51666667, 0.48333333],\n",
       "       [0.55      , 0.45      ],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.47      , 0.53      ],\n",
       "       [0.73      , 0.27      ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.67333333, 0.32666667],\n",
       "       [0.43333333, 0.56666667],\n",
       "       [0.89      , 0.11      ],\n",
       "       [0.55666667, 0.44333333],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.23      , 0.77      ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.17333333, 0.82666667],\n",
       "       [0.61666667, 0.38333333],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.62666667, 0.37333333],\n",
       "       [0.72666667, 0.27333333],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.32666667, 0.67333333],\n",
       "       [0.73      , 0.27      ],\n",
       "       [0.42666667, 0.57333333],\n",
       "       [0.47      , 0.53      ],\n",
       "       [0.32666667, 0.67333333],\n",
       "       [0.50666667, 0.49333333],\n",
       "       [0.50666667, 0.49333333],\n",
       "       [0.39      , 0.61      ],\n",
       "       [0.32666667, 0.67333333],\n",
       "       [0.52666667, 0.47333333],\n",
       "       [0.59333333, 0.40666667],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.39      , 0.61      ],\n",
       "       [0.20333333, 0.79666667],\n",
       "       [0.61666667, 0.38333333],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.46      , 0.54      ],\n",
       "       [0.41      , 0.59      ],\n",
       "       [0.29666667, 0.70333333],\n",
       "       [0.85333333, 0.14666667],\n",
       "       [0.22666667, 0.77333333],\n",
       "       [0.23333333, 0.76666667],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.29333333, 0.70666667],\n",
       "       [0.16666667, 0.83333333],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.37333333, 0.62666667],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.78333333, 0.21666667],\n",
       "       [0.39      , 0.61      ],\n",
       "       [0.32666667, 0.67333333],\n",
       "       [0.83666667, 0.16333333],\n",
       "       [0.25      , 0.75      ],\n",
       "       [0.81666667, 0.18333333],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.56333333, 0.43666667],\n",
       "       [0.80333333, 0.19666667],\n",
       "       [0.62666667, 0.37333333],\n",
       "       [0.36333333, 0.63666667],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.43      , 0.57      ],\n",
       "       [0.36666667, 0.63333333],\n",
       "       [0.36666667, 0.63333333],\n",
       "       [0.39      , 0.61      ],\n",
       "       [0.55333333, 0.44666667],\n",
       "       [0.38      , 0.62      ],\n",
       "       [0.57333333, 0.42666667],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.57333333, 0.42666667],\n",
       "       [0.41      , 0.59      ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.41      , 0.59      ],\n",
       "       [0.70666667, 0.29333333],\n",
       "       [0.71      , 0.29      ],\n",
       "       [0.52333333, 0.47666667],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.67666667, 0.32333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.36333333, 0.63666667],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.12      , 0.88      ],\n",
       "       [0.92333333, 0.07666667],\n",
       "       [0.45      , 0.55      ],\n",
       "       [0.69      , 0.31      ],\n",
       "       [0.52333333, 0.47666667],\n",
       "       [0.55      , 0.45      ],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.53      , 0.47      ],\n",
       "       [0.29666667, 0.70333333],\n",
       "       [0.56666667, 0.43333333],\n",
       "       [0.58333333, 0.41666667],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.37666667, 0.62333333],\n",
       "       [0.40666667, 0.59333333],\n",
       "       [0.49333333, 0.50666667],\n",
       "       [0.46666667, 0.53333333],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.39333333, 0.60666667],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.33      , 0.67      ],\n",
       "       [0.39666667, 0.60333333],\n",
       "       [0.74666667, 0.25333333],\n",
       "       [0.63      , 0.37      ],\n",
       "       [0.77666667, 0.22333333],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.54666667, 0.45333333],\n",
       "       [0.50666667, 0.49333333],\n",
       "       [0.16      , 0.84      ],\n",
       "       [0.37666667, 0.62333333],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.24666667, 0.75333333],\n",
       "       [0.67666667, 0.32333333],\n",
       "       [0.31333333, 0.68666667],\n",
       "       [0.85      , 0.15      ],\n",
       "       [0.53      , 0.47      ],\n",
       "       [0.27666667, 0.72333333],\n",
       "       [0.69666667, 0.30333333],\n",
       "       [0.47333333, 0.52666667],\n",
       "       [0.21333333, 0.78666667],\n",
       "       [0.58666667, 0.41333333],\n",
       "       [0.31      , 0.69      ],\n",
       "       [0.24      , 0.76      ],\n",
       "       [0.13333333, 0.86666667],\n",
       "       [0.77333333, 0.22666667],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.45333333, 0.54666667],\n",
       "       [0.39666667, 0.60333333],\n",
       "       [0.68666667, 0.31333333],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.46333333, 0.53666667],\n",
       "       [0.30666667, 0.69333333],\n",
       "       [0.10666667, 0.89333333],\n",
       "       [0.48666667, 0.51333333],\n",
       "       [0.84333333, 0.15666667],\n",
       "       [0.11333333, 0.88666667],\n",
       "       [0.49333333, 0.50666667],\n",
       "       [0.64333333, 0.35666667],\n",
       "       [0.38333333, 0.61666667],\n",
       "       [0.59      , 0.41      ],\n",
       "       [0.40333333, 0.59666667],\n",
       "       [0.30333333, 0.69666667],\n",
       "       [0.39      , 0.61      ],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.56666667, 0.43333333],\n",
       "       [0.45      , 0.55      ],\n",
       "       [0.39333333, 0.60666667],\n",
       "       [0.57666667, 0.42333333],\n",
       "       [0.42      , 0.58      ],\n",
       "       [0.68666667, 0.31333333],\n",
       "       [0.32666667, 0.67333333],\n",
       "       [0.77333333, 0.22666667],\n",
       "       [0.69333333, 0.30666667],\n",
       "       [0.53333333, 0.46666667],\n",
       "       [0.67333333, 0.32666667],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.53333333, 0.46666667],\n",
       "       [0.44333333, 0.55666667],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.51666667, 0.48333333],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.32333333, 0.67666667],\n",
       "       [0.48333333, 0.51666667],\n",
       "       [0.42666667, 0.57333333],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.39333333, 0.60666667],\n",
       "       [0.63333333, 0.36666667],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.46666667, 0.53333333],\n",
       "       [0.86      , 0.14      ],\n",
       "       [0.71333333, 0.28666667],\n",
       "       [0.40666667, 0.59333333],\n",
       "       [0.44666667, 0.55333333],\n",
       "       [0.34      , 0.66      ],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.57333333, 0.42666667],\n",
       "       [0.12      , 0.88      ],\n",
       "       [0.68      , 0.32      ],\n",
       "       [0.74666667, 0.25333333],\n",
       "       [0.17333333, 0.82666667],\n",
       "       [0.73      , 0.27      ],\n",
       "       [0.31      , 0.69      ],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.63666667, 0.36333333],\n",
       "       [0.29333333, 0.70666667],\n",
       "       [0.33666667, 0.66333333],\n",
       "       [0.15333333, 0.84666667],\n",
       "       [0.33      , 0.67      ],\n",
       "       [0.53666667, 0.46333333],\n",
       "       [0.16666667, 0.83333333],\n",
       "       [0.38666667, 0.61333333],\n",
       "       [0.67333333, 0.32666667],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.40333333, 0.59666667],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.72      , 0.28      ],\n",
       "       [0.57333333, 0.42666667],\n",
       "       [0.55333333, 0.44666667],\n",
       "       [0.28666667, 0.71333333],\n",
       "       [0.38      , 0.62      ],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.33666667, 0.66333333],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.46333333, 0.53666667],\n",
       "       [0.28333333, 0.71666667],\n",
       "       [0.55      , 0.45      ],\n",
       "       [0.63333333, 0.36666667],\n",
       "       [0.11333333, 0.88666667],\n",
       "       [0.42      , 0.58      ],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.28333333, 0.71666667],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.51666667, 0.48333333],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.49333333, 0.50666667],\n",
       "       [0.64666667, 0.35333333],\n",
       "       [0.73666667, 0.26333333],\n",
       "       [0.40333333, 0.59666667],\n",
       "       [0.34666667, 0.65333333],\n",
       "       [0.76666667, 0.23333333],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.28333333, 0.71666667],\n",
       "       [0.53      , 0.47      ],\n",
       "       [0.73333333, 0.26666667],\n",
       "       [0.71      , 0.29      ],\n",
       "       [0.24      , 0.76      ],\n",
       "       [0.36333333, 0.63666667],\n",
       "       [0.59      , 0.41      ],\n",
       "       [0.80333333, 0.19666667],\n",
       "       [0.21      , 0.79      ],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.91666667, 0.08333333],\n",
       "       [0.65333333, 0.34666667],\n",
       "       [0.67      , 0.33      ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tf.predict_proba(X_testcv_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train on the TfIdf samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_tfidf = RandomForestClassifier(n_estimators=300, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth=None, \n",
    "                                      min_samples_split=2, \n",
    "                                      min_samples_leaf=1, \n",
    "                                      min_weight_fraction_leaf=0.0, \n",
    "                                      max_features='auto', \n",
    "                                      max_leaf_nodes=None, \n",
    "                                      bootstrap=False, \n",
    "                                      oob_score=False, \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=0, \n",
    "                                      verbose=0, \n",
    "                                      warm_start=False, \n",
    "                                      class_weight=None).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tfidf_tts = clf_RF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_RF_tfidf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56      , 0.44      ],\n",
       "       [0.44      , 0.56      ],\n",
       "       [0.64333333, 0.35666667],\n",
       "       [0.28666667, 0.71333333],\n",
       "       [0.53333333, 0.46666667],\n",
       "       [0.64666667, 0.35333333],\n",
       "       [0.37333333, 0.62666667],\n",
       "       [0.47333333, 0.52666667],\n",
       "       [0.31      , 0.69      ],\n",
       "       [0.34666667, 0.65333333],\n",
       "       [0.53      , 0.47      ],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.53      , 0.47      ],\n",
       "       [0.47333333, 0.52666667],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.50333333, 0.49666667],\n",
       "       [0.67      , 0.33      ],\n",
       "       [0.74333333, 0.25666667],\n",
       "       [0.62666667, 0.37333333],\n",
       "       [0.54      , 0.46      ],\n",
       "       [0.84666667, 0.15333333],\n",
       "       [0.33666667, 0.66333333],\n",
       "       [0.60666667, 0.39333333],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.51      , 0.49      ],\n",
       "       [0.65      , 0.35      ],\n",
       "       [0.33      , 0.67      ],\n",
       "       [0.57      , 0.43      ],\n",
       "       [0.45333333, 0.54666667],\n",
       "       [0.50333333, 0.49666667],\n",
       "       [0.54333333, 0.45666667],\n",
       "       [0.34333333, 0.65666667],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.58666667, 0.41333333],\n",
       "       [0.34      , 0.66      ],\n",
       "       [0.58666667, 0.41333333],\n",
       "       [0.41666667, 0.58333333],\n",
       "       [0.45      , 0.55      ],\n",
       "       [0.51666667, 0.48333333],\n",
       "       [0.49      , 0.51      ],\n",
       "       [0.18      , 0.82      ],\n",
       "       [0.42333333, 0.57666667],\n",
       "       [0.59      , 0.41      ],\n",
       "       [0.46333333, 0.53666667],\n",
       "       [0.38666667, 0.61333333],\n",
       "       [0.27      , 0.73      ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.45      , 0.55      ],\n",
       "       [0.52666667, 0.47333333],\n",
       "       [0.29      , 0.71      ],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.75333333, 0.24666667],\n",
       "       [0.27666667, 0.72333333],\n",
       "       [0.22666667, 0.77333333],\n",
       "       [0.23      , 0.77      ],\n",
       "       [0.55666667, 0.44333333],\n",
       "       [0.29666667, 0.70333333],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.47333333, 0.52666667],\n",
       "       [0.59333333, 0.40666667],\n",
       "       [0.42333333, 0.57666667],\n",
       "       [0.42      , 0.58      ],\n",
       "       [0.69333333, 0.30666667],\n",
       "       [0.42333333, 0.57666667],\n",
       "       [0.48666667, 0.51333333],\n",
       "       [0.63333333, 0.36666667],\n",
       "       [0.44333333, 0.55666667],\n",
       "       [0.69333333, 0.30666667],\n",
       "       [0.40333333, 0.59666667],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.82333333, 0.17666667],\n",
       "       [0.69      , 0.31      ],\n",
       "       [0.42333333, 0.57666667],\n",
       "       [0.44      , 0.56      ],\n",
       "       [0.43      , 0.57      ],\n",
       "       [0.31      , 0.69      ],\n",
       "       [0.35666667, 0.64333333],\n",
       "       [0.45333333, 0.54666667],\n",
       "       [0.66333333, 0.33666667],\n",
       "       [0.32333333, 0.67666667],\n",
       "       [0.41      , 0.59      ],\n",
       "       [0.27666667, 0.72333333],\n",
       "       [0.54      , 0.46      ],\n",
       "       [0.43666667, 0.56333333],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.47666667, 0.52333333],\n",
       "       [0.68333333, 0.31666667],\n",
       "       [0.68      , 0.32      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.48333333, 0.51666667],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.67333333, 0.32666667],\n",
       "       [0.65333333, 0.34666667],\n",
       "       [0.54      , 0.46      ],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.39333333, 0.60666667],\n",
       "       [0.59666667, 0.40333333],\n",
       "       [0.53333333, 0.46666667],\n",
       "       [0.56333333, 0.43666667],\n",
       "       [0.30666667, 0.69333333],\n",
       "       [0.64333333, 0.35666667],\n",
       "       [0.34333333, 0.65666667],\n",
       "       [0.57666667, 0.42333333],\n",
       "       [0.6       , 0.4       ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.55666667, 0.44333333],\n",
       "       [0.45      , 0.55      ],\n",
       "       [0.42      , 0.58      ],\n",
       "       [0.46      , 0.54      ],\n",
       "       [0.48666667, 0.51333333],\n",
       "       [0.41      , 0.59      ],\n",
       "       [0.54      , 0.46      ],\n",
       "       [0.28333333, 0.71666667],\n",
       "       [0.44      , 0.56      ],\n",
       "       [0.75      , 0.25      ],\n",
       "       [0.62666667, 0.37333333],\n",
       "       [0.69      , 0.31      ],\n",
       "       [0.32666667, 0.67333333],\n",
       "       [0.65333333, 0.34666667],\n",
       "       [0.54666667, 0.45333333],\n",
       "       [0.38333333, 0.61666667],\n",
       "       [0.27666667, 0.72333333],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.40666667, 0.59333333],\n",
       "       [0.65666667, 0.34333333],\n",
       "       [0.47666667, 0.52333333],\n",
       "       [0.72      , 0.28      ],\n",
       "       [0.58666667, 0.41333333],\n",
       "       [0.43333333, 0.56666667],\n",
       "       [0.62666667, 0.37333333],\n",
       "       [0.37666667, 0.62333333],\n",
       "       [0.36333333, 0.63666667],\n",
       "       [0.70333333, 0.29666667],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.27666667, 0.72333333],\n",
       "       [0.31333333, 0.68666667],\n",
       "       [0.72666667, 0.27333333],\n",
       "       [0.38333333, 0.61666667],\n",
       "       [0.59      , 0.41      ],\n",
       "       [0.50666667, 0.49333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.68      , 0.32      ],\n",
       "       [0.35      , 0.65      ],\n",
       "       [0.39      , 0.61      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.48666667, 0.51333333],\n",
       "       [0.76666667, 0.23333333],\n",
       "       [0.11666667, 0.88333333],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.54333333, 0.45666667],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.52333333, 0.47666667],\n",
       "       [0.34      , 0.66      ],\n",
       "       [0.30333333, 0.69666667],\n",
       "       [0.34      , 0.66      ],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.54333333, 0.45666667],\n",
       "       [0.35333333, 0.64666667],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.49666667, 0.50333333],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.63333333, 0.36666667],\n",
       "       [0.45      , 0.55      ],\n",
       "       [0.72333333, 0.27666667],\n",
       "       [0.77      , 0.23      ],\n",
       "       [0.51      , 0.49      ],\n",
       "       [0.77      , 0.23      ],\n",
       "       [0.40333333, 0.59666667],\n",
       "       [0.46666667, 0.53333333],\n",
       "       [0.46      , 0.54      ],\n",
       "       [0.65333333, 0.34666667],\n",
       "       [0.6       , 0.4       ],\n",
       "       [0.56666667, 0.43333333],\n",
       "       [0.35666667, 0.64333333],\n",
       "       [0.48666667, 0.51333333],\n",
       "       [0.57      , 0.43      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.30666667, 0.69333333],\n",
       "       [0.61666667, 0.38333333],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.37666667, 0.62333333],\n",
       "       [0.81666667, 0.18333333],\n",
       "       [0.63666667, 0.36333333],\n",
       "       [0.31333333, 0.68666667],\n",
       "       [0.48333333, 0.51666667],\n",
       "       [0.54      , 0.46      ],\n",
       "       [0.20666667, 0.79333333],\n",
       "       [0.62333333, 0.37666667],\n",
       "       [0.27      , 0.73      ],\n",
       "       [0.75333333, 0.24666667],\n",
       "       [0.70333333, 0.29666667],\n",
       "       [0.35666667, 0.64333333],\n",
       "       [0.63      , 0.37      ],\n",
       "       [0.41666667, 0.58333333],\n",
       "       [0.61      , 0.39      ],\n",
       "       [0.69666667, 0.30333333],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.45666667, 0.54333333],\n",
       "       [0.30666667, 0.69333333],\n",
       "       [0.35      , 0.65      ],\n",
       "       [0.34666667, 0.65333333],\n",
       "       [0.25      , 0.75      ],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.43333333, 0.56666667],\n",
       "       [0.57333333, 0.42666667],\n",
       "       [0.21666667, 0.78333333],\n",
       "       [0.61      , 0.39      ],\n",
       "       [0.48666667, 0.51333333],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.41      , 0.59      ],\n",
       "       [0.39333333, 0.60666667],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.24      , 0.76      ],\n",
       "       [0.61333333, 0.38666667],\n",
       "       [0.25666667, 0.74333333],\n",
       "       [0.55      , 0.45      ],\n",
       "       [0.42666667, 0.57333333],\n",
       "       [0.54666667, 0.45333333],\n",
       "       [0.66333333, 0.33666667],\n",
       "       [0.35      , 0.65      ],\n",
       "       [0.34666667, 0.65333333],\n",
       "       [0.23      , 0.77      ],\n",
       "       [0.46      , 0.54      ],\n",
       "       [0.59666667, 0.40333333],\n",
       "       [0.44666667, 0.55333333],\n",
       "       [0.65      , 0.35      ],\n",
       "       [0.30666667, 0.69333333],\n",
       "       [0.54333333, 0.45666667],\n",
       "       [0.67      , 0.33      ],\n",
       "       [0.69      , 0.31      ],\n",
       "       [0.21333333, 0.78666667],\n",
       "       [0.39666667, 0.60333333],\n",
       "       [0.71      , 0.29      ],\n",
       "       [0.25333333, 0.74666667],\n",
       "       [0.27333333, 0.72666667],\n",
       "       [0.64333333, 0.35666667],\n",
       "       [0.65      , 0.35      ],\n",
       "       [0.57666667, 0.42333333],\n",
       "       [0.21333333, 0.78666667],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.67      , 0.33      ],\n",
       "       [0.66333333, 0.33666667],\n",
       "       [0.27666667, 0.72333333],\n",
       "       [0.57666667, 0.42333333],\n",
       "       [0.82666667, 0.17333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.60333333, 0.39666667]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tfidf.predict_proba(X_testcv_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=0,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_tts = clf_LR_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR_tfidf = LR(penalty='l2',\n",
    "                  dual=False,\n",
    "                  tol=0.0001,\n",
    "                  C=1.0,\n",
    "                  fit_intercept=True,\n",
    "                  intercept_scaling=1,\n",
    "                  class_weight=None,\n",
    "                  random_state=0,\n",
    "                  solver='liblinear',\n",
    "                  max_iter=100,\n",
    "                  multi_class='ovr',\n",
    "                  verbose=0).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tfidf_tts = clf_LR_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_LR_tfidf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_results = {'RandomForest_TF': eval_RF_tf_tts,\n",
    "               'RandomForest_TFIDF': eval_RF_tfidf_tts,\n",
    "               'LogiReg_TF': eval_LR_tf_tts,\n",
    "               'LogiReg_TFIDF': eval_LR_tfidf_tts,\n",
    "              }\n",
    "\n",
    "import operator\n",
    "tup_results = sorted(dic_results.items(), key=operator.itemgetter(1))\n",
    "\n",
    "N = len(dic_results)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.40       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects = ax.bar(ind, list(zip(*tup_results))[1], width,)\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., \n",
    "            1.005*height, \n",
    "            '{0:.4f}'.format(height), \n",
    "            ha='center', \n",
    "            va='bottom',)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(ymin=0.78,ymax = 0.9)\n",
    "ax.set_title(\"Classificators' performance\")\n",
    "ax.set_xticks(ind + width/2.)\n",
    "ax.set_xticklabels(list(zip(*tup_results))[0], rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--\n",
    "\n",
    "All that remains is to run the best classifier on our test set and create a submission file. If you haven't already done so, download testData.tsv from the Data page. This file contains another 25,000 reviews and ids; our task is to predict the sentiment label.\n",
    "\n",
    "Note that when we use the Bag of Words for the test set, we only call \"transform\", not \"fit_transform\" as we did for the training set. In machine learning, you shouldn't use the test set to fit your model, otherwise you run the risk of overfitting. For this reason, we keep the test set off-limits until we are ready to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1250 entries, 0 to 1249\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      1250 non-null   object\n",
      " 1   review  1250 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 19.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5203_1\"</td>\n",
       "      <td>\"As a huge fan of the original Operation Delta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"11917_10\"</td>\n",
       "      <td>\"oh boy !!! my god !!!! what a movie this one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"2973_8\"</td>\n",
       "      <td>\"I too saw this movie when it first came out. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"5671_3\"</td>\n",
       "      <td>\"I won't say the show is all bad, because ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"11566_1\"</td>\n",
       "      <td>\"In fact it was awful. The main chick in it wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0    \"5203_1\"  \"As a huge fan of the original Operation Delta...\n",
       "1  \"11917_10\"  \"oh boy !!! my god !!!! what a movie this one ...\n",
       "2    \"2973_8\"  \"I too saw this movie when it first came out. ...\n",
       "3    \"5671_3\"  \"I won't say the show is all bad, because ther...\n",
       "4   \"11566_1\"  \"In fact it was awful. The main chick in it wh..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 5000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logistic regression with tfidf vectors to make sentiment label predictions\n",
    "result = clf_LR_tfidf.predict(test_data_features_tfidf)\n",
    "result_prob = clf_LR_tfidf.predict_proba(test_data_features_tfidf)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv('./data/LR_tfidf_model.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5203_1\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"11917_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"2973_8\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"5671_3\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"11566_1\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0    \"5203_1\"          1\n",
       "1  \"11917_10\"          1\n",
       "2    \"2973_8\"          1\n",
       "3    \"5671_3\"          1\n",
       "4   \"11566_1\"          1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Alternative Vectors\n",
    "--\n",
    "\n",
    "In the subsequent sections, we are going to explore alternate ways to codify text into vectors. We are going to explore three techniques, namely Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA) and Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling and Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the models, the more we have texts, the better. The size of the Corpus is essential for having good results. We don't need labels in order to create the models, so we will use the train examples and also some unlabeled reviews. The list of cleaned sentences will be used for all the subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Parsing sentences from test set\n",
      "Parsing sentences with stopwords from training set\n",
      "Parsing sentences with stopwords from unlabeled set\n",
      "Parsing sentences with stopwords from test set\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "labeled_sentences = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                                 'review', \n",
    "                                                                 remove_html=True,\n",
    "                                                                 remove_stopwords=True,)\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "all_sentences = labeled_sentences + Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                                                 'review', \n",
    "                                                                                 remove_html=True,\n",
    "                                                                                 remove_stopwords=True,)\n",
    "\n",
    "print(\"Parsing sentences from test set\")\n",
    "test_labeled_sentences = Text_Cleaning_Utilities.df_to_list_of_tokens(test,\n",
    "                                                                      'review', \n",
    "                                                                      remove_html=True,\n",
    "                                                                      remove_stopwords=True,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from training set\")\n",
    "labeled_sentences_sw = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                                    'review', \n",
    "                                                                    remove_html=True,\n",
    "                                                                    remove_stopwords=False,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from unlabeled set\")\n",
    "all_sentences_sw = labeled_sentences_sw + Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                                                       'review', \n",
    "                                                                                       remove_html=True,\n",
    "                                                                                       remove_stopwords=False,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from test set\")\n",
    "test_labeled_sentences_sw = Text_Cleaning_Utilities.df_to_list_of_tokens(test,\n",
    "                                                                         'review', \n",
    "                                                                         remove_html=True,\n",
    "                                                                         remove_stopwords=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "3750\n",
      "1250\n",
      "1250\n",
      "3750\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print(len(labeled_sentences))\n",
    "print(len(all_sentences))\n",
    "print(len(test_labeled_sentences))\n",
    "print(len(labeled_sentences_sw))\n",
    "print(len(all_sentences_sw))\n",
    "print(len(test_labeled_sentences_sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=4, no_above=0.7):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    print('{} Tokens extracted from {} texts'.format(len(dictionary.keys()), dictionary.num_docs))\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    #stopword_ids = [dictionary.token2id[sw] for sw in stopwords if sw in dictionary.token2id]\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    #low_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < 4]\n",
    "    #dictionary.filter_tokens(low_freq_ids)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    print('{} Tokens after cleaning'.format(len(dictionary.keys())))\n",
    "    #print('Building corpus...')\n",
    "    #corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    return dictionary #, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compacting and saving the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "43254 Tokens extracted from 3750 texts\n",
      "10765 Tokens after cleaning\n",
      "dictionary done\n",
      "dictionary saved\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords=set(['n\\'t', 'movie'])\n",
    "\n",
    "dictionary = prep_corpus(all_sentences, additional_stopwords)\n",
    "dictionary.compactify()\n",
    "print('dictionary done')\n",
    "\n",
    "dictionary.save('./data/reviews.dict')\n",
    "print('dictionary saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "#print(dictionary.token2id['movie']) #verify if these words were in the stopwords list\n",
    "#print(dictionary.token2id['n\\'t'])\n",
    "print(dictionary.token2id['like'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.dictionary.Dictionary.load('./data/reviews.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Corpora (tf and tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus tf done\n",
      "corpus tfidf done\n"
     ]
    }
   ],
   "source": [
    "corpus_tf = [dictionary.doc2bow(sentence) for sentence in all_sentences]\n",
    "print('corpus tf done')\n",
    "tfidf = models.TfidfModel(corpus_tf)\n",
    "corpus_tfidf = tfidf[corpus_tf]\n",
    "print('corpus tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing\n",
    "https://en.wikipedia.org/wiki/Latent_semantic_analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lsi - TF done\n",
      "corpus lsi - TFIDF done\n"
     ]
    }
   ],
   "source": [
    "lsi_tf = models.LsiModel(corpus_tf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tf = lsi_tf[corpus_tf]\n",
    "print('corpus lsi - TF done')\n",
    "lsi_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tfidf = lsi_tfidf[corpus_tfidf]\n",
    "print('corpus lsi - TFIDF done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.513*\"film\" + 0.287*\"one\" + 0.218*\"like\" + 0.143*\"would\" + 0.143*\"good\" + 0.131*\"even\" + 0.123*\"really\" + 0.123*\"time\" + 0.115*\"story\" + 0.112*\"see\"'),\n",
       " (1,\n",
       "  '-0.821*\"film\" + 0.198*\"one\" + 0.187*\"like\" + 0.122*\"show\" + 0.111*\"movies\" + 0.098*\"good\" + 0.097*\"get\" + 0.096*\"would\" + 0.093*\"really\" + 0.090*\"even\"'),\n",
       " (2,\n",
       "  '0.699*\"one\" + -0.503*\"like\" + -0.160*\"would\" + -0.124*\"people\" + -0.118*\"really\" + -0.115*\"bad\" + 0.085*\"two\" + -0.083*\"see\" + -0.082*\"good\" + 0.076*\"time\"'),\n",
       " (3,\n",
       "  '-0.422*\"one\" + -0.328*\"bad\" + -0.307*\"like\" + 0.245*\"story\" + 0.180*\"life\" + 0.177*\"would\" + -0.166*\"really\" + -0.152*\"movies\" + -0.116*\"good\" + 0.108*\"man\"'),\n",
       " (4,\n",
       "  '0.567*\"good\" + -0.285*\"like\" + 0.267*\"story\" + 0.204*\"really\" + -0.186*\"people\" + 0.178*\"great\" + -0.175*\"would\" + -0.164*\"one\" + 0.142*\"bad\" + 0.134*\"characters\"'),\n",
       " (5,\n",
       "  '0.482*\"like\" + -0.290*\"bad\" + -0.254*\"would\" + 0.234*\"sam\" + 0.217*\"dean\" + -0.187*\"show\" + 0.182*\"story\" + -0.170*\"even\" + 0.159*\"great\" + 0.157*\"also\"'),\n",
       " (6,\n",
       "  '-0.393*\"show\" + 0.354*\"sam\" + 0.350*\"dean\" + -0.236*\"like\" + 0.209*\"good\" + 0.201*\"would\" + 0.164*\"john\" + 0.156*\"bad\" + 0.155*\"could\" + -0.153*\"people\"'),\n",
       " (7,\n",
       "  '0.649*\"show\" + 0.279*\"sam\" + -0.263*\"story\" + 0.261*\"dean\" + 0.191*\"good\" + -0.119*\"movies\" + 0.118*\"shows\" + 0.115*\"gordon\" + 0.115*\"john\" + -0.111*\"bad\"'),\n",
       " (8,\n",
       "  '0.518*\"people\" + 0.372*\"really\" + 0.263*\"great\" + -0.231*\"like\" + 0.173*\"see\" + -0.172*\"even\" + 0.168*\"movies\" + -0.141*\"would\" + -0.133*\"character\" + -0.120*\"could\"'),\n",
       " (9,\n",
       "  '0.526*\"good\" + -0.391*\"really\" + -0.251*\"story\" + 0.248*\"people\" + -0.221*\"even\" + -0.167*\"show\" + -0.156*\"bad\" + 0.152*\"first\" + -0.136*\"could\" + 0.135*\"would\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', -0.8214762984099792),\n",
       " ('one', 0.19786518491243515),\n",
       " ('like', 0.18672354325107413),\n",
       " ('show', 0.1222467647940966),\n",
       " ('movies', 0.11122048853542961),\n",
       " ('good', 0.09834676873649496),\n",
       " ('get', 0.09744504677016358),\n",
       " ('would', 0.0961710905833087),\n",
       " ('really', 0.0931232182114836),\n",
       " ('even', 0.08991220448565104)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.152*\"film\" + 0.101*\"good\" + 0.100*\"like\" + 0.098*\"really\" + 0.098*\"bad\" + 0.093*\"one\" + 0.090*\"would\" + 0.088*\"story\" + 0.087*\"see\" + 0.087*\"great\"'),\n",
       " (1,\n",
       "  '-0.269*\"bad\" + -0.188*\"worst\" + -0.116*\"horrible\" + -0.116*\"ever\" + -0.112*\"funny\" + -0.109*\"acting\" + -0.105*\"movies\" + -0.102*\"waste\" + -0.099*\"terrible\" + -0.095*\"watch\"'),\n",
       " (2,\n",
       "  '0.617*\"show\" + 0.247*\"series\" + 0.174*\"episode\" + 0.160*\"tv\" + -0.160*\"film\" + 0.144*\"episodes\" + 0.142*\"shows\" + -0.140*\"horror\" + 0.115*\"season\" + -0.104*\"bad\"'),\n",
       " (3,\n",
       "  '0.276*\"great\" + 0.251*\"book\" + -0.214*\"show\" + 0.116*\"film\" + 0.115*\"actors\" + -0.115*\"horror\" + 0.111*\"read\" + 0.100*\"movies\" + 0.100*\"wonderful\" + -0.097*\"killer\"'),\n",
       " (4,\n",
       "  '0.267*\"horror\" + -0.250*\"book\" + 0.174*\"series\" + -0.140*\"life\" + -0.139*\"people\" + 0.135*\"effects\" + -0.119*\"read\" + 0.114*\"special\" + 0.112*\"great\" + 0.109*\"original\"'),\n",
       " (5,\n",
       "  '-0.444*\"book\" + 0.266*\"funny\" + 0.246*\"comedy\" + -0.225*\"series\" + -0.209*\"read\" + -0.121*\"original\" + -0.114*\"books\" + 0.108*\"great\" + 0.098*\"laugh\" + -0.095*\"game\"'),\n",
       " (6,\n",
       "  '-0.191*\"game\" + 0.188*\"book\" + 0.178*\"show\" + -0.160*\"horror\" + 0.159*\"bad\" + -0.151*\"great\" + -0.142*\"love\" + 0.140*\"comedy\" + 0.122*\"funny\" + 0.120*\"worst\"'),\n",
       " (7,\n",
       "  '-0.306*\"game\" + 0.297*\"book\" + 0.176*\"funny\" + -0.164*\"ever\" + -0.141*\"worst\" + 0.132*\"really\" + -0.121*\"film\" + -0.120*\"seen\" + 0.118*\"horror\" + -0.115*\"music\"'),\n",
       " (8,\n",
       "  '-0.518*\"game\" + 0.179*\"horror\" + 0.179*\"show\" + 0.160*\"film\" + -0.124*\"games\" + -0.120*\"comedy\" + -0.120*\"book\" + 0.117*\"films\" + -0.109*\"funny\" + -0.107*\"series\"'),\n",
       " (9,\n",
       "  '-0.245*\"game\" + 0.171*\"horror\" + 0.162*\"series\" + 0.157*\"ever\" + -0.148*\"show\" + -0.143*\"characters\" + 0.141*\"book\" + 0.135*\"movies\" + 0.135*\"seen\" + 0.135*\"worst\"')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', -0.2689210810364485),\n",
       " ('worst', -0.1884212522723442),\n",
       " ('horrible', -0.1164308197360946),\n",
       " ('ever', -0.11594577078959217),\n",
       " ('funny', -0.11171900178091068),\n",
       " ('acting', -0.10909461075508625),\n",
       " ('movies', -0.10450863750715143),\n",
       " ('waste', -0.10239010208408474),\n",
       " ('terrible', -0.09910889747801704),\n",
       " ('watch', -0.09480869056710979)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lda tf done\n",
      "corpus lda tfidf done\n"
     ]
    }
   ],
   "source": [
    "lda_tf = models.LdaModel(corpus_tf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf = lda_tf[corpus_tf]\n",
    "print('corpus lda tf done')\n",
    "\n",
    "lda_tfidf = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tfidf = lda_tfidf[corpus_tfidf]\n",
    "print('corpus lda tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"film\" + 0.010*\"one\" + 0.006*\"would\" + 0.005*\"like\" + 0.005*\"get\" + 0.004*\"see\" + 0.003*\"first\"'),\n",
       " (1,\n",
       "  '0.009*\"man\" + 0.006*\"sam\" + 0.005*\"woman\" + 0.005*\"series\" + 0.005*\"murder\" + 0.004*\"one\" + 0.004*\"young\"'),\n",
       " (2,\n",
       "  '0.016*\"like\" + 0.014*\"one\" + 0.012*\"good\" + 0.012*\"bad\" + 0.010*\"really\" + 0.010*\"would\" + 0.009*\"see\"'),\n",
       " (3,\n",
       "  '0.020*\"film\" + 0.009*\"one\" + 0.007*\"story\" + 0.004*\"films\" + 0.004*\"great\" + 0.004*\"good\" + 0.004*\"like\"'),\n",
       " (4,\n",
       "  '0.007*\"one\" + 0.006*\"action\" + 0.006*\"film\" + 0.005*\"also\" + 0.005*\"even\" + 0.005*\"well\" + 0.004*\"like\"'),\n",
       " (5,\n",
       "  '0.025*\"series\" + 0.015*\"crawford\" + 0.012*\"joan\" + 0.010*\"arthur\" + 0.005*\"graham\" + 0.005*\"story\" + 0.005*\"anna\"'),\n",
       " (6,\n",
       "  '0.010*\"one\" + 0.007*\"film\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"woman\" + 0.004*\"good\" + 0.004*\"cop\"'),\n",
       " (7,\n",
       "  '0.019*\"film\" + 0.008*\"one\" + 0.008*\"like\" + 0.005*\"good\" + 0.005*\"much\" + 0.005*\"would\" + 0.005*\"even\"'),\n",
       " (8,\n",
       "  '0.013*\"film\" + 0.010*\"one\" + 0.006*\"story\" + 0.005*\"life\" + 0.004*\"like\" + 0.004*\"even\" + 0.004*\"love\"'),\n",
       " (9,\n",
       "  '0.029*\"film\" + 0.010*\"one\" + 0.008*\"great\" + 0.005*\"would\" + 0.005*\"films\" + 0.004*\"well\" + 0.004*\"really\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.print_topics(num_topics=10, num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('series', 0.024933074),\n",
       " ('crawford', 0.015101903),\n",
       " ('joan', 0.012061813),\n",
       " ('arthur', 0.009797324),\n",
       " ('graham', 0.0054370407),\n",
       " ('story', 0.0053737),\n",
       " ('anna', 0.004551899),\n",
       " ('powell', 0.00439501),\n",
       " ('young', 0.0039202236),\n",
       " ('holmes', 0.0038365459)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.show_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"gay\" + 0.002*\"clich\" + 0.002*\"murphy\" + 0.002*\"shocking\" + 0.002*\"max\" + 0.002*\"matt\" + 0.002*\"massacre\"'),\n",
       " (1,\n",
       "  '0.003*\"film\" + 0.002*\"good\" + 0.002*\"like\" + 0.002*\"one\" + 0.002*\"bad\" + 0.002*\"really\" + 0.002*\"great\"'),\n",
       " (2,\n",
       "  '0.002*\"beach\" + 0.002*\"welles\" + 0.001*\"rocket\" + 0.001*\"bell\" + 0.001*\"singing\" + 0.001*\"soldier\" + 0.001*\"deniro\"'),\n",
       " (3,\n",
       "  '0.002*\"smith\" + 0.002*\"sandler\" + 0.002*\"wasted\" + 0.002*\"military\" + 0.002*\"teen\" + 0.002*\"laughed\" + 0.002*\"al\"'),\n",
       " (4,\n",
       "  '0.002*\"ninja\" + 0.002*\"moon\" + 0.002*\"dolls\" + 0.002*\"addams\" + 0.002*\"mcdowell\" + 0.002*\"marlene\" + 0.002*\"someday\"'),\n",
       " (5,\n",
       "  '0.002*\"crawford\" + 0.002*\"indie\" + 0.002*\"hindi\" + 0.001*\"previews\" + 0.001*\"notes\" + 0.001*\"huston\" + 0.001*\"flesh\"'),\n",
       " (6,\n",
       "  '0.002*\"fairbanks\" + 0.001*\"kidnap\" + 0.001*\"rubin\" + 0.001*\"bunuel\" + 0.001*\"institute\" + 0.001*\"waited\" + 0.001*\"african-american\"'),\n",
       " (7,\n",
       "  '0.002*\"roberts\" + 0.002*\"dutch\" + 0.002*\"carole\" + 0.002*\"poirot\" + 0.002*\"lombard\" + 0.001*\"crawford\" + 0.001*\"ginger\"'),\n",
       " (8,\n",
       "  '0.002*\"hitler\" + 0.002*\"superman\" + 0.002*\"interaction\" + 0.001*\"godzilla\" + 0.001*\"1950s\" + 0.001*\"happiness\" + 0.001*\"robots\"'),\n",
       " (9,\n",
       "  '0.003*\"seagal\" + 0.002*\"che\" + 0.002*\"hepburn\" + 0.001*\"chad\" + 0.001*\"ladd\" + 0.001*\"bravo\" + 0.001*\"letters\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.print_topics(num_topics=10, num_words=7)\n",
    "#lda_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('gay', 0.003632257),\n",
       "   ('clich', 0.0022078815),\n",
       "   ('murphy', 0.0020703946),\n",
       "   ('shocking', 0.0019272104),\n",
       "   ('max', 0.0016423697),\n",
       "   ('matt', 0.0015288289),\n",
       "   ('massacre', 0.0015284921),\n",
       "   ('julia', 0.0015215717),\n",
       "   ('teacher', 0.0015042572),\n",
       "   ('creative', 0.0014460043),\n",
       "   ('sarah', 0.0014029263),\n",
       "   ('comedies', 0.001339309),\n",
       "   ('britain', 0.001329276),\n",
       "   ('somebody', 0.0013226969),\n",
       "   ('mitchum', 0.0013208727),\n",
       "   ('mummy', 0.0013148345),\n",
       "   ('gold', 0.0013057688),\n",
       "   ('holmes', 0.0013004207),\n",
       "   ('i.e', 0.0012848581),\n",
       "   ('twisted', 0.0012435127)]),\n",
       " (1,\n",
       "  [('film', 0.003372825),\n",
       "   ('good', 0.002232563),\n",
       "   ('like', 0.0022049558),\n",
       "   ('one', 0.00211721),\n",
       "   ('bad', 0.0021093637),\n",
       "   ('really', 0.0020821197),\n",
       "   ('great', 0.0020334134),\n",
       "   ('story', 0.002023062),\n",
       "   ('would', 0.0019687964),\n",
       "   ('see', 0.0019434566),\n",
       "   ('time', 0.0018623293),\n",
       "   ('even', 0.0018329847),\n",
       "   ('could', 0.0018249645),\n",
       "   ('people', 0.0018214819),\n",
       "   ('movies', 0.0017998168),\n",
       "   ('show', 0.0017977534),\n",
       "   ('well', 0.0017299608),\n",
       "   ('think', 0.0017036616),\n",
       "   ('much', 0.001697986),\n",
       "   ('get', 0.0016966584)])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.show_topics(formatted=False, num_words=20)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing using PyLDAvis  \n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1759140533210849104471899557\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1759140533210849104471899557_data = {\"mdsDat\": {\"x\": [0.041062120045440686, 0.15234907628770397, 0.04675411406030467, 0.03633734505600434, 0.03230160757324232, 0.028883865692429907, 0.046981801881323594, -0.004028176581215545, -0.17588806686550168, -0.204753687149732], \"y\": [0.005835665789192857, 0.019473890967453767, 0.014814399889257795, 0.005150702120269398, -0.0011350116340889155, -0.0032291386566015985, 0.027690726489003132, -0.07209168854826252, -0.16743078351346427, 0.17092123709724044], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [28.19215278478346, 21.32057940149999, 14.49280861468144, 9.408516503291173, 8.563814091058576, 5.763453456260595, 5.191892923635623, 4.21506094670334, 1.810040208792375, 1.0416810692934333]}, \"tinfo\": {\"Term\": [\"film\", \"series\", \"one\", \"man\", \"story\", \"also\", \"young\", \"bad\", \"woman\", \"great\", \"show\", \"love\", \"action\", \"time\", \"life\", \"like\", \"get\", \"john\", \"good\", \"two\", \"tv\", \"makes\", \"would\", \"really\", \"new\", \"role\", \"horror\", \"films\", \"movies\", \"played\", \"dracula\", \"gordon\", \"clive\", \"endings\", \"chaney\", \"alexander\", \"broadway\", \"beneath\", \"rip-off\", \"tame\", \"wes\", \"mae\", \"roof\", \"potter\", \"affleck\", \"basement\", \"lively\", \"servant\", \"vicki\", \"ladd\", \"widescreen\", \"coppola\", \"rat\", \"combo\", \"dumped\", \"hooper\", \"cary\", \"dusty\", \"haines\", \"employee\", \"em\", \"dutch\", \"flynn\", \"press\", \"tall\", \"brown\", \"owen\", \"pulp\", \"mrs\", \"clothes\", \"sing\", \"lon\", \"batman\", \"book\", \"texas\", \"creature\", \"screen\", \"film\", \"harry\", \"page\", \"characters\", \"horror\", \"scene\", \"west\", \"much\", \"films\", \"character\", \"also\", \"little\", \"scenes\", \"paul\", \"like\", \"version\", \"many\", \"look\", \"well\", \"one\", \"even\", \"short\", \"best\", \"first\", \"classic\", \"really\", \"would\", \"around\", \"way\", \"good\", \"people\", \"great\", \"time\", \"plot\", \"story\", \"made\", \"could\", \"make\", \"see\", \"never\", \"marc\", \"wan\", \"stiller\", \"crow\", \"rex\", \"ski\", \"stinker\", \"liza\", \"figured\", \"insanely\", \"inspirational\", \"lol\", \"monkeys\", \"monday\", \"mins\", \"iq\", \"slashers\", \"mustache\", \"renting\", \"specials\", \"slumber\", \"pod\", \"im\", \"loretta\", \"thats\", \"nope\", \"ta\", \"weed\", \"marvellous\", \"einstein\", \"crap\", \"movie.the\", \"halfway\", \"waste\", \"horrible\", \"sat\", \"walken\", \"laugh\", \"stupid\", \"idiot\", \"hey\", \"garbage\", \"worst\", \"bad\", \"crappy\", \"ok\", \"sucks\", \"na\", \"lame\", \"watch\", \"cheesy\", \"movies\", \"funny\", \"rent\", \"show\", \"terrible\", \"worse\", \"fake\", \"really\", \"watched\", \"think\", \"good\", \"awful\", \"like\", \"kids\", \"watching\", \"thing\", \"acting\", \"want\", \"get\", \"ever\", \"see\", \"seen\", \"know\", \"would\", \"better\", \"could\", \"one\", \"people\", \"tv\", \"time\", \"even\", \"made\", \"make\", \"go\", \"say\", \"first\", \"film\", \"much\", \"great\", \"simpsons\", \"addams\", \"shearer\", \"lister\", \"mole\", \"screens\", \"contention\", \"norwegian\", \"authenticity\", \"forcing\", \"glued\", \"trough\", \"pryor\", \"frustrating\", \"tolerable\", \"heavy-handed\", \"mick\", \"carvey\", \"garner\", \"vaudeville\", \"trigger\", \"danish\", \"representing\", \"black-and-white\", \"shiny\", \"farnsworth\", \"editors\", \"inject\", \"inspire\", \"cuba\", \"barrymore\", \"el\", \"blacks\", \"patton\", \"werewolf\", \"whites\", \"coast\", \"sunshine\", \"futurama\", \"gay\", \"keaton\", \"family\", \"university\", \"bear\", \"asian\", \"marriage\", \"issue\", \"species\", \"children\", \"us\", \"earth\", \"life\", \"touching\", \"issues\", \"love\", \"comedy\", \"mother\", \"story\", \"living\", \"dream\", \"white\", \"one\", \"black\", \"film\", \"child\", \"american\", \"true\", \"well\", \"way\", \"years\", \"still\", \"young\", \"seems\", \"two\", \"ever\", \"even\", \"could\", \"show\", \"see\", \"time\", \"never\", \"first\", \"characters\", \"think\", \"good\", \"like\", \"character\", \"also\", \"people\", \"many\", \"would\", \"amelie\", \"streisand\", \"bands\", \"lundgren\", \"marisa\", \"claudius\", \"esther\", \"edgar\", \"fi\", \"shanghai\", \"phillips\", \"intrigue\", \"aladdin\", \"kellogg\", \"parodies\", \"edith\", \"transparent\", \"ringu\", \"glover\", \"literature\", \"interests\", \"examination\", \"barbra\", \"politicians\", \"griffin\", \"resident\", \"thailand\", \"ricci\", \"morton\", \"hmm\", \"branagh\", \"wayne\", \"zhang\", \"derek\", \"lynch\", \"mitchum\", \"metal\", \"bath\", \"princess\", \"monkey\", \"france\", \"blues\", \"lancaster\", \"western\", \"germans\", \"french\", \"british\", \"predator\", \"palace\", \"film\", \"mexican\", \"history\", \"genre\", \"war\", \"american\", \"john\", \"story\", \"king\", \"island\", \"chinese\", \"music\", \"excellent\", \"films\", \"cast\", \"la\", \"one\", \"heavy\", \"great\", \"years\", \"director\", \"version\", \"love\", \"also\", \"character\", \"interest\", \"real\", \"good\", \"see\", \"little\", \"plot\", \"well\", \"even\", \"like\", \"first\", \"though\", \"best\", \"original\", \"made\", \"many\", \"time\", \"really\", \"much\", \"che\", \"luke\", \"stan\", \"cusack\", \"poland\", \"serbs\", \"hidalgo\", \"anti\", \"mortensen\", \"duck\", \"viggo\", \"southerners\", \"ruby\", \"guevara\", \"elmer\", \"missile\", \"nasa\", \"peace\", \"toro\", \"formation\", \"reno\", \"islam\", \"rutger\", \"purports\", \"wastes\", \"apollo\", \"egotistical\", \"lists\", \"terrorism\", \"alba\", \"clinton\", \"ants\", \"africa\", \"dolls\", \"demon\", \"freddy\", \"aircraft\", \"tarantino\", \"nathan\", \"church\", \"experiment\", \"elite\", \"del\", \"elvis\", \"soldier\", \"jerry\", \"priest\", \"military\", \"government\", \"balls\", \"cult\", \"band\", \"film\", \"dan\", \"water\", \"footage\", \"one\", \"town\", \"get\", \"would\", \"world\", \"see\", \"man\", \"like\", \"point\", \"father\", \"us\", \"back\", \"first\", \"two\", \"director\", \"make\", \"goes\", \"go\", \"way\", \"also\", \"time\", \"could\", \"never\", \"even\", \"last\", \"people\", \"much\", \"fact\", \"part\", \"take\", \"made\", \"end\", \"story\", \"bava\", \"duvall\", \"phoenix\", \"memento\", \"bunuel\", \"aamir\", \"harriet\", \"violin\", \"islamic\", \"handicapped\", \"bennett\", \"mantee\", \"expresses\", \"lyrical\", \"leone\", \"clockwork\", \"throne\", \"villainous\", \"hinted\", \"fitness\", \"sparse\", \"hayek\", \"sheds\", \"kudrow\", \"lagaan\", \"pitched\", \"over-sized\", \"mocking\", \"magazines\", \"execute\", \"timothy\", \"vega\", \"verhoeven\", \"macmurray\", \"perfection\", \"discuss\", \"shark\", \"caine\", \"woody\", \"alex\", \"immigrant\", \"decline\", \"italian\", \"game\", \"orange\", \"kubrick\", \"gripping\", \"film\", \"norman\", \"great\", \"works\", \"excellent\", \"one\", \"pool\", \"films\", \"performance\", \"two\", \"life\", \"would\", \"violence\", \"well\", \"red\", \"best\", \"role\", \"story\", \"really\", \"much\", \"something\", \"actors\", \"part\", \"world\", \"character\", \"also\", \"man\", \"people\", \"new\", \"end\", \"time\", \"little\", \"good\", \"acting\", \"even\", \"zorro\", \"fairbanks\", \"desmond\", \"rickman\", \"kapoor\", \"claire\", \"glenn\", \"bible\", \"damme\", \"der\", \"fiona\", \"malkovich\", \"icing\", \"laser\", \"big-budget\", \"boogeyman\", \"scooby\", \"powerhouse\", \"astor\", \"saif\", \"preachy\", \"brett\", \"shannon\", \"felix\", \"rapper\", \"shaggy\", \"lata\", \"midler\", \"sen\", \"dual\", \"unger\", \"streep\", \"norma\", \"meryl\", \"coach\", \"seagal\", \"wilder\", \"jackie\", \"jesus\", \"van\", \"match\", \"angus\", \"bette\", \"altman\", \"indian\", \"action\", \"game\", \"agent\", \"agents\", \"bronson\", \"accent\", \"sin\", \"team\", \"also\", \"joe\", \"best\", \"character\", \"one\", \"well\", \"even\", \"performance\", \"villain\", \"great\", \"film\", \"long\", \"time\", \"new\", \"real\", \"like\", \"plot\", \"made\", \"could\", \"ever\", \"films\", \"john\", \"would\", \"get\", \"see\", \"story\", \"think\", \"man\", \"good\", \"dixon\", \"christine\", \"madman\", \"chad\", \"rubin\", \"bambi\", \"reagan\", \"pare\", \"foley\", \"stooges\", \"relentlessly\", \"costello\", \"quarters\", \"notices\", \"african-american\", \"sylvester\", \"selection\", \"katharine\", \"splatter\", \"institute\", \"rosie\", \"preminger\", \"pursue\", \"pond\", \"tenant\", \"bumping\", \"hans\", \"vicinity\", \"pov\", \"golf\", \"skull\", \"karloff\", \"carter\", \"robertson\", \"seed\", \"cop\", \"dana\", \"kidnapping\", \"karl\", \"beverly\", \"detective\", \"andrews\", \"criminals\", \"cops\", \"matthau\", \"noir\", \"grandfather\", \"guilty\", \"police\", \"woman\", \"daughter\", \"murder\", \"simon\", \"father\", \"girlfriend\", \"violence\", \"one\", \"also\", \"goes\", \"wife\", \"tries\", \"mother\", \"like\", \"another\", \"man\", \"film\", \"two\", \"people\", \"time\", \"good\", \"bit\", \"life\", \"things\", \"take\", \"story\", \"much\", \"years\", \"really\", \"way\", \"get\", \"make\", \"ling\", \"poirot\", \"vera\", \"niece\", \"silk\", \"alter\", \"banter\", \"agatha\", \"troupe\", \"sadako\", \"evocative\", \"saintly\", \"devotion\", \"highlighted\", \"towns\", \"funky\", \"ivy\", \"kramer\", \"wallet\", \"distorted\", \"fletcher\", \"simmons\", \"goings\", \"uncanny\", \"proposes\", \"1965\", \"requisite\", \"bewitched\", \"amusingly\", \"sleek\", \"arm\", \"wicked\", \"rice\", \"ninja\", \"marie\", \"susan\", \"collette\", \"louis\", \"monk\", \"harold\", \"investigation\", \"sam\", \"scientist\", \"dean\", \"christie\", \"murder\", \"jean\", \"named\", \"husband\", \"man\", \"hitler\", \"woman\", \"police\", \"series\", \"peter\", \"john\", \"young\", \"wedding\", \"love\", \"makes\", \"order\", \"case\", \"also\", \"one\", \"find\", \"film\", \"time\", \"new\", \"way\", \"crawford\", \"excalibur\", \"sherlock\", \"lowe\", \"graham\", \"arthurian\", \"romeo\", \"fritz\", \"juliet\", \"dwarf\", \"seals\", \"consciousness\", \"trevor\", \"retro\", \"palpable\", \"programs\", \"arthur\", \"luana\", \"replay\", \"transcends\", \"life-long\", \"battlestar\", \"universally\", \"rko\", \"pounds\", \"gleason\", \"literal\", \"barrie\", \"powell\", \"walters\", \"joan\", \"massey\", \"pauline\", \"oliver\", \"holmes\", \"montgomery\", \"carole\", \"anna\", \"jagger\", \"series\", \"tank\", \"robinson\", \"programme\", \"unpleasant\", \"pilot\", \"wallace\", \"root\", \"wind\", \"carol\", \"story\", \"young\", \"reality\", \"played\", \"doctor\", \"role\", \"tv\", \"son\"], \"Freq\": [5612.0, 448.0, 3823.0, 781.0, 1664.0, 1276.0, 498.0, 1375.0, 385.0, 1337.0, 911.0, 866.0, 475.0, 1646.0, 831.0, 2925.0, 1341.0, 351.0, 2122.0, 959.0, 411.0, 615.0, 1933.0, 1709.0, 549.0, 514.0, 514.0, 987.0, 1110.0, 366.0, 33.87623067351981, 47.25254166588504, 24.02948692733127, 20.825544782555394, 41.91369197026772, 18.687066478541695, 25.93620798901666, 17.64216189967911, 17.686536110194165, 16.225273560473926, 16.160275820772156, 15.963019139164338, 16.135929491416835, 15.20366038696099, 14.732847646479078, 14.525643642820725, 14.251852459725457, 14.896199415393385, 13.247555401354697, 13.107567926298605, 13.932893913742367, 12.984090874053459, 14.350393020820128, 12.514149660684705, 12.75368637974431, 12.315667198137156, 26.322739668670792, 12.100187605640828, 13.03810439388931, 11.71584336499103, 34.35251083589146, 27.508386471339445, 22.391580252726268, 24.50077148375034, 24.89397285381182, 72.63380319810395, 35.92744497408642, 18.789455239389607, 40.11593844513873, 35.90998935190635, 32.29474127203847, 20.68199831053811, 28.053329176273582, 206.7756637571843, 27.294260410609684, 40.25904758610463, 191.7604930533709, 2099.622026341067, 65.38750042072597, 34.29182684190086, 454.25874131530696, 254.10673377795237, 366.44695486268915, 48.561854403923185, 519.3186541311327, 396.07398612623876, 387.0855445395139, 461.06869115805847, 351.9657606701343, 310.0345974027384, 66.88411311335256, 833.205602013362, 138.06678604817412, 324.91411083590236, 246.34384741170365, 422.00998093596155, 886.1031595957207, 497.7589041855553, 128.21874284878987, 315.31166272301004, 391.5315617325457, 124.95752601123046, 459.5007950715507, 499.0702828817502, 213.02678289100427, 357.56535372206645, 519.7427053665072, 385.1712446404326, 376.1105728929625, 425.97677107638305, 299.46286066051357, 419.95766625403985, 320.51970811190296, 338.3971995745775, 309.7327355226244, 314.9214944554017, 260.02607069076635, 16.803791878662924, 29.608064498286705, 14.215186377126537, 13.431630826567833, 13.252104169010556, 13.190739064053542, 13.132326871750546, 13.714435930193378, 15.204823461341544, 10.749660651956177, 10.716719850338405, 15.860432898462463, 10.320777057552071, 9.961652616480373, 10.492203654023475, 11.150923666336745, 9.478706774768824, 9.211183083035271, 18.803334574013213, 9.420878408030703, 8.649465836505094, 8.167121601847706, 15.00733868503849, 11.035138906137648, 52.682370546772106, 7.918704713067774, 16.977795605648115, 7.4257876048812035, 7.2408939011412405, 7.157921915837059, 140.77441003486754, 18.670179669647386, 22.11478369452438, 197.39448427132652, 174.5388312694223, 40.618801273961495, 25.619700160192686, 179.92596816413493, 198.23944167525343, 25.767286521262662, 50.183910083317684, 63.37750011282001, 313.08130876916897, 964.1780125455583, 30.376080212642467, 113.41715727159244, 34.00506879794955, 48.47780226588784, 82.18539860241651, 619.018878138355, 65.08791816325267, 650.8906661037399, 394.95310828481524, 80.38865260693517, 517.64902600404, 167.88322457157554, 144.1631920377141, 63.874319665188445, 852.2530683713474, 194.52201981656629, 548.4504726325059, 995.4858001211887, 153.43536147998464, 1288.470194679474, 170.88293810539358, 358.70413408229257, 353.3647558969357, 470.2246276138104, 293.3366861657408, 624.9959242381437, 448.9307299284508, 712.5321760136694, 455.85259427599397, 451.557724479922, 789.8122165046715, 429.3872515318206, 602.8700994242057, 1144.819735441606, 552.642352042636, 234.8000803385121, 573.6470041689508, 573.5494383226737, 445.5772643341905, 450.19430626573353, 337.91392197503086, 324.192529926229, 384.51257645322295, 529.4900374152223, 358.6375981202513, 356.9878173191476, 25.871035714469862, 22.424684581091306, 16.069493509549865, 13.481042791846297, 13.124971681574936, 12.815427802345049, 12.549663356133577, 11.370629155373672, 11.56329975675416, 10.414210107998624, 10.388595150943786, 9.674985557908398, 9.671278045091949, 9.6576236645932, 9.62000864649455, 9.898400070718145, 9.396052632138911, 9.3655278999068, 9.382469162556886, 8.907585199702549, 8.805434800937991, 9.359241002494512, 8.449432989971612, 8.21611270557903, 8.16903752654469, 7.965855187565724, 7.9451854611465205, 7.95398324965554, 7.954606943400363, 7.953776963352342, 17.457379699403177, 12.644292362890146, 20.356719186336775, 17.806746504820623, 37.87788164372154, 13.995399155333931, 12.896956823042713, 18.11307038378654, 14.360168134186202, 78.86446564439285, 23.569186777018317, 228.58902422493767, 23.76585175810564, 27.513632305584405, 23.850985148468155, 40.80376276446262, 30.573186950797393, 12.88045714201041, 110.80694562035615, 204.87655549423133, 60.89651606329392, 265.4196644414663, 31.721228246113636, 38.39731377839605, 233.06352885601643, 149.38809272713593, 83.63197418337559, 341.2921277584963, 59.464263813747486, 34.82762922149335, 80.16807904455479, 566.0910279490005, 98.46831718142285, 711.1723912740268, 64.95460600558084, 100.93799614264292, 93.19064972173902, 227.1485785412861, 212.06061782491383, 141.6159339318256, 161.76859728561828, 123.64891263909105, 129.9806902152196, 175.41764657271904, 167.60457862490586, 238.09255685587766, 214.93993625915508, 162.16125037086059, 216.4545934403819, 216.31965641230292, 161.94116867113686, 187.3873061725959, 167.7653014408845, 168.0870951809785, 221.66187859199357, 244.33898347405827, 151.9887441515454, 164.72430611651234, 162.09443939441252, 143.8930897541397, 167.2174179124862, 25.21135433096119, 45.53533740987079, 22.76707653623522, 13.763580406156619, 13.400902186924549, 12.90430281905491, 12.939146611264668, 12.375408286102074, 11.592459779725056, 11.753660445480007, 10.460532448038458, 10.657100023229471, 10.14015662642881, 9.75390883744832, 9.36878365803548, 9.367749978396303, 9.231923636820527, 8.963620996386236, 21.41318638048938, 8.939485413798952, 9.351561592711052, 8.41893303125401, 8.712536236338567, 8.217940407445676, 8.159972741364308, 8.038328630910902, 7.960144053384506, 7.951452983786666, 8.183937264618592, 9.474234674748601, 14.81599078074142, 42.388753814140614, 12.656761655098233, 17.65818187859449, 28.644438769194377, 23.530266904974678, 47.67242612202783, 13.729550584350992, 21.838254698834003, 11.06654283002435, 27.27353684124908, 14.83526451182742, 20.56407505356369, 55.41749889769244, 18.839577267669444, 59.06784856729264, 62.579053239633396, 20.263642553089717, 15.003778356406123, 733.236525703443, 24.04888113819616, 71.12357050028935, 68.52066054538959, 78.22950283379168, 91.53157039068142, 96.19838705024698, 254.05575620768778, 48.72283714257369, 43.69086791929564, 34.316351942694304, 102.5120520671029, 74.15087081360844, 158.46601814271617, 113.66821743280681, 35.203311847177645, 315.0505886843641, 31.272807793263972, 155.0588259147162, 100.24244311424034, 96.60650218552149, 64.78171155740446, 107.45165134302881, 127.67196514249811, 109.84237239612563, 49.38995807893659, 87.60229043107731, 141.76744668899434, 125.36491773598331, 100.47734367354057, 101.85628905195753, 111.61863577026617, 117.42432466581475, 136.55054506387276, 105.16688454541683, 86.39379756967973, 93.86151777689497, 77.98994862364624, 87.80411324192379, 84.34801583067096, 90.53090154095051, 87.96501573085975, 78.69348688883562, 44.8892493069178, 24.22983258002332, 13.528727046292047, 15.796622066725112, 12.484818344776562, 11.27443376787157, 10.981722527523404, 10.809462364450322, 10.072656104381808, 9.821180530972452, 9.599358716894464, 9.586845457071922, 9.627223099117268, 9.179582655780532, 9.158822453030801, 9.12110744983354, 12.679847117369977, 17.099013386229423, 8.63075622239931, 8.43136209341118, 8.37924636608633, 8.172345233507553, 8.047928767046693, 8.056170948459489, 7.884800236274859, 7.892388185610173, 7.593691721671087, 7.630896290525511, 7.854341304859779, 7.474618626661087, 7.833580149807171, 24.84741623352992, 22.235588457826683, 19.60885714207605, 25.844460205792206, 23.90262321560122, 15.819718268435626, 18.736488662657557, 20.092771754354988, 36.00453390520219, 20.319117007105508, 20.5689574764726, 13.993509893743365, 13.06210149277097, 28.36752954401444, 29.28575521624076, 30.83687566765304, 43.02826664031686, 32.711984795152574, 20.674990688168286, 38.70084999227956, 35.04746189395904, 630.7258903342895, 28.28549436485514, 38.4445890969061, 43.720057543322085, 338.79215576661034, 51.87963239522992, 169.38643295975243, 195.87309956227588, 83.302755237872, 144.02511409630188, 101.51789426209962, 178.8033218744142, 77.18954975851445, 62.40791209797832, 77.79123757237433, 86.69868253919225, 112.49833498994866, 95.76884940345292, 77.86019953761857, 93.36467317499665, 60.33365548793617, 79.06288950726587, 91.57463326362709, 92.88116233913817, 100.61564442296181, 93.58005361240497, 81.0671899478587, 95.5462390825916, 62.89291614475018, 84.36567000023516, 76.41941099309518, 63.07615826461729, 63.58621930472635, 61.358973332658984, 67.38188907150703, 63.97583688598334, 67.14538274704368, 20.037506310499577, 20.028738790727935, 13.259030424572632, 12.296938691362325, 12.468148869125196, 13.748701531023308, 12.531495481276437, 12.118695270765098, 9.887032065990546, 9.709278934327916, 9.412443913509591, 7.974449540753152, 7.003344822853621, 6.943525727429217, 14.597835820906706, 15.073508377148888, 6.748035672544354, 6.493987814553113, 6.476659785893329, 6.486768069653439, 6.487358339076661, 6.023850075617061, 6.006330417687415, 6.006425911872062, 5.754673759715764, 5.7447353146062, 5.739101157712012, 5.674538757183082, 6.795380279311214, 5.529709969505008, 13.353970871129047, 12.753968731879546, 14.256315289778577, 11.255944407411796, 19.563243735507218, 15.418393636259554, 25.608546034939803, 22.63840771417155, 16.989856193223883, 23.452900046547878, 10.318454283408668, 8.43709707474979, 38.435555640963706, 84.2373199066281, 13.102616077324972, 10.491214799645778, 17.021247503049537, 643.7336972633088, 16.734213774619203, 177.32642006438644, 44.4740182495335, 56.2797758727067, 223.07673283244358, 17.032296628843625, 101.2420885742706, 62.46283590384949, 95.85112011884783, 81.81492597458742, 118.81866872877404, 39.01951603899361, 98.58696570073603, 30.945484008869272, 80.67932194877362, 59.79288002756675, 95.21915114106251, 95.89741877588754, 86.64243515935276, 63.227281592663836, 61.369044231098684, 56.773141130031924, 51.98094300296676, 66.17186592598071, 68.99694537576694, 58.264655186528884, 67.09509088513902, 50.96317239200199, 53.47112398628207, 63.967798617867516, 55.01529443345433, 64.27799962003361, 53.688866108891446, 54.5417304400812, 25.96178669158071, 29.167890160396514, 14.329774625333537, 12.629513410304991, 13.611110120796242, 23.177288801073814, 11.701071477046456, 14.382766591956925, 10.784708705377865, 10.266942100584977, 10.254599670775695, 10.220120771523645, 9.794122770818388, 9.330680061995414, 8.2230327219864, 10.533561216294855, 7.948767281063961, 7.895326719251042, 7.907031762845081, 7.439984704256021, 7.407204924235364, 7.304472584434463, 7.238119178337026, 17.456279871780286, 6.6615848716877055, 6.50344842793798, 6.494023310217121, 6.251871395970785, 5.7918627208509434, 5.78480643976155, 6.012154135410749, 15.788639667561172, 32.44424698448001, 14.584621725709367, 23.62004590237332, 22.056773072159427, 17.89114700929582, 23.152021980128293, 19.369615141697672, 46.75035749188751, 57.340081440289545, 14.26319201422935, 26.02570775336219, 12.519379538548108, 31.645772950187084, 125.46389703167134, 56.02161572287625, 25.917874014469007, 16.596033644811975, 14.46569146230807, 27.364946257029487, 14.64572997441054, 31.504608069103355, 103.30911890080739, 28.211529094351132, 79.18430930336915, 80.14008378934689, 143.1133892186876, 91.12519503453014, 92.36369187563399, 51.7016844616654, 28.82876374396215, 77.04003122551786, 123.42472301696534, 49.99360487333359, 77.789070878204, 52.977588483618874, 54.82106183654206, 88.32003058880512, 60.3557634679314, 61.90993737898568, 64.25356954250569, 53.18190309678882, 51.70808603626197, 40.43727334820744, 56.8502593713573, 52.29694314307264, 52.48699508499137, 52.06758571788063, 47.464049922071844, 43.935521403157296, 45.763457312398785, 18.83095029189221, 16.673558993058847, 15.220621001055104, 14.69411464614136, 11.426283295828723, 11.60818716594754, 8.8914783077415, 8.435006873942072, 8.398374679529296, 7.661529202222435, 7.785574978755269, 10.325220418664545, 7.388680427815714, 7.143763502443545, 7.834620234618946, 6.927365261167532, 6.877817070782207, 6.872251512058951, 6.993365925592355, 6.432146776886662, 13.726162084852197, 6.022047968342861, 5.9689843922051, 5.949446815629265, 5.929733938483909, 5.925069724667577, 5.946192974576811, 5.69554315811957, 5.670780312161314, 5.642517548147936, 13.220405880239827, 10.387982709734224, 18.347701930457436, 8.889682179981055, 9.778361386398501, 69.57052272000558, 16.53023812208636, 10.585686129578786, 15.035721084839013, 15.140195537092788, 35.07629405311161, 15.918279770302417, 13.843817827523305, 25.773462178471735, 12.40674219347075, 25.239455396516597, 17.7415294349039, 21.53566371647474, 53.95846773387971, 81.59064466329589, 46.48676748741638, 39.50109791303135, 18.408850893825797, 59.00652792667362, 24.751887329069227, 39.372519163750276, 168.85808797916127, 87.14338675881623, 45.792453490532985, 43.06259052755943, 33.40321041861976, 36.31957818858491, 99.5650163765895, 51.33804482271477, 53.965243522738, 106.3885619846894, 57.549744571374816, 63.68737561094915, 66.54954081979275, 71.42860376407839, 43.178408895277265, 50.69827836347476, 43.97141242562023, 42.903203874980214, 55.496245576914625, 47.05487257373394, 39.64923908094758, 44.770621783611496, 42.06420597488859, 41.719709420384326, 39.74103658345997, 22.789271366718506, 15.231239592487308, 12.27920695973988, 12.303644511917266, 9.926145147599584, 9.138825715852535, 8.761609196595321, 8.347834170130557, 7.653306264792769, 18.815023164344232, 6.053481497895742, 6.02473176802712, 5.856757713411336, 6.002420918122214, 5.443357868948834, 6.574498998749602, 5.344511098249845, 9.954360283370564, 4.881631108070553, 4.5136649184592414, 12.941284037216775, 7.592216812463064, 4.211475943426928, 4.1392779638950525, 4.141372864259615, 3.786346025612585, 3.7584778929381195, 3.7581405511807207, 3.768257180900516, 3.7488394995044696, 8.67133766947164, 18.878058571393826, 6.975733204488538, 19.414499595031664, 27.659872848522987, 26.128978918314104, 7.731914140271018, 15.210826793162514, 6.70446413762334, 20.969598400167605, 12.511573436994984, 39.77534667896235, 20.13164630545862, 28.5594175903072, 10.139313738923486, 34.05056205273729, 15.529621999908688, 26.746914947602107, 28.445120728141667, 58.92676156902581, 13.421575773258558, 37.57539857421305, 25.896934485696136, 36.67194099008765, 19.875612767820478, 29.45675310588067, 30.353663522533118, 14.551954059825114, 29.93817119304083, 27.2777636661161, 18.395213826491155, 19.163850643726416, 26.416149341693632, 30.44204545274841, 22.445865508802488, 27.71141609269642, 22.945636919832967, 20.513336829837233, 19.318844286686684, 60.106873615488254, 12.285154794959569, 10.9985475910901, 8.7125641577096, 21.639889217565816, 6.182436944511062, 6.42801152758896, 12.396321906785438, 5.4091785977903735, 13.64994928085238, 5.073595925446417, 5.0466858892656985, 5.028664625668245, 4.878069339166969, 4.639424051021449, 8.363173863233591, 38.994192771622025, 4.226816160959763, 4.112408428020083, 4.076542434517536, 3.9289176401467496, 3.5719595640505104, 3.5369113709863216, 3.154492595690205, 3.14752808668796, 8.14267633133123, 6.2743062827067995, 3.816401041761833, 17.492516924912017, 5.1370090517884615, 48.007052323610715, 8.89013481175806, 9.436180859464383, 15.219117888078719, 15.269782594442866, 9.223122763164447, 13.13252485845952, 18.116949492142247, 8.446789672132741, 99.23577688470037, 12.159143099312669, 9.85445080128626, 7.073028572028538, 8.403311416079607, 13.50555954747911, 7.621457129567179, 9.452115226079194, 9.734117837748435, 9.192191836216548, 21.38778798464389, 15.602826147012202, 11.743861719999238, 12.690059143894828, 9.69005576948435, 9.78718451435193, 9.670699151290098, 9.295760122439543], \"Total\": [5612.0, 448.0, 3823.0, 781.0, 1664.0, 1276.0, 498.0, 1375.0, 385.0, 1337.0, 911.0, 866.0, 475.0, 1646.0, 831.0, 2925.0, 1341.0, 351.0, 2122.0, 959.0, 411.0, 615.0, 1933.0, 1709.0, 549.0, 514.0, 514.0, 987.0, 1110.0, 366.0, 34.739194581233605, 48.767265005027525, 24.888611525756804, 21.686434461423108, 43.79500868269401, 19.56910230878584, 27.209016630730503, 18.51260561027892, 18.568633875752724, 17.086109580305305, 17.024350089870605, 16.82233891168583, 17.008107600976576, 16.08842278267928, 15.595446411111999, 15.387554083732622, 15.123660448608454, 15.861102998336232, 14.112114554381671, 13.966543293518964, 14.850620906141451, 13.848550679343338, 15.325868946156342, 13.372680339495645, 13.63084987528978, 13.174958319029454, 28.16309970248334, 12.959678703989487, 13.968417617711912, 12.57404777691427, 37.065977515051266, 29.65858658127429, 24.138829614259457, 26.678632734967476, 27.145943069092553, 82.38360906840481, 39.839321595025474, 20.417437970430594, 45.922419084228075, 41.27003853479105, 36.930530519689995, 22.77726823617811, 32.115378519832355, 311.39734322118244, 31.38742129266614, 50.00942012590927, 321.5539534423502, 5612.259952612594, 90.23629376273378, 42.13080105388211, 998.5940043206642, 514.6464221739366, 824.2233408167897, 66.07432023255814, 1350.0624829653, 987.5805783362134, 990.2132053157511, 1276.8005902029608, 906.2678780525528, 778.2362660332343, 101.07625904259116, 2925.3049224437545, 274.76409131984747, 896.6246537705601, 617.0068120354174, 1342.1316939674098, 3823.8093574804775, 1712.0992164091404, 254.36344709104134, 916.4646373381795, 1253.6246391776249, 248.58810043795842, 1709.6287239330397, 1933.9607981807253, 545.7033291682222, 1195.6140763330634, 2122.5945687016992, 1383.6910110866659, 1337.9615839444748, 1646.4717479709832, 952.2788072739836, 1664.3157121562688, 1142.3871555711098, 1445.2473963710563, 1179.7773183777397, 1609.3303416140864, 924.4195670361288, 17.65801026879265, 31.22426817343409, 15.089131720958836, 14.284312893279884, 14.104006634815834, 14.065884943091728, 14.009685659705172, 14.68997971286253, 16.29782593664401, 11.604431705071375, 11.569424508262223, 17.14563866439968, 11.179836257231608, 10.81545107254588, 11.401096197405161, 12.133661813885622, 10.332270879670927, 10.074793190116573, 20.571124329093486, 10.311319795386401, 9.535250724610549, 9.019096202280304, 16.621137328890356, 12.222927415954521, 58.46337722459952, 8.800491874642715, 18.900539074003373, 8.283288912140405, 8.092803741844271, 8.009317478253703, 158.7056464975161, 20.962093386044607, 24.903534282468975, 231.20238562440147, 206.033589074902, 46.385766249941796, 29.006311395132492, 220.4510990466743, 244.13250990470834, 29.295143100978965, 58.74106363771216, 75.15577475574307, 411.6342701023736, 1375.223996991473, 35.199134598018375, 145.16354491218092, 39.86992684370617, 58.904075585521646, 104.70900943614419, 963.6715960345751, 83.31364800310463, 1110.2681816474487, 640.2245582715176, 107.16881894054005, 911.183873710226, 252.5643985236471, 213.8701330575924, 84.15629164193768, 1709.6287239330397, 308.6557922403353, 1046.752337297251, 2122.5945687016992, 236.01030553038152, 2925.3049224437545, 271.46945134748637, 658.3557568393838, 646.9332659880259, 918.0915829027817, 521.6782454064851, 1341.0285312068434, 897.8842698107542, 1609.3303416140864, 929.9669663130742, 935.5189000178322, 1933.9607981807253, 909.7345882711936, 1445.2473963710563, 3823.8093574804775, 1383.6910110866659, 411.3611083953915, 1646.4717479709832, 1712.0992164091404, 1142.3871555711098, 1179.7773183777397, 755.1651292470077, 712.5829631359386, 1253.6246391776249, 5612.259952612594, 1350.0624829653, 1337.9615839444748, 26.741579582078618, 23.28515742090642, 16.93028170359225, 14.339257412992156, 13.985192924303526, 13.726246352742853, 13.447582054164261, 12.22800744081964, 12.48985753364042, 11.272409682604655, 11.273938073444947, 10.533178158585793, 10.529231802000055, 10.516241180929212, 10.477460314708857, 10.797863505743875, 10.253401417053126, 10.222714507658102, 10.246799542616067, 9.766152330241468, 9.663473383159129, 10.302901425939563, 9.307051782406981, 9.07684820609495, 9.034515128734878, 8.824191221402296, 8.803038793111666, 8.814144210025079, 8.815195309101098, 8.817854602309906, 19.42781172134055, 14.043105026044621, 22.78983377340047, 20.00817475382973, 43.488049911412524, 15.699940759449534, 14.64882544736523, 21.367660395494745, 16.638429419383908, 114.46411142079693, 29.451813928717343, 421.66684911745506, 30.368298859625842, 36.598836887262834, 31.255700306767633, 60.83679475583213, 42.925980785462, 15.020458146217061, 225.7460117191551, 506.965668372444, 109.2150126841553, 831.0170116124757, 49.78793802368594, 65.08597271891192, 866.541951444806, 463.9682714352407, 208.3625787546952, 1664.3157121562688, 129.5048523107806, 59.2153015577338, 202.24830291742612, 3823.8093574804775, 280.36780997341145, 5612.259952612594, 153.4605878122407, 319.05016998008, 299.07983033322904, 1342.1316939674098, 1195.6140763330634, 613.5863928648828, 788.8780020491681, 498.12699175547476, 547.2021762196968, 959.6291848772917, 897.8842698107542, 1712.0992164091404, 1445.2473963710563, 911.183873710226, 1609.3303416140864, 1646.4717479709832, 924.4195670361288, 1253.6246391776249, 998.5940043206642, 1046.752337297251, 2122.5945687016992, 2925.3049224437545, 990.2132053157511, 1276.8005902029608, 1383.6910110866659, 896.6246537705601, 1933.9607981807253, 26.081274229205984, 47.905073761503125, 24.096139388801134, 14.621057948297638, 14.259817095031321, 13.782811390255779, 13.827572060145037, 13.235591723215782, 12.48864447518435, 12.701509181198277, 11.321279542304262, 11.536856379929413, 10.99839921369689, 10.611868631255064, 10.226107181453331, 10.239452036322783, 10.095462529398727, 9.821049910936264, 23.510244470935138, 9.824490692986618, 10.281456909661673, 9.279266986331887, 9.617390521637313, 9.079764801767046, 9.017716751889223, 8.89638979201943, 8.817902909825019, 8.814410463720359, 9.07527326282768, 10.513503132311728, 16.53391881636879, 48.41317966512461, 14.224344880703512, 20.941059296772217, 35.77065013076842, 29.79264806823458, 66.60589055692338, 16.220483409848885, 28.185932556464792, 12.719312775467591, 39.50496815539497, 18.450929124594072, 28.123435124187775, 102.26837119771662, 25.64777318821738, 117.8057785307722, 127.82850078289259, 28.87653953891012, 19.05918550347176, 5612.259952612594, 38.038748646908, 197.15378871032354, 189.78051363991617, 244.47400573173653, 319.05016998008, 351.84003646680105, 1664.3157121562688, 121.21866064651158, 102.33301409402755, 68.85388604942956, 458.07967959257206, 264.5246488452982, 987.5805783362134, 574.8350613008105, 74.20401650886151, 3823.8093574804775, 61.26335993676427, 1337.9615839444748, 613.5863928648828, 589.5267726793838, 274.76409131984747, 866.541951444806, 1276.8005902029608, 990.2132053157511, 155.2240916592899, 635.2836489899433, 2122.5945687016992, 1609.3303416140864, 906.2678780525528, 952.2788072739836, 1342.1316939674098, 1712.0992164091404, 2925.3049224437545, 1253.6246391776249, 637.4908480733573, 916.4646373381795, 498.21709871994625, 1142.3871555711098, 896.6246537705601, 1646.4717479709832, 1709.6287239330397, 1350.0624829653, 45.82389740720324, 25.783371814759988, 14.409607612222395, 16.853536422263584, 13.344004881281023, 12.132946643890353, 11.839825511795995, 11.668638623868118, 10.931616054033123, 10.695820102983209, 10.457506593563247, 10.446984450548278, 10.500362839352212, 10.038733169820562, 10.01846349026296, 9.980526166324005, 13.893262747451582, 18.765676390755377, 9.50157510013536, 9.293173572517173, 9.238721284047173, 9.031189273782179, 8.912003310817449, 8.929766893464485, 8.750059604753455, 8.763210996932594, 8.452903205750797, 8.501874262745076, 8.752154948104936, 8.338788476815042, 8.742189378611446, 28.40015728837478, 25.721017320609096, 22.855121379730054, 31.5173493262743, 29.445034796873706, 18.75323480357982, 22.916543723827214, 24.902860898506045, 49.637394082820975, 25.83212001284581, 26.383400889775995, 16.794511441419186, 15.616554040276673, 41.43830401245862, 43.6800728852216, 46.97600223776387, 73.20509778037182, 52.6592614073536, 29.316503595770595, 73.96305980522959, 67.21252477844253, 5612.259952612594, 51.53019101192928, 83.8429886797695, 104.45588160807615, 3823.8093574804775, 162.10250110942076, 1341.0285312068434, 1933.9607981807253, 504.99284188604094, 1609.3303416140864, 781.2335903444382, 2925.3049224437545, 474.43176129710065, 308.943809457895, 506.965668372444, 674.2529427296898, 1253.6246391776249, 959.6291848772917, 589.5267726793838, 1179.7773183777397, 343.9882224596469, 755.1651292470077, 1195.6140763330634, 1276.8005902029608, 1646.4717479709832, 1445.2473963710563, 924.4195670361288, 1712.0992164091404, 436.2583707364437, 1383.6910110866659, 1350.0624829653, 554.5732427268831, 591.4939267497558, 503.12475089690514, 1142.3871555711098, 752.9780555724584, 1664.3157121562688, 20.897689873572464, 20.89408592145599, 14.148461498805245, 13.160510358569294, 13.345500924869672, 14.738011823645756, 13.50370662602749, 13.079950659247828, 10.744252929767526, 10.579593040495435, 10.270715412312327, 8.861212117657567, 7.871194300258602, 7.806163843266749, 16.418802487062408, 16.997485068987057, 7.625169370659404, 7.351699847538912, 7.333823117541598, 7.346443101370087, 7.349294881488384, 6.88233345497006, 6.86346379779345, 6.863951740524206, 6.61178666848319, 6.602056299473298, 6.595876679461949, 6.533083574004051, 7.834910598500135, 6.38719604694135, 15.53055718922219, 14.945512976199069, 17.01699604015686, 13.329784931124244, 23.884286187788966, 18.97677509891308, 34.22877843318753, 30.380717424796526, 22.65415980116004, 33.889484533389194, 12.798970891217751, 10.061982645854373, 66.86219357195195, 193.49657833532882, 17.83710846488251, 13.545888412271331, 26.285411520407706, 5612.259952612594, 26.86416973557868, 1337.9615839444748, 145.93061480078418, 264.5246488452982, 3823.8093574804775, 29.955137262661264, 987.5805783362134, 401.7140086856131, 959.6291848772917, 831.0170116124757, 1933.9607981807253, 170.28348041191566, 1342.1316939674098, 102.12431139546742, 916.4646373381795, 514.4304720720452, 1664.3157121562688, 1709.6287239330397, 1350.0624829653, 709.7035864694352, 659.8397678288792, 591.4939267497558, 504.99284188604094, 990.2132053157511, 1276.8005902029608, 781.2335903444382, 1383.6910110866659, 549.9190107054965, 752.9780555724584, 1646.4717479709832, 906.2678780525528, 2122.5945687016992, 918.0915829027817, 1712.0992164091404, 26.82345363667825, 30.160887107386305, 15.194458305528068, 13.491610109878643, 14.569815844564122, 24.810301676098618, 12.562234161320829, 15.441323677764517, 11.64532465103477, 11.12753173389902, 11.115264213354827, 11.083663038172434, 10.655789131013444, 10.198584234341167, 9.088196398769474, 11.643554806965966, 8.80996644279465, 8.756009809804484, 8.8178092779245, 8.300675968419704, 8.268384176409144, 8.165567939312965, 8.099166888971627, 19.690607724880522, 7.522236330166852, 7.364185183536481, 7.368449169607736, 7.112440558937379, 6.654460179082412, 6.646358678567068, 6.909259443933248, 18.589890145182853, 40.59789859740063, 17.67196931360037, 29.748534768025092, 27.809427668602247, 23.413999098875642, 31.990561507806024, 26.26040562052395, 76.46642727368298, 98.71377328816727, 18.615037769912444, 41.25695825481647, 16.237314413730733, 60.018690904687965, 475.41221131239905, 193.49657833532882, 57.647113840572, 26.695494945359247, 21.02594205094564, 71.51293925498307, 21.868135440322853, 97.28761773504802, 1276.8005902029608, 85.55080909767653, 916.4646373381795, 990.2132053157511, 3823.8093574804775, 1342.1316939674098, 1712.0992164091404, 401.7140086856131, 95.2433276820679, 1337.9615839444748, 5612.259952612594, 459.0909375010457, 1646.4717479709832, 549.9190107054965, 635.2836489899433, 2925.3049224437545, 952.2788072739836, 1142.3871555711098, 1445.2473963710563, 897.8842698107542, 987.5805783362134, 351.84003646680105, 1933.9607981807253, 1341.0285312068434, 1609.3303416140864, 1664.3157121562688, 1046.752337297251, 781.2335903444382, 2122.5945687016992, 19.716773222550522, 17.53132763875204, 16.100516190789804, 15.552538953352666, 12.286698862514099, 12.493702909011281, 9.74938168349322, 9.294810832326565, 9.260683108561302, 8.526633250374518, 8.666244421170406, 11.530727001336233, 8.25661409882567, 8.004100605452939, 8.783086953595074, 7.785784767109995, 7.735801685983797, 7.731503359454091, 7.887638016949066, 7.296252091969908, 15.615424928562396, 6.8820636337591665, 6.826974438290058, 6.8098023169439745, 6.787564986483802, 6.789207257094486, 6.816545255452047, 6.553310557643631, 6.530634596201483, 6.500330405052705, 15.569322006664107, 12.219664984366966, 22.259609433566514, 10.564433450818804, 11.801822261228995, 104.65049904570527, 21.47551429278563, 13.231994022416396, 20.68458926999105, 20.980609370951647, 60.601815746472134, 22.353967804184748, 18.85878336203271, 44.34485668726764, 16.952155870239363, 45.768571041346654, 29.307400913413836, 39.575999604311384, 163.23811748209485, 385.8515738642264, 161.63459518734686, 140.06035838082784, 36.77207347545069, 308.943809457895, 63.561227539611856, 170.28348041191566, 3823.8093574804775, 1276.8005902029608, 343.9882224596469, 300.5425863035007, 168.78400110107125, 208.3625787546952, 2925.3049224437545, 596.643246304691, 781.2335903444382, 5612.259952612594, 959.6291848772917, 1383.6910110866659, 1646.4717479709832, 2122.5945687016992, 458.12866296633933, 831.0170116124757, 509.18209298530644, 503.12475089690514, 1664.3157121562688, 1350.0624829653, 613.5863928648828, 1709.6287239330397, 1195.6140763330634, 1341.0285312068434, 1179.7773183777397, 23.667606544344345, 16.096003773670727, 13.156731212543322, 13.35149069835617, 10.806350811471221, 10.003682399243193, 9.625512901318311, 9.216264673987359, 8.518841933752778, 21.283085172025885, 6.917029293286403, 6.907756382889861, 6.720846505327728, 6.921047661205665, 6.310890234115039, 7.636656847898488, 6.211176772464786, 11.642516628728295, 5.7698016233871305, 5.377937107679052, 15.438171839385085, 9.100799515759725, 5.076377972761586, 5.003518636978756, 5.01921685993762, 4.65531896618548, 4.622087677080214, 4.621680991050721, 4.635599279671966, 4.612349221482065, 10.671492690628748, 24.415650822043226, 8.815824743142645, 25.927129472305495, 38.790826030758645, 38.94530234402779, 10.193739289764755, 23.220342634364766, 8.854979221169682, 38.041410213812874, 20.520418411559668, 104.7990238147217, 46.664886926148554, 85.92207577356722, 16.93950691414039, 140.06035838082784, 39.25287280602762, 122.70052043784283, 160.57725124143016, 781.2335903444382, 34.080085057607185, 385.8515738642264, 163.23811748209485, 448.8389697697165, 96.75949226353194, 351.84003646680105, 498.12699175547476, 48.96084259390505, 866.541951444806, 615.0028610769987, 154.8631086230633, 195.05657577477518, 1276.8005902029608, 3823.8093574804775, 573.1976770279787, 5612.259952612594, 1646.4717479709832, 549.9190107054965, 1195.6140763330634, 63.39453708704482, 13.153522298499448, 11.981142993626579, 9.582856655575391, 24.494962498918326, 7.0505385853610445, 7.355545061659298, 14.29542614639295, 6.280380141699034, 15.956115104515261, 5.94168620658609, 5.915072509939592, 5.896880224447297, 5.7627323650042115, 5.517023126014096, 9.956358060779056, 47.14025121263434, 5.113549771124218, 4.984494838632433, 4.946396911647567, 4.797685597705047, 4.440053576353805, 4.405758128134024, 4.022935003418174, 4.016020514792689, 10.408734817325758, 8.042070026221603, 4.891704639224494, 23.440965025124058, 6.9085659684252185, 68.88861064750122, 12.609865369611608, 13.496245076188762, 23.171931886804593, 25.483093939559115, 13.927314247142089, 22.5864304993871, 36.54479345310804, 13.25212108177653, 448.8389697697165, 23.700727378820577, 19.459567204186172, 11.64457113985275, 17.1392407213977, 50.19185925430341, 14.76065825612423, 28.44208827569465, 32.94404264752038, 34.71344997851167, 1664.3157121562688, 498.12699175547476, 125.9277728516567, 366.0092702224976, 121.93277368080733, 514.4304720720452, 411.3611083953915, 197.66084528963685], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.0646, -7.7318, -8.408, -8.5511, -7.8517, -8.6594, -8.3316, -8.717, -8.7145, -8.8007, -8.8047, -8.817, -8.8062, -8.8657, -8.8972, -8.9114, -8.9304, -8.8862, -9.0035, -9.0141, -8.953, -9.0235, -8.9235, -9.0604, -9.0414, -9.0764, -8.3168, -9.094, -9.0194, -9.1263, -8.0506, -8.2728, -8.4786, -8.3886, -8.3726, -7.3018, -8.0058, -8.654, -7.8955, -8.0063, -8.1124, -8.558, -8.2532, -6.2556, -8.2806, -7.8919, -6.331, -3.9378, -7.4069, -8.0524, -5.4686, -6.0495, -5.6834, -7.7044, -5.3347, -5.6057, -5.6286, -5.4537, -5.7237, -5.8506, -7.3843, -4.862, -6.6595, -5.8037, -6.0805, -5.5422, -4.8004, -5.3772, -6.7335, -5.8337, -5.6172, -6.7593, -5.4571, -5.3745, -6.2258, -5.7079, -5.3339, -5.6336, -5.6574, -5.5329, -5.8853, -5.5471, -5.8173, -5.763, -5.8516, -5.8349, -6.0265, -8.4863, -7.9198, -8.6536, -8.7103, -8.7237, -8.7284, -8.7328, -8.6894, -8.5863, -8.933, -8.9361, -8.5441, -8.9737, -9.0092, -8.9573, -8.8964, -9.0588, -9.0875, -8.3739, -9.065, -9.1504, -9.2078, -8.5994, -8.9068, -7.3436, -9.2387, -8.476, -9.3029, -9.3282, -9.3397, -6.3607, -8.381, -8.2116, -6.0227, -6.1457, -7.6037, -8.0645, -6.1154, -6.0184, -8.0588, -7.3922, -7.1588, -5.5614, -4.4366, -7.8942, -6.5768, -7.7814, -7.4268, -6.8989, -4.8798, -7.1322, -4.8296, -5.3291, -6.921, -5.0586, -6.1846, -6.3369, -7.151, -4.56, -6.0374, -5.0008, -4.4047, -6.2746, -4.1467, -6.1669, -5.4254, -5.4404, -5.1547, -5.6266, -4.8702, -5.201, -4.7391, -5.1857, -5.1952, -4.6361, -5.2455, -4.9062, -4.2649, -4.9932, -5.8492, -4.9559, -4.9561, -5.2085, -5.1982, -5.4851, -5.5266, -5.3559, -5.036, -5.4256, -5.4302, -7.6688, -7.8117, -8.145, -8.3206, -8.3474, -8.3712, -8.3922, -8.4908, -8.474, -8.5787, -8.5812, -8.6523, -8.6527, -8.6541, -8.658, -8.6295, -8.6816, -8.6848, -8.683, -8.735, -8.7465, -8.6855, -8.7878, -8.8158, -8.8215, -8.8467, -8.8493, -8.8482, -8.8481, -8.8482, -8.0621, -8.3847, -7.9085, -8.0423, -7.2875, -8.2831, -8.3649, -8.0252, -8.2574, -6.5541, -7.7619, -5.4899, -7.7536, -7.6072, -7.7501, -7.2131, -7.5018, -8.3662, -6.2141, -5.5995, -6.8127, -5.3406, -7.4649, -7.2739, -5.4706, -5.9153, -6.4954, -5.0891, -6.8365, -7.3715, -6.5378, -4.5831, -6.3321, -4.355, -6.7482, -6.3074, -6.3872, -5.4963, -5.565, -5.9688, -5.8357, -6.1044, -6.0545, -5.7547, -5.8003, -5.4492, -5.5515, -5.8333, -5.5445, -5.5451, -5.8346, -5.6887, -5.7993, -5.7974, -5.5207, -5.4233, -5.8981, -5.8176, -5.8337, -5.9528, -5.8026, -7.2625, -6.6713, -7.3645, -7.8678, -7.8945, -7.9323, -7.9296, -7.9741, -8.0395, -8.0257, -8.1422, -8.1236, -8.1733, -8.2122, -8.2525, -8.2526, -8.2672, -8.2967, -7.4258, -8.2994, -8.2543, -8.3594, -8.3251, -8.3835, -8.3906, -8.4056, -8.4154, -8.4165, -8.3877, -8.2413, -7.7941, -6.743, -7.9516, -7.6186, -7.1349, -7.3316, -6.6255, -7.8703, -7.4062, -8.0859, -7.1839, -7.7928, -7.4663, -6.4749, -7.5539, -6.4112, -6.3534, -7.481, -7.7815, -3.8924, -7.3097, -6.2254, -6.2627, -6.1302, -5.9732, -5.9234, -4.9523, -6.6037, -6.7127, -6.9542, -5.8599, -6.1837, -5.4243, -5.7566, -6.9287, -4.7371, -7.0471, -5.446, -5.8822, -5.9192, -6.3188, -5.8128, -5.6404, -5.7908, -6.5901, -6.017, -5.5357, -5.6586, -5.8799, -5.8663, -5.7748, -5.724, -5.5731, -5.8343, -6.0309, -5.948, -6.1333, -6.0147, -6.0549, -5.9841, -6.0129, -6.1243, -6.5916, -7.2082, -7.791, -7.636, -7.8713, -7.9732, -7.9995, -8.0153, -8.0859, -8.1112, -8.1341, -8.1354, -8.1312, -8.1788, -8.1811, -8.1852, -7.8558, -7.5567, -8.2404, -8.2638, -8.27, -8.295, -8.3104, -8.3093, -8.3308, -8.3299, -8.3685, -8.3636, -8.3347, -8.3843, -8.3373, -7.183, -7.2941, -7.4198, -7.1437, -7.2218, -7.6345, -7.4653, -7.3954, -6.8121, -7.3842, -7.372, -7.7572, -7.8261, -7.0505, -7.0187, -6.9671, -6.6339, -6.908, -7.3668, -6.7399, -6.8391, -3.9489, -7.0534, -6.7466, -6.618, -4.5704, -6.4468, -5.2636, -5.1183, -5.9733, -5.4258, -5.7755, -5.2095, -6.0495, -6.2621, -6.0417, -5.9333, -5.6728, -5.8338, -6.0409, -5.8593, -6.2959, -6.0255, -5.8786, -5.8644, -5.7845, -5.857, -6.0005, -5.8362, -6.2543, -5.9606, -6.0595, -6.2514, -6.2434, -6.279, -6.1854, -6.2373, -6.1889, -7.0022, -7.0026, -7.4151, -7.4904, -7.4766, -7.3788, -7.4715, -7.505, -7.7085, -7.7267, -7.7577, -7.9235, -8.0534, -8.062, -7.3189, -7.2868, -8.0905, -8.1289, -8.1316, -8.13, -8.1299, -8.204, -8.2069, -8.2069, -8.2497, -8.2515, -8.2525, -8.2638, -8.0835, -8.2896, -7.4079, -7.4539, -7.3426, -7.5789, -7.0261, -7.2642, -6.7568, -6.8801, -7.1671, -6.8448, -7.6658, -7.8671, -6.3508, -5.5661, -7.4269, -7.6492, -7.1653, -3.5325, -7.1823, -4.8218, -6.2049, -5.9694, -4.5922, -7.1646, -5.3822, -5.8652, -5.437, -5.5953, -5.2222, -6.3357, -5.4088, -6.5675, -5.6093, -5.9089, -5.4436, -5.4365, -5.538, -5.853, -5.8829, -5.9607, -6.0489, -5.8075, -5.7657, -5.9348, -5.7936, -6.0687, -6.0206, -5.8414, -5.9921, -5.8365, -6.0166, -6.0008, -6.6387, -6.5223, -7.233, -7.3593, -7.2844, -6.7521, -7.4356, -7.2293, -7.5172, -7.5664, -7.5676, -7.571, -7.6135, -7.662, -7.7884, -7.5408, -7.8223, -7.8291, -7.8276, -7.8885, -7.8929, -7.9068, -7.916, -7.0356, -7.999, -8.023, -8.0244, -8.0624, -8.1389, -8.1401, -8.1015, -7.136, -6.4158, -7.2154, -6.7332, -6.8017, -7.011, -6.7532, -6.9316, -6.0505, -5.8463, -7.2376, -6.6362, -7.368, -6.4407, -5.0633, -5.8696, -6.6404, -7.0862, -7.2235, -6.5861, -7.2112, -6.4452, -5.2576, -6.5556, -5.5235, -5.5115, -4.9317, -5.3831, -5.3696, -5.9498, -6.5339, -5.551, -5.0797, -5.9834, -5.5413, -5.9255, -5.8912, -5.4144, -5.7951, -5.7696, -5.7325, -5.9216, -5.9497, -6.1956, -5.8549, -5.9384, -5.9348, -5.9428, -6.0353, -6.1126, -6.0718, -6.7514, -6.8731, -6.9642, -6.9994, -7.251, -7.2352, -7.5018, -7.5545, -7.5588, -7.6507, -7.6346, -7.3523, -7.6869, -7.7206, -7.6283, -7.7514, -7.7586, -7.7594, -7.7419, -7.8256, -7.0676, -7.8915, -7.9003, -7.9036, -7.9069, -7.9077, -7.9041, -7.9472, -7.9516, -7.9566, -7.1051, -7.3462, -6.7774, -7.502, -7.4067, -5.4445, -6.8817, -7.3274, -6.9765, -6.9695, -6.1294, -6.9194, -7.059, -6.4375, -7.1686, -6.4585, -6.811, -6.6172, -5.6987, -5.2852, -5.8477, -6.0106, -6.7741, -5.6092, -6.478, -6.0138, -4.5578, -5.2193, -5.8628, -5.9242, -6.1782, -6.0945, -5.0861, -5.7485, -5.6985, -5.0198, -5.6342, -5.5329, -5.4889, -5.4182, -5.9215, -5.761, -5.9033, -5.9279, -5.6706, -5.8356, -6.0068, -5.8853, -5.9477, -5.9559, -6.0045, -5.7153, -6.1182, -6.3337, -6.3317, -6.5464, -6.629, -6.6712, -6.7196, -6.8064, -5.9069, -7.0409, -7.0457, -7.074, -7.0494, -7.1472, -6.9584, -7.1655, -6.5436, -7.2561, -7.3345, -6.2811, -6.8144, -7.4038, -7.4211, -7.4205, -7.5102, -7.5176, -7.5176, -7.515, -7.5201, -6.6815, -5.9036, -6.8991, -5.8756, -5.5216, -5.5785, -6.7962, -6.1196, -6.9388, -5.7985, -6.3149, -5.1583, -5.8393, -5.4896, -6.5252, -5.3137, -6.0988, -5.5552, -5.4936, -4.7653, -6.2447, -5.2152, -5.5874, -5.2396, -5.8521, -5.4586, -5.4287, -6.1638, -5.4424, -5.5355, -5.9295, -5.8885, -5.5676, -5.4257, -5.7305, -5.5197, -5.7084, -5.8205, -5.8805, -4.1929, -5.7807, -5.8913, -6.1243, -5.2145, -6.4673, -6.4284, -5.7717, -6.601, -5.6753, -6.665, -6.6703, -6.6739, -6.7043, -6.7545, -6.1652, -4.6256, -6.8476, -6.875, -6.8838, -6.9207, -7.0159, -7.0258, -7.1402, -7.1424, -6.1919, -6.4526, -6.9498, -5.4273, -6.6526, -4.4177, -6.1041, -6.0445, -5.5665, -5.5632, -6.0673, -5.714, -5.3922, -6.1553, -3.6916, -5.791, -6.0011, -6.3328, -6.1604, -5.686, -6.2581, -6.0428, -6.0134, -6.0707, -5.2262, -5.5416, -5.8257, -5.7482, -6.018, -6.008, -6.02, -6.0595], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.241, 1.2346, 1.231, 1.2256, 1.2222, 1.22, 1.2182, 1.218, 1.2175, 1.2144, 1.214, 1.2137, 1.2135, 1.2096, 1.2092, 1.2085, 1.2068, 1.2034, 1.2029, 1.2027, 1.2023, 1.2017, 1.2004, 1.1998, 1.1996, 1.1987, 1.1985, 1.1975, 1.1972, 1.1954, 1.1901, 1.1909, 1.191, 1.181, 1.1795, 1.1402, 1.1628, 1.183, 1.1309, 1.127, 1.132, 1.1696, 1.1309, 0.8567, 1.1264, 1.0492, 0.7492, 0.2829, 0.944, 1.0603, 0.4784, 0.5604, 0.4555, 0.9582, 0.3107, 0.3525, 0.3269, 0.2476, 0.3203, 0.3458, 0.8532, 0.0103, 0.578, 0.2511, 0.348, 0.1091, -0.196, 0.0308, 0.5811, 0.1992, 0.1024, 0.5783, -0.0478, -0.0885, 0.3255, 0.059, -0.1409, -0.0127, -0.0029, -0.0859, 0.1093, -0.1109, -0.0048, -0.1857, -0.0712, -0.3651, -0.0023, 1.4959, 1.4923, 1.4858, 1.4839, 1.4832, 1.4813, 1.4808, 1.4768, 1.4761, 1.469, 1.4689, 1.4676, 1.4655, 1.4633, 1.4624, 1.461, 1.4593, 1.4559, 1.4556, 1.4552, 1.448, 1.4463, 1.4434, 1.4433, 1.4414, 1.4399, 1.4382, 1.4362, 1.4343, 1.4331, 1.4256, 1.4297, 1.4267, 1.3874, 1.3796, 1.4127, 1.4213, 1.3424, 1.3373, 1.4172, 1.3881, 1.375, 1.2718, 1.1904, 1.3981, 1.2987, 1.3864, 1.3507, 1.3033, 1.1029, 1.2986, 1.0115, 1.0624, 1.258, 0.9801, 1.1371, 1.1511, 1.2697, 0.8493, 1.0838, 0.8991, 0.7883, 1.1149, 0.7256, 1.0826, 0.9382, 0.9408, 0.8764, 0.9698, 0.7821, 0.8523, 0.7307, 0.8325, 0.8171, 0.65, 0.7947, 0.6712, 0.3395, 0.6277, 0.9848, 0.4911, 0.4519, 0.604, 0.5821, 0.7414, 0.7579, 0.3637, -0.8153, 0.2199, 0.2243, 1.8984, 1.8939, 1.8793, 1.8698, 1.868, 1.8629, 1.8624, 1.8588, 1.8544, 1.8523, 1.8497, 1.8465, 1.8465, 1.8463, 1.8461, 1.8445, 1.8442, 1.8439, 1.8434, 1.8395, 1.8385, 1.8355, 1.8348, 1.8319, 1.8308, 1.8292, 1.829, 1.8288, 1.8288, 1.8284, 1.8246, 1.8266, 1.8186, 1.815, 1.7934, 1.8166, 1.8041, 1.7663, 1.7843, 1.559, 1.7087, 1.3192, 1.6864, 1.6462, 1.6611, 1.5321, 1.5922, 1.7778, 1.2199, 1.0255, 1.3474, 0.7902, 1.4807, 1.4038, 0.6183, 0.7982, 1.0187, 0.3471, 1.1532, 1.4007, 1.0061, 0.0213, 0.8852, -0.1343, 1.0718, 0.7807, 0.7655, 0.1551, 0.202, 0.4653, 0.3471, 0.5381, 0.4941, 0.2321, 0.2531, -0.0413, 0.0258, 0.2054, -0.0747, -0.0981, 0.1896, 0.0309, 0.1477, 0.1026, -0.3277, -0.5511, 0.0574, -0.1163, -0.2128, 0.102, -0.5165, 2.3296, 2.3128, 2.3068, 2.3031, 2.3014, 2.2977, 2.2971, 2.2964, 2.2891, 2.286, 2.2845, 2.2842, 2.2823, 2.2792, 2.276, 2.2746, 2.2741, 2.2722, 2.2701, 2.2692, 2.2688, 2.2663, 2.2647, 2.2638, 2.2636, 2.2621, 2.2612, 2.2605, 2.2602, 2.2595, 2.2538, 2.2307, 2.2468, 2.193, 2.1414, 2.1276, 2.0291, 2.1968, 2.1084, 2.2244, 1.993, 2.1454, 2.0505, 1.7508, 2.0551, 1.6732, 1.6493, 2.0094, 2.1243, 0.3283, 1.905, 1.344, 1.3448, 1.2241, 1.1149, 1.0668, 0.4839, 1.4521, 1.5125, 1.6672, 0.8665, 1.0917, 0.5338, 0.7428, 1.6179, -0.1327, 1.6911, 0.2085, 0.5518, 0.5549, 0.9187, 0.2761, 0.0609, 0.1647, 1.2184, 0.3823, -0.3427, -0.1888, 0.1642, 0.1283, -0.1234, -0.3161, -0.7009, -0.1147, 0.3649, 0.0849, 0.5091, -0.2022, -0.0001, -0.5371, -0.6035, -0.4788, 2.437, 2.3955, 2.3945, 2.3929, 2.3911, 2.3842, 2.3824, 2.3811, 2.3758, 2.3723, 2.372, 2.3717, 2.3708, 2.3682, 2.3679, 2.3676, 2.3662, 2.3646, 2.3615, 2.3603, 2.36, 2.3577, 2.3556, 2.3547, 2.3535, 2.353, 2.3504, 2.3495, 2.3494, 2.3482, 2.3479, 2.324, 2.312, 2.3044, 2.2592, 2.2491, 2.2875, 2.2562, 2.243, 2.1365, 2.2176, 2.2087, 2.2752, 2.279, 2.0787, 2.0578, 2.0367, 1.9262, 1.9815, 2.1084, 1.8099, 1.8065, 0.2718, 1.8578, 1.6779, 1.5867, 0.034, 1.3183, 0.3886, 0.1678, 0.6556, 0.044, 0.417, -0.3372, 0.6418, 0.8582, 0.5832, 0.4065, 0.0468, 0.153, 0.4332, -0.0789, 0.7169, 0.2009, -0.1116, -0.1632, -0.3375, -0.2796, 0.0237, -0.4282, 0.5208, -0.3397, -0.414, 0.2838, 0.2274, 0.3535, -0.3729, -0.0079, -0.7527, 2.8116, 2.8113, 2.7887, 2.7858, 2.7856, 2.7841, 2.7789, 2.7773, 2.7705, 2.7678, 2.7664, 2.7482, 2.7368, 2.7365, 2.7361, 2.7335, 2.7314, 2.7296, 2.7293, 2.7292, 2.7289, 2.7204, 2.7202, 2.7202, 2.7148, 2.7145, 2.7145, 2.7127, 2.7113, 2.7095, 2.7026, 2.6951, 2.6766, 2.6845, 2.6541, 2.646, 2.5635, 2.5595, 2.5659, 2.4855, 2.6382, 2.6775, 2.3, 2.022, 2.5452, 2.5981, 2.4191, 0.6882, 2.3803, 0.8327, 1.6654, 1.306, 0.0121, 2.289, 0.5759, 0.9925, 0.5499, 0.5354, 0.0639, 1.3802, 0.2426, 1.6597, 0.4236, 0.7015, -0.0074, -0.0271, 0.1075, 0.4355, 0.4785, 0.51, 0.58, 0.148, -0.0644, 0.2578, -0.1728, 0.475, 0.2087, -0.3944, 0.0519, -0.6435, 0.0145, -0.5929, 2.9254, 2.9246, 2.8995, 2.892, 2.89, 2.89, 2.8871, 2.8871, 2.8813, 2.8776, 2.8775, 2.877, 2.8738, 2.8691, 2.858, 2.8579, 2.8552, 2.8546, 2.8491, 2.8486, 2.8481, 2.8466, 2.8457, 2.8376, 2.8366, 2.8338, 2.8317, 2.8291, 2.8192, 2.8192, 2.819, 2.7947, 2.7339, 2.7661, 2.7274, 2.7263, 2.689, 2.6347, 2.6537, 2.466, 2.4148, 2.6918, 2.4973, 2.698, 2.318, 1.6259, 1.7185, 2.1587, 2.4827, 2.5841, 1.9975, 2.5572, 1.8305, 0.4437, 1.8487, 0.5093, 0.4439, -0.3273, 0.2683, 0.0383, 0.9078, 1.763, 0.1035, -0.859, 0.7407, -0.0943, 0.6182, 0.5081, -0.5421, 0.1995, 0.0429, -0.1551, 0.1317, 0.0084, 0.7946, -0.5688, -0.2862, -0.4649, -0.5066, -0.1354, 0.0799, -0.8788, 3.1205, 3.1163, 3.1103, 3.1097, 3.0939, 3.093, 3.0744, 3.0694, 3.0688, 3.0595, 3.0593, 3.0561, 3.0554, 3.0528, 3.0522, 3.0497, 3.0489, 3.0487, 3.0462, 3.0405, 3.0376, 3.033, 3.0322, 3.0314, 3.0314, 3.0304, 3.0299, 3.0262, 3.0253, 3.025, 3.003, 3.0041, 2.9732, 2.9939, 2.9784, 2.7582, 2.9048, 2.9434, 2.8475, 2.8403, 2.6197, 2.827, 2.8574, 2.6239, 2.8544, 2.5713, 2.6646, 2.558, 2.0595, 1.6128, 1.9203, 1.9008, 2.4746, 1.511, 2.2234, 1.7021, 0.0466, 0.4819, 1.15, 1.2236, 1.5465, 1.4196, -0.2138, 0.7136, 0.494, -0.7991, 0.3526, 0.088, -0.0419, -0.2252, 0.8047, 0.3697, 0.7172, 0.7046, -0.2343, -0.1901, 0.4273, -0.476, -0.1807, -0.3037, -0.2242, 3.974, 3.9566, 3.9428, 3.9301, 3.9269, 3.9214, 3.9178, 3.9129, 3.9047, 3.8886, 3.8785, 3.875, 3.8742, 3.8694, 3.8639, 3.8621, 3.8615, 3.8552, 3.8447, 3.8366, 3.8354, 3.8306, 3.825, 3.8222, 3.8196, 3.8052, 3.805, 3.805, 3.8047, 3.8045, 3.8043, 3.7546, 3.7777, 3.7226, 3.6736, 3.6127, 3.7354, 3.5888, 3.7336, 3.4162, 3.5171, 3.043, 3.1711, 2.9104, 3.4986, 2.5976, 3.0845, 2.4885, 2.281, 1.4272, 3.08, 1.6827, 2.1707, 1.5072, 2.4291, 1.5316, 1.2139, 2.7985, 0.6464, 0.8963, 1.8814, 1.6916, 0.1337, -0.8214, 0.7717, -1.299, -0.2614, 0.7231, -0.1135, 4.5111, 4.496, 4.4788, 4.4691, 4.4404, 4.4329, 4.4295, 4.4218, 4.415, 4.4082, 4.4064, 4.4056, 4.4051, 4.3977, 4.3911, 4.39, 4.3746, 4.3739, 4.372, 4.3709, 4.3646, 4.3468, 4.3447, 4.3212, 4.3207, 4.3188, 4.3161, 4.3161, 4.2716, 4.268, 4.2032, 4.2148, 4.2065, 4.1439, 4.0522, 4.1522, 4.0221, 3.8626, 4.114, 3.0552, 3.8969, 3.8839, 4.0658, 3.8516, 3.2516, 3.9033, 3.4627, 3.3452, 3.2356, 0.21, 1.1009, 2.192, 1.2025, 2.032, 0.6023, 0.814, 1.5073]}, \"token.table\": {\"Topic\": [9, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 10, 3, 1, 2, 3, 5, 8, 9, 1, 3, 5, 6, 7, 9, 4, 6, 7, 9, 3, 5, 4, 5, 1, 3, 4, 6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 4, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 5, 6, 8, 9, 1, 7, 1, 3, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 5, 7, 5, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 10, 10, 3, 4, 7, 3, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 3, 5, 8, 2, 3, 4, 5, 6, 4, 9, 4, 10, 3, 4, 1, 1, 2, 4, 1, 4, 10, 6, 1, 3, 7, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 9, 7, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 3, 3, 8, 3, 4, 7, 1, 2, 3, 4, 5, 7, 4, 9, 7, 1, 3, 4, 6, 7, 9, 1, 1, 7, 9, 1, 2, 3, 8, 6, 1, 6, 8, 1, 2, 7, 10, 1, 5, 6, 9, 10, 4, 6, 7, 8, 3, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 6, 7, 8, 4, 5, 9, 8, 1, 4, 5, 6, 9, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 1, 6, 7, 1, 4, 7, 9, 10, 2, 7, 1, 3, 2, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 3, 1, 2, 3, 4, 6, 7, 8, 1, 1, 2, 7, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 7, 1, 2, 3, 7, 1, 3, 10, 1, 4, 5, 10, 1, 7, 8, 2, 3, 1, 2, 4, 5, 8, 5, 7, 1, 2, 4, 5, 7, 8, 9, 1, 8, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 7, 9, 3, 6, 1, 5, 7, 1, 2, 5, 7, 1, 3, 4, 7, 1, 4, 6, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 8, 10, 9, 8, 1, 2, 3, 4, 5, 7, 8, 10, 1, 5, 1, 1, 3, 4, 5, 6, 9, 7, 5, 1, 1, 1, 7, 6, 6, 10, 1, 2, 3, 4, 5, 7, 4, 4, 3, 5, 2, 1, 3, 1, 3, 5, 5, 1, 5, 1, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 9, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 5, 6, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 8, 4, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 9, 1, 4, 8, 1, 2, 3, 4, 5, 7, 10, 3, 5, 1, 4, 5, 6, 7, 1, 2, 5, 1, 3, 4, 5, 6, 7, 9, 9, 10, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 1, 2, 3, 6, 7, 8, 1, 2, 3, 5, 3, 1, 2, 3, 4, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 3, 10, 7, 3, 4, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 2, 4, 5, 6, 7, 8, 9, 10, 3, 4, 10, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 3, 6, 7, 8, 5, 3, 4, 5, 7, 8, 9, 1, 2, 5, 6, 8, 3, 4, 6, 9, 10, 6, 1, 4, 6, 8, 6, 1, 2, 3, 4, 6, 9, 3, 1, 2, 5, 7, 5, 9, 6, 1, 2, 3, 4, 5, 6, 7, 9, 2, 3, 5, 9, 4, 5, 7, 9, 10, 1, 1, 2, 3, 4, 5, 7, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 2, 3, 2, 3, 6, 8, 1, 3, 4, 5, 6, 7, 3, 2, 2, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 4, 1, 4, 9, 2, 5, 6, 1, 2, 3, 4, 5, 6, 7, 9, 1, 3, 4, 8, 9, 1, 3, 4, 6, 10, 1, 2, 3, 6, 9, 2, 5, 6, 7, 9, 3, 10, 1, 5, 7, 9, 10, 2, 3, 4, 5, 10, 1, 2, 4, 7, 1, 3, 4, 7, 9, 10, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 8, 5, 8, 8, 3, 5, 7, 4, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 9, 3, 6, 6, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 9, 4, 8, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 7, 1, 2, 3, 5, 6, 7, 8, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 3, 5, 4, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 2, 1, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 2, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 3, 5, 4, 3, 4, 6, 6, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 9, 10, 4, 1, 3, 4, 5, 7, 8, 9, 2, 4, 10, 1, 2, 3, 5, 6, 7, 10, 2, 7, 8, 6, 4, 7, 2, 3, 4, 1, 4, 8, 3, 7, 2, 3, 4, 5, 8, 9, 2, 5, 1, 4, 6, 10, 6, 3, 2, 6, 9, 4, 7, 2, 1, 7, 10, 5, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 2, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 7, 9, 1, 4, 8, 9, 2, 3, 7, 3, 4, 6, 3, 8, 1, 2, 3, 5, 7, 8, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 4, 7, 1, 5, 1, 4, 7, 10, 8, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 1, 3, 4, 5, 6, 7, 8, 10, 3, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 8, 9, 10, 4, 6, 1, 2, 3, 4, 5, 7, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 4, 5, 6, 8, 9, 10, 4, 8, 1, 3, 5, 6, 8, 1, 10, 8, 4, 10, 7, 7, 4, 5, 8, 1, 4, 2, 4, 5, 8, 10, 1, 2, 4, 7, 5, 10, 6, 10, 9, 3, 1, 9, 5, 8, 8, 7, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 5, 1, 2, 3, 5, 7, 1, 2, 10, 3, 9, 4, 10, 2, 4, 3, 9, 7, 4, 1, 10, 7, 8, 1, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 1, 3, 6, 10, 1, 8, 8, 5, 5, 4, 9, 7, 9, 1, 2, 4, 6, 9, 10, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 9, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 1, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 7, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 4, 7, 1, 6, 3, 6, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 9, 9, 1, 2, 4, 5, 6, 7, 8, 3, 3, 7, 8, 1, 2, 5, 6, 7, 2, 8, 9, 2, 9, 2, 2, 3, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 5, 6, 2, 2, 3, 8, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 3, 4, 1, 2, 3, 4, 5, 7, 8, 1, 2, 5, 3, 7, 1, 4, 6, 7, 8, 9, 8, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 5, 10, 1, 5, 1, 2, 3, 4, 5, 7, 8, 10, 8, 1, 2, 3, 5, 7, 8, 5, 1, 4, 9, 4, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 3, 5, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 4, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 7, 10, 1, 3, 4, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 4, 5, 7, 8, 3, 6, 7, 9, 1, 6, 1, 2, 3, 4, 5, 6, 7, 10, 8, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 2, 3, 5, 1, 3, 8, 10, 9, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 7, 1, 2, 3, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 8, 1, 1, 2, 3, 4, 5, 6, 10, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 5, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 8, 1, 3, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 7], \"Freq\": [0.8592322092330352, 0.9499246009246861, 0.349587085364527, 0.1398348341458108, 0.027966966829162162, 0.055933933658324324, 0.04195045024374324, 0.37755405219368915, 0.21239711117213114, 0.5119314987225725, 0.09585100401614124, 0.041390206279697354, 0.034854910551324086, 0.0588176615553594, 0.034854910551324086, 0.007624511683102144, 0.0010892159547288777, 0.28606753626408804, 0.25872284529766787, 0.03155156649971559, 0.07572375959931742, 0.04206875533295412, 0.01682750213318165, 0.2629297208309633, 0.025241253199772475, 0.2697624615528811, 0.3379608366645645, 0.1591295419272613, 0.06971389455860971, 0.012124155575410385, 0.09244668626250419, 0.05455870008934673, 0.004546558340778895, 0.9448078706243769, 0.9618192134155433, 0.07775742207511714, 0.03887871103755857, 0.8553316428262885, 0.9108414891333232, 0.8680306266138134, 0.17346922220001945, 0.06938768888000778, 0.017346922220001944, 0.26020383330002916, 0.45101997772005054, 0.017346922220001944, 0.03745950401169993, 0.03745950401169993, 0.6368115681988988, 0.26221652808189955, 0.10664826740281727, 0.8531861392225382, 0.9092232247349659, 0.8394504812614716, 0.17704606849621965, 0.029507678082703277, 0.08852303424810982, 0.6786765959021753, 0.9709183231910268, 0.3610587303430986, 0.10808265680552627, 0.12922926357182488, 0.10025058022541566, 0.07283831219502857, 0.054041328402763135, 0.08067038877513917, 0.06813906624696221, 0.020363399108287557, 0.004699245948066359, 0.8996687060637667, 0.18475961748102354, 0.8006250090844352, 0.9585421241422643, 0.050148852768199324, 0.06582036925826161, 0.31656463309925825, 0.2883559034171461, 0.07835758245031145, 0.018805819788074746, 0.08462618904633636, 0.06582036925826161, 0.0062686065960249155, 0.025074426384099662, 0.8628873547247287, 0.04473478752227585, 0.0894695750445517, 0.04473478752227585, 0.7157566003564136, 0.0894695750445517, 0.16116002755841363, 0.7520801286059302, 0.08209103723214115, 0.1641820744642823, 0.21890943261904303, 0.4925462233928468, 0.2782232113212053, 0.19609708267819892, 0.17598456137787083, 0.06704173766776031, 0.08883030240978242, 0.05530943357590226, 0.035196912275574165, 0.0854782155263944, 0.015084390975246071, 0.0016760434416940079, 0.9426978034523734, 0.8802768148834659, 0.1056332177860159, 0.9129073809589039, 0.09370760295587867, 0.843368426602908, 0.39032197279181186, 0.19241224010863966, 0.16125978218628848, 0.05130993069563724, 0.09712236881674192, 0.038482448021727934, 0.043979940596260494, 0.007329990099376749, 0.01282748267390931, 0.0036649950496883744, 0.1272797629553553, 0.04242658765178511, 0.8273184592098096, 0.8509988176588004, 0.7678599348101443, 0.22395914765295874, 0.9072548234886532, 0.9607795739606294, 0.20761805248242537, 0.6482767761185936, 0.03389682489508986, 0.03389682489508986, 0.01694841244754493, 0.004237103111886233, 0.046608134230748555, 0.004237103111886233, 0.004237103111886233, 0.234333422944129, 0.32035455288564474, 0.19132285797337115, 0.044493687900783986, 0.12903169491227356, 0.02076372102036586, 0.019280598090339728, 0.02966245860052266, 0.007415614650130665, 0.0029662458600522657, 0.14397654522692835, 0.7009767151452471, 0.043629256129372226, 0.039993484785257875, 0.04144779332290362, 0.002908617075291482, 0.018178856720571762, 0.008725851225874446, 0.2728838373875572, 0.7163200731423377, 0.960483860340941, 0.07439089688241676, 0.31244176690615044, 0.04463453812945006, 0.5207362781769174, 0.029756358752966706, 0.9545097506652632, 0.935015109560277, 0.9358047777878729, 0.8177108584859571, 0.8750342160937399, 0.05147260094669058, 0.9748137955113779, 0.061650443746504614, 0.061650443746504614, 0.8631062124510646, 0.8718564529049232, 0.09341319138267035, 0.9008900300894163, 0.957043583333692, 0.10929309071546162, 0.7650516350082314, 0.10929309071546162, 0.027323272678865404, 0.9723104558553172, 0.876277809159329, 0.3437121162851411, 0.19204232528947565, 0.12984679948549774, 0.10256806009778813, 0.03928138471830184, 0.08838311561617913, 0.08620081646516237, 0.00982034617957546, 0.00763804702855869, 0.3150983627951374, 0.6301967255902748, 0.024238335599625956, 0.23633266534206798, 0.4715661089848705, 0.08683851424196916, 0.03077820757943211, 0.06595330195592594, 0.046167311369148166, 0.03627431607575927, 0.02198443398530865, 0.0010992216992654325, 0.002198443398530865, 0.23831528968470603, 0.7149458690541182, 0.8654859579762159, 0.9066580231175374, 0.880262666977926, 0.388537138993808, 0.26193514988346606, 0.10477405995338643, 0.03492468665112881, 0.052387029976693214, 0.026193514988346607, 0.028376307904042157, 0.09386009537490868, 0.004365585831391101, 0.0021827929156955504, 0.2461052857906319, 0.09986881162518396, 0.34954084068814384, 0.19973762325036792, 0.032100689450951986, 0.014266973089311994, 0.010700229816983995, 0.046367662540263976, 0.8813632021110738, 0.8775842859961238, 0.08775842859961239, 0.16259343796411668, 0.8129671898205834, 0.944728665975708, 0.6647455558185992, 0.12203058512611964, 0.019267987125176786, 0.08349461087576607, 0.019267987125176786, 0.089917273250825, 0.9072259375768683, 0.060481729171791225, 0.8572581909824838, 0.09387577830065556, 0.10952174135076481, 0.4928478360784417, 0.17992857507625648, 0.11734472287581944, 0.00782298152505463, 0.9555655888950794, 0.047560294686298014, 0.6658441256081722, 0.23780147343149008, 0.886098592007381, 0.10924503189132094, 0.012138336876813437, 0.8837556098659689, 0.8991794363925075, 0.06583123011991857, 0.7570591463790636, 0.16457807529979643, 0.2592655009966219, 0.4321091683277031, 0.02880727788851354, 0.2592655009966219, 0.22137185422617706, 0.08854874169047082, 0.04427437084523541, 0.04427437084523541, 0.5755668209880603, 0.08984883611587936, 0.04492441805793968, 0.04492441805793968, 0.8086395250429143, 0.8803923843571945, 0.9231938342961373, 0.035507455165236054, 0.2819694736336729, 0.14354809566805168, 0.07177404783402584, 0.09740763634617793, 0.0871542009413171, 0.10253435404860835, 0.03588702391701292, 0.07690076553645626, 0.09740763634617793, 0.30095589438910253, 0.10437776684015117, 0.15308739136555505, 0.19831775699628723, 0.06784554844609826, 0.07480406623544167, 0.06436628955142655, 0.03131333005204535, 0.006958517789343411, 0.9644727491112597, 0.9590133958940549, 0.022833652283191782, 0.39082492328164475, 0.12825520738183174, 0.15350229544912147, 0.11108718749607474, 0.030296505680747658, 0.06665231249764485, 0.08079068181532709, 0.030296505680747658, 0.0030296505680747657, 0.005049417613457943, 0.45463922078007335, 0.2082928588595931, 0.1682365398481329, 0.06509151839362284, 0.011015487728151557, 0.041057726986746715, 0.024033791406876125, 0.014019711654011073, 0.005007039876432526, 0.009012671777578547, 0.98202035501516, 0.12002835357332278, 0.7801842982265981, 0.012002835357332278, 0.012002835357332278, 0.024005670714664556, 0.024005670714664556, 0.012002835357332278, 0.23458791936888748, 0.14335928405876458, 0.4235615210827135, 0.019548993280740622, 0.06516331093580208, 0.006516331093580208, 0.026065324374320832, 0.07167964202938229, 0.013032662187160416, 0.006516331093580208, 0.29236396912344537, 0.0487273281872409, 0.49170303898033996, 0.03986781397137892, 0.05315708529517189, 0.022148785539654953, 0.05315708529517189, 0.07261754255105582, 0.08714105106126699, 0.4937992893471796, 0.014523508510211164, 0.13071157659190047, 0.18880561063274515, 0.17710078665251616, 0.17710078665251616, 0.5903359555083872, 0.9696926753238261, 0.10073050957625618, 0.08058440766100494, 0.7252596689490445, 0.04029220383050247, 0.060438305745753707, 0.04030583799645472, 0.9270342739184586, 0.5028398373847222, 0.20113593495388887, 0.060340780486166666, 0.13274971706956665, 0.02011359349538889, 0.02011359349538889, 0.028159030893544445, 0.01609087479631111, 0.02011359349538889, 0.94320379434277, 0.91510257368397, 0.9642964604579409, 0.8824834932415038, 0.05883223288276691, 0.8723035228002427, 0.048461306822235706, 0.048461306822235706, 0.024230653411117853, 0.024230653411117853, 0.168075504860636, 0.8067624233310527, 0.06826485874878536, 0.8874431637342097, 0.19619885727390962, 0.7847954290956385, 0.972131216028925, 0.2500170963009287, 0.31683200996755617, 0.3211426495589515, 0.021553197956976612, 0.04095107611825556, 0.023708517752674272, 0.002155319795697661, 0.01508723856988363, 0.006465959387092984, 0.002155319795697661, 0.845298175398202, 0.9667165403890835, 0.06688931313115674, 0.1433342424239073, 0.009555616161593821, 0.07644492929275057, 0.02866684848478146, 0.009555616161593821, 0.6688931313115675, 0.9387263910144008, 0.022550529524816832, 0.33825794287225247, 0.022550529524816832, 0.5863137676452377, 0.8672480060312897, 0.233869994056866, 0.41722960478192367, 0.14876345775806565, 0.046358844975769295, 0.06504076757794498, 0.03044461461095297, 0.044283075797749774, 0.013146538127456964, 0.0013838461186796804, 0.012601946081554835, 0.8884371987496159, 0.018902919122332253, 0.06300973040777418, 0.012601946081554835, 0.056819578743637494, 0.8522936811545624, 0.028409789371818747, 0.056819578743637494, 0.015774229861903324, 0.03154845972380665, 0.9464537917141993, 0.7998493063765097, 0.03999246531882549, 0.11997739595647647, 0.019996232659412746, 0.15907706994713802, 0.05302568998237934, 0.7423596597533108, 0.910089277455964, 0.9072501601358155, 0.1352026271808315, 0.1622431526169978, 0.1352026271808315, 0.5272902460052429, 0.0270405254361663, 0.9493556485191998, 0.9445850871167059, 0.03881219845540643, 0.1552487938216257, 0.03881219845540643, 0.54337077837569, 0.05821829768310965, 0.1358426945939225, 0.05821829768310965, 0.18625863601989448, 0.7915992030845516, 0.8735403385827555, 0.16085665305663066, 0.10517550392164313, 0.16704344740496263, 0.11136229826997508, 0.08042832652831533, 0.012373588696663898, 0.05568114913498754, 0.28459254002326967, 0.012373588696663898, 0.10474607275222196, 0.5004534587050605, 0.03491535758407399, 0.011638452528024663, 0.3375151233127152, 0.09938399172373935, 0.7950719337899148, 0.05954326230257384, 0.8336056722360338, 0.11908652460514768, 0.031728556537156324, 0.1269142261486253, 0.8249424699660645, 0.8986718923061711, 0.04775307618531679, 0.04775307618531679, 0.8595553713357021, 0.9213885561755434, 0.21451502467162925, 0.01650115574397148, 0.04950346723191444, 0.5775404510390019, 0.11550809020780037, 0.01650115574397148, 0.8927446855457418, 0.3138110236438963, 0.123828133654078, 0.14248716749236373, 0.1645387529376105, 0.1323095126714806, 0.05428082571137666, 0.05597710151485718, 0.006785103213922082, 0.003392551606961041, 0.003392551606961041, 0.7904398888544105, 0.05269599259029403, 0.10539198518058807, 0.9297245207387416, 0.963646524993718, 0.24603721455999414, 0.04920744291199883, 0.25423845504532727, 0.09021364533866452, 0.1722260501919959, 0.04920744291199883, 0.05740868339733197, 0.08201240485333137, 0.08750773915266874, 0.8750773915266874, 0.9787215970276719, 0.05066258080396765, 0.5910634427129559, 0.01688752693465588, 0.11821268854259118, 0.16887526934655883, 0.03377505386931176, 0.9027499553023791, 0.9349446703213403, 0.9537189624226297, 0.9259488814569098, 0.9440773559208826, 0.033717048425745806, 0.9572086606316739, 0.06267189685270069, 0.8774065559378097, 0.05493750220358184, 0.1831250073452728, 0.558531272403082, 0.05493750220358184, 0.13734375550895458, 0.009156250367263639, 0.9066462800413749, 0.8789532846165957, 0.9087770925489912, 0.9464203960785128, 0.8739820863645216, 0.07120932287733946, 0.9257211974054129, 0.07580523861785511, 0.15161047723571022, 0.7959550054874786, 0.8983413483262365, 0.12806922672196425, 0.8324499736927676, 0.917283241382039, 0.05395783772835523, 0.9543466203486032, 0.17928809345893224, 0.37451290633643625, 0.11952539563928816, 0.07304329733512055, 0.08499583689904935, 0.07038717743202526, 0.026561199030952925, 0.04781015825571527, 0.019920899273214693, 0.0026561199030952923, 0.968347288133318, 0.9401505877860993, 0.29087099347225737, 0.33526094428328457, 0.13901063543453263, 0.06833716111697613, 0.05607151681392913, 0.03212430650798023, 0.05373520361334875, 0.020442740505078327, 0.0017522349004352852, 0.0029203915007254755, 0.10580428145825856, 0.5000644460500852, 0.18710651878934145, 0.06905121526749505, 0.044549171140319394, 0.031184419798223575, 0.05902765176092319, 0.0022274585570159697, 0.0011137292785079848, 0.8674244022392009, 0.8621370644668147, 0.9123031631891451, 0.24194342674443575, 0.03780366042881809, 0.07182695481475437, 0.27974708717325386, 0.0415840264716999, 0.2117004984013813, 0.04536439251458171, 0.056705490643227136, 0.007560732085763617, 0.011341098128645427, 0.9393793388999282, 0.15484598236656064, 0.7742299118328031, 0.03871149559164016, 0.8893186641028568, 0.36965360786610946, 0.15868057313276895, 0.1929411514227986, 0.05409564993162578, 0.11360086485641413, 0.0486860849384632, 0.018031883310541928, 0.025244636634758698, 0.009015941655270964, 0.010819129986325155, 0.9615101802790804, 0.0237653057303126, 0.7604897833700032, 0.0237653057303126, 0.0118826528651563, 0.1544744872470319, 0.0118826528651563, 0.18735169759099227, 0.08300391665423708, 0.543082768966294, 0.00948616190334138, 0.054545430944212936, 0.06403159284755432, 0.02371540475835345, 0.03320156666169483, 0.906598667149994, 0.21363108105584144, 0.03884201473742572, 0.2168679156172936, 0.06149985666759072, 0.20068374281003287, 0.003236834561452143, 0.035605180175973576, 0.19097323912567646, 0.032368345614521433, 0.006473669122904286, 0.8633557804576675, 0.05078563414456868, 0.9608728972823821, 0.920368155747327, 0.3741808144546864, 0.09425792897453766, 0.1266869328939438, 0.1306069223787072, 0.11243242567662243, 0.11474878309943716, 0.02191630484663163, 0.018887222062950834, 0.004989077526062485, 0.0012472693815156212, 0.4009799389404205, 0.08708150189110142, 0.13669770645696153, 0.15998694533481422, 0.05062878016924501, 0.10227013594187492, 0.05265393137601481, 0.007088029223694301, 0.0020251512067698003, 0.21109645912625324, 0.2721574183776488, 0.16748148823239925, 0.09944213363798705, 0.0976975348022329, 0.05059336623687061, 0.027913581372066544, 0.0296581802078207, 0.0383811743865915, 0.005233796507262477, 0.8996637244110802, 0.3126932797501102, 0.3071094711831439, 0.14916745743181276, 0.0837571285044938, 0.08934093707146006, 0.018346799577174833, 0.019144486515312868, 0.018346799577174833, 0.0007976869381380361, 0.0015953738762760723, 0.8167217682365252, 0.06477450895117325, 0.8420686163652523, 0.9113946430527853, 0.041427029229672056, 0.8638671582017715, 0.028720259250275198, 0.09573419750091733, 0.143601296251376, 0.11488103700110079, 0.4212304690040362, 0.18189497525174292, 0.019146839500183466, 0.8871217673566095, 0.8608469364715738, 0.10125308756776565, 0.6834583410824181, 0.02531327189194141, 0.12656635945970704, 0.05062654378388282, 0.13584633292485346, 0.033961583231213364, 0.8150779975491207, 0.19523660288015618, 0.050931287707866826, 0.5008243291273571, 0.042442739756555686, 0.0679083836104891, 0.11035112336704479, 0.025465643853933413, 0.06995244421253731, 0.8394293305504477, 0.9509101044710352, 0.9166314709985054, 0.19212009038215622, 0.6169710219589569, 0.09996492507689429, 0.012495615634611787, 0.015619519543264734, 0.01405756758893826, 0.02811513517787652, 0.01405756758893826, 0.006247807817305893, 0.12020365321681045, 0.8414255725176731, 0.05684855047378173, 0.1498734512490609, 0.020672200172284265, 0.43411620361796954, 0.2894108024119797, 0.04134440034456853, 0.07983418465846508, 0.8382589389138833, 0.05322278977231005, 0.026611394886155024, 0.8783230278458486, 0.09609999032414115, 0.008736362756740106, 0.6901726577824683, 0.11357271583762137, 0.052418176540440634, 0.01747272551348021, 0.008736362756740106, 0.3108854479756801, 0.0526924488094373, 0.08430791809509969, 0.36357789678511737, 0.03688471416660611, 0.11065414249981834, 0.010538489761887461, 0.02634622440471865, 0.07797947936153782, 0.15595895872307564, 0.7408050539346093, 0.1856783761162153, 0.46606018101459656, 0.08426368072743906, 0.03430202932267431, 0.12602267294634692, 0.018642407240583865, 0.03877620706041444, 0.03131924416418089, 0.011931140633973672, 0.002982785158493418, 0.14159575496541707, 0.28319150993083414, 0.01573286166282412, 0.04719858498847236, 0.09439716997694472, 0.01573286166282412, 0.393321541570603, 0.01573286166282412, 0.09607315562843044, 0.7685852450274435, 0.9552440947923141, 0.04253464914991691, 0.8932276321482551, 0.8870015015919213, 0.22114368570820991, 0.4475842261639219, 0.10328866757628967, 0.02516006005063466, 0.10461288126316517, 0.030456914798136697, 0.03707798323251424, 0.010593709495004069, 0.021187418990008137, 0.17442457643164969, 0.22384487308728376, 0.1453538136930414, 0.014535381369304141, 0.17442457643164969, 0.058141525477216564, 0.06686275429879905, 0.1337255085975981, 0.005814152547721657, 0.0029070762738608283, 0.7879633907212727, 0.9230300040342875, 0.24498319540978655, 0.4687659219860339, 0.1045889795787935, 0.0668992572080571, 0.021671590363173426, 0.030151777896589113, 0.021671590363173426, 0.03344962860402855, 0.004240093766707844, 0.003768972237073639, 0.9637612442517469, 0.02050555838833504, 0.09495005942680722, 0.03798002377072289, 0.6266703922169277, 0.11394007131216867, 0.03798002377072289, 0.018990011885361445, 0.03798002377072289, 0.018990011885361445, 0.040824720594863494, 0.040824720594863494, 0.898143853086997, 0.20472644495929468, 0.1706053707994122, 0.614179334877884, 0.2810245111010631, 0.26682380442308384, 0.08669905129713648, 0.11584787026772547, 0.03288584704374142, 0.13229079378959618, 0.05755023232654749, 0.018685140365762173, 0.007474056146304869, 0.8871425240013211, 0.2282634987602026, 0.646746579820574, 0.07608783292006754, 0.03804391646003377, 0.8965274649451482, 0.20214271477626772, 0.05053567869406693, 0.10107135738813386, 0.05053567869406693, 0.5558924656347362, 0.05053567869406693, 0.9306709146149839, 0.8834087463435686, 0.040154943015616756, 0.9452159418347255, 0.88021127640883, 0.18401000280106053, 0.026287143257294362, 0.15772285954376616, 0.5520300084031816, 0.026287143257294362, 0.9626986397159555, 0.7203310030763255, 0.04432806172777388, 0.03324604629583041, 0.188394262343039, 0.8717973401400821, 0.22852158312000403, 0.06529188089142972, 0.11426079156000202, 0.5060120769085803, 0.04896891066857229, 0.01632297022285743, 0.926109132114936, 0.051071598200921574, 0.8511933033486929, 0.03404773213394772, 0.051071598200921574, 0.9290677458920928, 0.8669207746728309, 0.8181271764857189, 0.3296918635203302, 0.015216547547092162, 0.1217323803767373, 0.3601249586145145, 0.02028873006278955, 0.08622710276685558, 0.0405774601255791, 0.030433095094184324, 0.058685299541339324, 0.08802794931200898, 0.4401397465600449, 0.38145444701870557, 0.8560419763741549, 0.19620851423532074, 0.07848340569412829, 0.11772510854119243, 0.5886255427059622, 0.910818820782728, 0.07765724060742016, 0.849376069143658, 0.04368219784167384, 0.00485357753796376, 0.014560732613891281, 0.00485357753796376, 0.00485357753796376, 0.49354272964158463, 0.31089305804194306, 0.042747795480767174, 0.03303238741695645, 0.011658489676572865, 0.0077723264510485765, 0.06800785644667505, 0.029146224191432164, 0.0019430816127621441, 0.13700570803097564, 0.031137660916130825, 0.1432332402142018, 0.13077817584774948, 0.04359272528258316, 0.08718545056516631, 0.06850285401548782, 0.1805984333135588, 0.17437090113033263, 0.9384570093354434, 0.8875191327920549, 0.10240605378369864, 0.9024653189001366, 0.06016435459334244, 0.7813128168657438, 0.15626256337314876, 0.2665836218488775, 0.049984429096664534, 0.03332295273110969, 0.09996885819332907, 0.016661476365554845, 0.533167243697755, 0.9076320751481372, 0.9479137177560167, 0.9507819504889312, 0.9075238516542607, 0.8223400074955559, 0.2770188540989057, 0.03865379359519615, 0.1352882775831865, 0.31567264769410186, 0.012884597865065382, 0.12884597865065384, 0.04509609252772884, 0.025769195730130763, 0.012884597865065382, 0.8753623225850935, 0.9534659735503531, 0.0974638996090523, 0.24365974902263077, 0.63351534745884, 0.9065688634416799, 0.8858191050456939, 0.9307301368803843, 0.039088069821970106, 0.11726420946591033, 0.1465802618323879, 0.4299687680416712, 0.1074921920104178, 0.04886008727746264, 0.019544034910985053, 0.07817613964394021, 0.13977549004616002, 0.7221733652384934, 0.06988774502308001, 0.023295915007693336, 0.04659183001538667, 0.21510011167017626, 0.5838431602476213, 0.1382786432165419, 0.030728587381453752, 0.015364293690726876, 0.29912270195678725, 0.014956135097839364, 0.10469294568487554, 0.5683331337178958, 0.8050004344049357, 0.15629609998498928, 0.03125921999699786, 0.03125921999699786, 0.7189620599309507, 0.03125921999699786, 0.3018384736538926, 0.6036769473077852, 0.3566617931172207, 0.10190336946206305, 0.050951684731031524, 0.4076134778482522, 0.10190336946206305, 0.13736240815729034, 0.16025614285017206, 0.022893734692881723, 0.6639183060935699, 0.022893734692881723, 0.038080142951731295, 0.15232057180692518, 0.038080142951731295, 0.7235227160828946, 0.05806475065185671, 0.07258093831482089, 0.014516187662964178, 0.029032375325928356, 0.10161331364074924, 0.6967770078222806, 0.33897984491169014, 0.0467558406774745, 0.21040128304863526, 0.011688960169368625, 0.3272908847423215, 0.0467558406774745, 0.2700090670578475, 0.05968621482331366, 0.06252841552918574, 0.2728512677637196, 0.08526602117616237, 0.022737605646976633, 0.11368802823488317, 0.03126420776459287, 0.08242382047029029, 0.7961301525049641, 0.9608906625421271, 0.24172585371341837, 0.7251775611402551, 0.08183530410034433, 0.8183530410034433, 0.9053866595608979, 0.8148903852946903, 0.03395376605394543, 0.1358150642157817, 0.9423411038605461, 0.15114879863245015, 0.8313183924784758, 0.20260106515483722, 0.6299051298450393, 0.09945870471237464, 0.014734622920351798, 0.018418278650439747, 0.011050967190263848, 0.0036836557300879494, 0.007367311460175899, 0.014734622920351798, 0.23923709307899005, 0.08249554933758278, 0.40422819175415564, 0.14024243387389074, 0.04949732960254967, 0.08249554933758278, 0.22019865124699606, 0.483154322153603, 0.12399535701287157, 0.03527454121917898, 0.050239498100042794, 0.023516360812785988, 0.03634346671066925, 0.010689254914902722, 0.01603388237235408, 0.0010689254914902722, 0.08589208260458628, 0.8589208260458627, 0.14764627753674567, 0.7382313876837283, 0.8741320199815064, 0.2560508297786145, 0.026952718924064683, 0.013476359462032341, 0.471672581171132, 0.0943345162342264, 0.013476359462032341, 0.013476359462032341, 0.06738179731016171, 0.013476359462032341, 0.9307958115901536, 0.9074702952230097, 0.13370387204873493, 0.7831226791425903, 0.03820110629963855, 0.009550276574909637, 0.019100553149819274, 0.009550276574909637, 0.7467082135332319, 0.10667260193331884, 0.1422301359110918, 0.8824754292556376, 0.19942310757988693, 0.24755971975434238, 0.1971308879525319, 0.07793546733007076, 0.1444098365233664, 0.04355217291974542, 0.050428831801810486, 0.0206299766461952, 0.016045537391485155, 0.8142826070847977, 0.09979537455307547, 0.8165076099797084, 0.05443384066531389, 0.009072306777552315, 0.009072306777552315, 0.004536153388776157, 0.009072306777552315, 0.0609057816968061, 0.9135867254520914, 0.18050172006580872, 0.131164583247821, 0.3188863721162621, 0.08664082563158819, 0.051743826418865164, 0.09867427363597543, 0.04572710241667154, 0.061370584822374966, 0.015643482405703422, 0.009626758403509798, 0.8337353331184069, 0.28475663976394117, 0.44029598081147203, 0.08341010816614843, 0.04683272466705875, 0.061190202302215446, 0.015041167046354635, 0.03008233409270927, 0.034184470559896894, 0.0023929129391927827, 0.0017092235279948448, 0.9717923929868659, 0.9066020384166675, 0.9409689855160206, 0.12434609456762326, 0.7460765674057396, 0.9160780218790175, 0.3884061308190691, 0.2140647425536915, 0.14675572556515964, 0.11034265080087191, 0.031999368732252856, 0.06068845794047955, 0.007723985556061034, 0.03972335428831389, 0.001103426508008719, 0.9257018198453507, 0.20848639659621768, 0.07721718392452506, 0.45558138515469787, 0.03860859196226253, 0.015443436784905014, 0.06949546553207256, 0.09266062070943008, 0.030886873569810028, 0.007721718392452507, 0.9530305877646391, 0.9331819195059543, 0.9219718441320721, 0.04390342114914629, 0.37465344216168117, 0.17643563264590797, 0.1306930612191911, 0.07188118367055511, 0.04356435373973037, 0.03920791836575733, 0.10891088434932591, 0.04356435373973037, 0.006534653060959555, 0.004356435373973037, 0.3986990016989944, 0.21069459439377752, 0.1572105819707417, 0.07455347186241358, 0.05348401242303583, 0.030793825334475177, 0.061587650668950354, 0.008103638245914521, 0.003241455298365808, 0.8999480750938404, 0.04306568665873422, 0.12919705997620265, 0.17226274663493688, 0.6459852998810133, 0.19964411383840444, 0.251574663680764, 0.2688848469615505, 0.1234793074029438, 0.03231234212413482, 0.034620366561573024, 0.02423425659310112, 0.021926232155662917, 0.034620366561573024, 0.008078085531033705, 0.9391771497243173, 0.7822354683213726, 0.03878468678125095, 0.9308324827500227, 0.9575230499397652, 0.11182351971174732, 0.8107205179101682, 0.0838676397838105, 0.896727270980597, 0.8252196158330855, 0.07501996507573504, 0.28099055423948943, 0.3904105519963, 0.10154175791832018, 0.07703167842079461, 0.058649118797650446, 0.019257919605198653, 0.054272318887378025, 0.015756479676980715, 0.0026260799461634526, 0.0008753599820544842, 0.9316471485914627, 0.9511162558308356, 0.893437125031144, 0.2627614509713303, 0.38142791270031823, 0.1067998155560891, 0.056790378113158485, 0.0788284352913991, 0.03644755610247485, 0.03475232093491788, 0.0339047033511394, 0.0033904703351139397, 0.0050857055026709095, 0.2504053391399087, 0.2682914347927593, 0.09593451304710787, 0.08292644348139833, 0.0894304782642531, 0.08130043478568463, 0.060162321741406635, 0.021138113044278005, 0.0439022347842697, 0.006504034782854771, 0.9022287997713149, 0.3187266946499553, 0.06400134430721993, 0.16896354897106064, 0.03968083347047636, 0.13056274238672866, 0.07424155939637513, 0.05632118299035354, 0.06912145185179754, 0.07552158628251952, 0.0025600537722887972, 0.9028110255998221, 0.36247051498448446, 0.19406113725323168, 0.1606023204854331, 0.09368468694983598, 0.06803292742785708, 0.04461175569039809, 0.047957637367177945, 0.022305877845199044, 0.004461175569039809, 0.0022305877845199044, 0.9627358768753485, 0.07733787358952325, 0.1546757471790465, 0.721820153502217, 0.02577929119650775, 0.9116526469704657, 0.08218710436779995, 0.6739342558159596, 0.08218710436779995, 0.01643742087355999, 0.01643742087355999, 0.03287484174711998, 0.09862452524135994, 0.8649659899456265, 0.23790896350326396, 0.7137268905097919, 0.15195447910001061, 0.07091209024667162, 0.10130298606667375, 0.030390895820002123, 0.050651493033336874, 0.5774270205800404, 0.010130298606667374, 0.0589895472678827, 0.17696864180364807, 0.7078745672145923, 0.911818742058613, 0.11317357813997546, 0.8488018360498158, 0.22520530653637236, 0.04504106130727447, 0.7206569809163915, 0.23660084309139257, 0.6309355815770468, 0.10515593026284113, 0.8777575005530838, 0.8435922873844612, 0.1912435120570765, 0.054641003444878995, 0.013660250861219749, 0.5873907870324492, 0.054641003444878995, 0.06830125430609875, 0.8771086417353408, 0.9017560647621499, 0.03356532785235082, 0.8055678684564197, 0.10069598355705246, 0.06713065570470164, 0.9184024560583831, 0.9295545703490831, 0.924603137948094, 0.11293081271261378, 0.7905156889882965, 0.8648265982747338, 0.07862059984315763, 0.8944674832363096, 0.07180135252604082, 0.21540405757812248, 0.6462121727343674, 0.9147778288746785, 0.8815161558570362, 0.0863878730412113, 0.05279258908074024, 0.40314340752565275, 0.03359528396047106, 0.11518383072161507, 0.038394610240538356, 0.019197305120269178, 0.1727757460824226, 0.07678922048107671, 0.0047993262800672945, 0.906398022854394, 0.04770515909759968, 0.1729311919171679, 0.5863448225941474, 0.07565739646376096, 0.06935261342511422, 0.025219132154586987, 0.0045034164561762475, 0.02251708228088124, 0.03512664835817473, 0.008106149621117246, 0.8710342529350308, 0.0653275689701273, 0.0653275689701273, 0.38442665176507956, 0.26591361846563305, 0.09629183955580027, 0.0585158101916017, 0.056293690817237084, 0.06444146185657403, 0.02666543249237546, 0.034813203531712404, 0.009629183955580028, 0.002962825832486162, 0.03569889480365934, 0.04283867376439121, 0.014279557921463737, 0.08567734752878242, 0.10709668441097803, 0.09995690545024616, 0.08567734752878242, 0.28559115842927474, 0.24275248466488353, 0.34928421217528827, 0.17464210608764413, 0.11351736895696868, 0.2248517115878418, 0.02619631591314662, 0.06985684243505765, 0.037111447543624373, 0.006549078978286655, 0.8933185853213393, 0.05093026195860353, 0.8148841913376564, 0.10186052391720705, 0.38304646004993176, 0.057049472773394096, 0.03259969872765377, 0.08149924681913442, 0.1385487195925285, 0.03259969872765377, 0.016299849363826883, 0.03259969872765377, 0.22004796641166294, 0.935705329720664, 0.16062411528949933, 0.8031205764474967, 0.2812575688262541, 0.28666636822675895, 0.1752451005763583, 0.04002511556373616, 0.08762255028817915, 0.03569807604333225, 0.041106875443837135, 0.03569807604333225, 0.009735838920908796, 0.005408799400504886, 0.31095488002973826, 0.1254730217663856, 0.13456526972047153, 0.049098138952063934, 0.10910697544903096, 0.09274092913167632, 0.09637782831331068, 0.03455054222552647, 0.038187441407160835, 0.010910697544903097, 0.8987760446462683, 0.23141782843368766, 0.7328231233733443, 0.21849054432934223, 0.17479243546347378, 0.5462263608233555, 0.043698108865868446, 0.9090400984348147, 0.1724227174765196, 0.7882181370355181, 0.037224303220345145, 0.29779442576276116, 0.6328131547458674, 0.8995741990865744, 0.8745517260529088, 0.12399807410937365, 0.7784323541310679, 0.013777563789930405, 0.041332691369791216, 0.02755512757986081, 0.013777563789930405, 0.30208961575561144, 0.6473348909048816, 0.23170611219587284, 0.2994396145194971, 0.14801993171880817, 0.08237858390711054, 0.0886550474428904, 0.058318807019954455, 0.03739726190068828, 0.044196764064449785, 0.007845579419724814, 0.00183063519793579, 0.7288176794795103, 0.22425159368600317, 0.12268900042711905, 0.10331705299125815, 0.1485182636749336, 0.10331705299125815, 0.21954873760642357, 0.1291463162390727, 0.012914631623907269, 0.03874389487172181, 0.11623168461516542, 0.3171308259109222, 0.2308230694921269, 0.11039364193101721, 0.15655825582944258, 0.060214713780554846, 0.0381359853943514, 0.028100199764258926, 0.0381359853943514, 0.0020071571260184945, 0.016057257008147956, 0.9096592146245892, 0.9036298450547695, 0.02510082902929915, 0.02510082902929915, 0.8070105279155877, 0.16614922633556217, 0.05246813930311152, 0.7870220895466729, 0.10493627860622304, 0.9062858512272303, 0.8606953002396431, 0.8801003001731615, 0.2654296061207442, 0.2299262830090523, 0.13356012027731715, 0.09974743159951534, 0.10820060376896579, 0.09636616273173515, 0.025359516508351356, 0.021978247640571175, 0.006762537735560362, 0.015215709905010814, 0.8996322863760798, 0.04997957146533777, 0.6628658463880007, 0.019787040190686587, 0.059361120572059764, 0.04946760047671647, 0.039574080381373174, 0.039574080381373174, 0.11872224114411953, 0.009893520095343293, 0.22228404886429176, 0.6668521465928753, 0.9059092593312964, 0.27824131031800564, 0.39965570027495356, 0.11707816174419979, 0.03468982570198512, 0.06070719497847396, 0.04842121504235423, 0.01011786582974566, 0.04625310093598016, 0.00361352351062345, 0.00144540940424938, 0.16747412790778868, 0.8373706395389434, 0.25142264849181295, 0.03236133099399573, 0.23648664957150722, 0.11450932505567718, 0.037339997300764295, 0.15433865550982576, 0.1294453239759829, 0.03982933045414858, 0.0024893331533842864, 0.2170329701896831, 0.11368393676602448, 0.0826792267389269, 0.06200942005419517, 0.19636316350495137, 0.11368393676602448, 0.20669806684731723, 0.020669806684731725, 0.8832923842780286, 0.9188278175049474, 0.019923549652412224, 0.19923549652412226, 0.2390825958289467, 0.09961774826206113, 0.0796941986096489, 0.059770648957236674, 0.03984709930482445, 0.27892969513377114, 0.9088077604667913, 0.3633787743112329, 0.10109033571064373, 0.12567987682944895, 0.11201902065233495, 0.06557210965014729, 0.0737686233564157, 0.07650079459183849, 0.03551822606049645, 0.010928684941691214, 0.03551822606049645, 0.31398367549092543, 0.32238457650740504, 0.09135979855421576, 0.10711148796011503, 0.03990427982827815, 0.019952139914139074, 0.06300675762359707, 0.037804054574158245, 0.0010501126270599512, 0.003150337881179854, 0.8870068375562238, 0.22131739180557614, 0.19602397559922458, 0.17073055939287302, 0.06744910988360416, 0.1622994206574225, 0.054802401780428375, 0.08009581798677994, 0.04426347836111523, 0.9319083302239571, 0.899280246579766, 0.14702448404939794, 0.10414234286832354, 0.036756121012349485, 0.12252040337449828, 0.09189030253087371, 0.33080508911114537, 0.15927652438684775, 0.006126020168724914, 0.8810800912423514, 0.8810828451027065, 0.16691627737030745, 0.10014976642218447, 0.06676651094812298, 0.5675153430590454, 0.06676651094812298, 0.9323474527378115, 0.7470081362756343, 0.9187468555490573, 0.21330179856678227, 0.7252261151270597, 0.9136581814974731, 0.8465983015124017, 0.6926037648330647, 0.2770415059332259, 0.8718315202096788, 0.9370795065982783, 0.03748318026393113, 0.021287464925997958, 0.042574929851995916, 0.6599114127059367, 0.23416211418597754, 0.042574929851995916, 0.03547869129384678, 0.10643607388154035, 0.7805312084646292, 0.03547869129384678, 0.343507712904108, 0.601138497582189, 0.10043833235962919, 0.8035066588770335, 0.7969370743725372, 0.9497369027530075, 0.9305770894231006, 0.04897774154858424, 0.8958800487675705, 0.8788666274108404, 0.8478051555050397, 0.9305743256068016, 0.9134881714821876, 0.9231354656303993, 0.2628747021358386, 0.18574380150915543, 0.17944740145799762, 0.13852080112547183, 0.06138990049878865, 0.036204300294157415, 0.0865755007034199, 0.04879710039647303, 0.0015741000127894528, 0.11911590001412989, 0.26205498003108574, 0.23823180002825978, 0.05558742000659395, 0.1111748400131879, 0.09529272001130391, 0.007941060000941993, 0.007941060000941993, 0.09529272001130391, 0.26906426732335165, 0.49835381686846875, 0.06668114451056976, 0.05147316418359771, 0.015207980326972051, 0.05615254274574296, 0.01111352408509496, 0.02632150441206701, 0.003509533921608935, 0.0023396892810726233, 0.36230354451762964, 0.07833590151732532, 0.03916795075866266, 0.11750385227598799, 0.04895993844832833, 0.3035516183796356, 0.03916795075866266, 0.9231218981612306, 0.8659206998498681, 0.03732428928062836, 0.7464857856125671, 0.05598643392094253, 0.11197286784188507, 0.03732428928062836, 0.04861183005858909, 0.9236247711131926, 0.8024885428705665, 0.8595632845969884, 0.8654098060136349, 0.8992411738946574, 0.8676439722177426, 0.9217238999242537, 0.907604658635716, 0.1134323820102987, 0.7940266740720909, 0.9635617909297065, 0.9163989676885788, 0.9693766445309013, 0.7457242032125762, 0.09465722933987476, 0.8519150640588729, 0.25694302178130624, 0.205554417425045, 0.5138860435626125, 0.37322827947313103, 0.05637302137875416, 0.1049704536018181, 0.1457922966691918, 0.04276574035629626, 0.11663383733535344, 0.07386809697905718, 0.05442912408983161, 0.013607281022457902, 0.019438972889225574, 0.8157111335331133, 0.940727820835359, 0.4219099485129813, 0.17579581188040888, 0.03515916237608178, 0.316432461384736, 0.06403924354122992, 0.8965494095772188, 0.8952770897283296, 0.952348042919336, 0.897665734738849, 0.09397133845185025, 0.8927277152925774, 0.843304813563596, 0.8685888249999197, 0.4771036807403565, 0.057252441688842776, 0.057252441688842776, 0.009542073614807129, 0.3816829445922852, 0.009542073614807129, 0.8838918339535126, 0.043116674827000614, 0.043116674827000614, 0.021558337413500307, 0.15015795428101997, 0.45468389894439687, 0.15296464501524462, 0.08139403129251549, 0.06736057762139214, 0.0701672683556168, 0.018243489772460368, 0.0042100361013370086, 0.001403345367112336, 0.44405439869890234, 0.247505730422339, 0.09706107075385843, 0.03639790153269691, 0.05702337906789183, 0.02426526768846461, 0.04610400860808276, 0.04125095507038984, 0.006066316922116152, 0.39833661515173024, 0.20302317804507541, 0.11436115725323868, 0.09637176173025731, 0.051398272922803905, 0.052683229745874, 0.04882835927666371, 0.029554006930612244, 0.0025699136461401952, 0.0012849568230700976, 0.08571755475011363, 0.08571755475011363, 0.12857633212517042, 0.10714694343764203, 0.17143510950022725, 0.4285877737505681, 0.9080624826378207, 0.5971004179689637, 0.04353857214357027, 0.13683551245122086, 0.08396724627688552, 0.04664847015382529, 0.04353857214357027, 0.037318776123060234, 0.0031098980102550193, 0.006219796020510039, 0.9470906805779622, 0.03595902842434376, 0.14383611369737503, 0.7910986253355627, 0.8415119590896144, 0.19573358673152777, 0.44304142012564857, 0.13421731661590477, 0.07767205822679674, 0.08947821107726985, 0.018641293974431216, 0.03231157622234744, 0.005592388192329365, 0.0024855058632574957, 0.0847326775361777, 0.847326775361777, 0.34722084132157527, 0.10964868673312904, 0.23757215458844624, 0.06396173392765861, 0.06578921203987742, 0.04751443091768925, 0.04568695280547043, 0.06761669015209625, 0.014619824897750539, 0.19678118323440844, 0.4903399975677063, 0.12473561341634634, 0.06451842073259294, 0.04408758750060517, 0.054840657622703994, 0.01397899115872847, 0.0075271490854691755, 0.0021506140244197643, 0.0010753070122098821, 0.9048835898524947, 0.9016508985748178, 0.9066223006543215, 0.2272529946593822, 0.19828937769299035, 0.15150199643958814, 0.006683911607628888, 0.020051734822886667, 0.04455941071752592, 0.037875499109897036, 0.008911882143505185, 0.08243490982742295, 0.22056908305175332, 0.9457097656810779, 0.9505464386812732, 0.9447696197994563, 0.8642864255003404, 0.23372145797184987, 0.7595947384085121, 0.9450522017365568, 0.8741941644580326, 0.9181093995665938, 0.8854930105275342, 0.5032169577187183, 0.0943531795722597, 0.04324520730395236, 0.10221594453661466, 0.14152976935838954, 0.03931382482177487, 0.04717658978612985, 0.011794147446532462, 0.015725529928709948, 0.19644772604582492, 0.5684911848700408, 0.17779067943812088, 0.010974733298649437, 0.009877259968784493, 0.006584839979189662, 0.015364626618109212, 0.003292419989594831, 0.009877259968784493, 0.9253817661911125, 0.8790436473352164, 0.027194550252044054, 0.10877820100817621, 0.027194550252044054, 0.027194550252044054, 0.10877820100817621, 0.21755640201635243, 0.489501904536793, 0.9722686694777147, 0.2286431787312079, 0.6859295361936237, 0.04572863574624158, 0.8664917495008305, 0.027077867171900954, 0.027077867171900954, 0.027077867171900954, 0.05415573434380191, 0.9242219776854337, 0.8349753441052625, 0.12845774524696346, 0.8710573023891376, 0.8672370212927413, 0.9438661090233251, 0.02413226177643142, 0.02413226177643142, 0.6757033297400797, 0.16892583243501993, 0.09652904710572568, 0.1733117914929647, 0.4184845697025245, 0.1254044670152346, 0.04227116865682066, 0.08454233731364132, 0.08876945417932339, 0.038044051791138594, 0.019726545373182976, 0.0028180779104547108, 0.007045194776136777, 0.1720118111919386, 0.09612424743078922, 0.2074260076138083, 0.07588756376114938, 0.16695264027452864, 0.005059170917409959, 0.10624258926560913, 0.12647927293524897, 0.045532538256689625, 0.9572140216476708, 0.8164048519964784, 0.8728271626321661, 0.06657586541405545, 0.865486250382721, 0.8874646611518306, 0.9715739926273245, 0.3004266811653706, 0.2408483941832085, 0.20535494661936723, 0.0595782869821621, 0.06591640261856233, 0.03802869381840134, 0.04690205570936165, 0.03802869381840134, 0.0025352462545600893, 0.0025352462545600893, 0.9278201197325338, 0.92792945650385, 0.938236671507903, 0.25235596643851466, 0.20548985838564765, 0.20488901084650835, 0.15261527494138744, 0.040256785122334486, 0.05708051621823546, 0.031244072035244673, 0.033046614652662634, 0.00961356062622913, 0.012617798321925734, 0.1075853587288817, 0.8606828698310536, 0.02087461559872615, 0.9602323175414029, 0.057345906145251146, 0.8110349583399805, 0.028672953072625573, 0.004096136153232225, 0.06963431460494782, 0.0163845446129289, 0.012288408459696674, 0.10032624377968821, 0.8527730721273498, 0.025081560944922053, 0.8423945189523511, 0.09359939099470568, 0.07703111336764461, 0.07703111336764461, 0.02567703778921487, 0.02567703778921487, 0.07703111336764461, 0.6676029825195866, 0.8990744297955118, 0.8994452451032227, 0.052908543829601334, 0.19677028375868155, 0.3200001584358357, 0.10136650981507837, 0.06161493733857706, 0.12124229605332905, 0.06757767321005226, 0.03776399385267626, 0.08546588082447786, 0.001987578623825066, 0.005962735871475199, 0.9209479271495323, 0.03683791708598129, 0.9364331842072912, 0.4219279788406912, 0.5063135746088295, 0.13090979321112828, 0.8290953570038124, 0.24669120859101848, 0.010278800357959104, 0.18501840644326387, 0.08223040286367284, 0.11306680393755014, 0.32892161145469134, 0.010278800357959104, 0.03083640107387731, 0.8839694370437566, 0.1900505387163894, 0.665176885507363, 0.043553248455839244, 0.03563447600932301, 0.055431407125613584, 0.007918772446516226, 0.9140605996392013, 0.8602172108451838, 0.03185989669796977, 0.06371979339593954, 0.907245189906355, 0.03420945034216162, 0.906550434067283, 0.05131417551324243, 0.14530092196826347, 0.5456513346255001, 0.09738253280851701, 0.04482688018169831, 0.09429102383046885, 0.009274526934144477, 0.021640562846337114, 0.04018961671462607, 0.003091508978048159, 0.2690589513798032, 0.28084255508986755, 0.19442946121606217, 0.019639339516773956, 0.06284588645367666, 0.021603273468451353, 0.0451704808885801, 0.08641309387380541, 0.015711471613419164, 0.003927867903354791, 0.14425570846097843, 0.5235240280570608, 0.16049641736055878, 0.04872212669874106, 0.03439208943440546, 0.031526081981538334, 0.044900783428251566, 0.009553358176223738, 0.0009553358176223738, 0.34039704359023115, 0.16313959692803703, 0.15372769710526568, 0.13490389745972295, 0.059608698877552, 0.050196799054780625, 0.04078489923200926, 0.05176544902524252, 0.003137299940923789, 0.003137299940923789, 0.9180123954931455, 0.25873508034679477, 0.34862426319028217, 0.13118961820400862, 0.05526970026187401, 0.061343293697244776, 0.038870997986372925, 0.047374028795892004, 0.04069307601698416, 0.01396926490135277, 0.004858874748296616, 0.06438919015049727, 0.8370594719564645, 0.9544297663395994, 0.9472113733934266, 0.28119260519163675, 0.02008518608511691, 0.6427259547237412, 0.04017037217023382, 0.148054470694439, 0.04318255395254471, 0.0740272353472195, 0.18506808836804875, 0.32078468650461783, 0.024675745115739833, 0.04318255395254471, 0.06168936278934958, 0.08636510790508942, 0.012337872557869917, 0.7922812494774976, 0.808669435843486, 0.8914896146453261, 0.8479059790414245, 0.22513982221125825, 0.14811830408635412, 0.16589250057671662, 0.053322589471087485, 0.14811830408635412, 0.023698928653816658, 0.19551616139398742, 0.029623660817270824, 0.011849464326908329, 0.9313421420174461, 0.9493810746805617, 0.9390947809822532, 0.23070763388865614, 0.13708714477441886, 0.31095376741514524, 0.073558955732615, 0.03009230007243341, 0.14043073367135592, 0.04012306676324454, 0.026748711175496365, 0.006687177793874091, 0.17989060825087233, 0.5712742289047973, 0.08265244162877917, 0.043757174979941915, 0.017016679158866302, 0.012154770827761643, 0.043757174979941915, 0.0048619083311046574, 0.021878587489970958, 0.024309541655523285, 0.26572764148748457, 0.18340417608547954, 0.18236210690317567, 0.05522966666210463, 0.10003864150117066, 0.10003864150117066, 0.033346213833723555, 0.06044001257362394, 0.012504830187646332, 0.007294484276127027, 0.7994374139905864, 0.8683998695791293, 0.907902768074589, 0.1317163012156053, 0.7902978072936317, 0.06585815060780265, 0.4667651344678472, 0.4667651344678472, 0.13215885054918203, 0.12821381023428108, 0.404366632277348, 0.05917560472351434, 0.15385657228113728, 0.03747788299155908, 0.045367963621360995, 0.031560322519207644, 0.007890080629801911, 0.28770796262337756, 0.01307763466469898, 0.01307763466469898, 0.02615526932939796, 0.614648829240852, 0.05231053865879592, 0.9215502375619279, 0.8698262830257265, 0.06690971407890205, 0.9120806533281974, 0.11752955664327487, 0.8227068965029241, 0.5022490360261702, 0.05459228652458372, 0.12738200189069535, 0.2365665749398628, 0.018197428841527908, 0.010918457304916744, 0.04731331498797256, 0.0036394857683055815, 0.915567780165971, 0.9211943362494623, 0.9562508912167601, 0.24148673255911832, 0.16799077047590838, 0.010499423154744274, 0.12599307785693128, 0.03149826946423282, 0.03149826946423282, 0.304483271487584, 0.08399538523795419, 0.8161377809798079, 0.19379448857970846, 0.04110792181993816, 0.023490241039964662, 0.19379448857970846, 0.07634328337988515, 0.22902985013965546, 0.0058725602599911655, 0.22902985013965546, 0.0058725602599911655, 0.9174346534339349, 0.8963566461733229, 0.03447525562205088, 0.06895051124410176, 0.2032429684330164, 0.13549531228867762, 0.06774765614433881, 0.5419812491547105, 0.8665809201711822, 0.14474783979343636, 0.7237391989671818, 0.03202637110485778, 0.9607911331457334, 0.14951744813361614, 0.5616488756814042, 0.11884720236261795, 0.0555898204599342, 0.049839149377872045, 0.017252013246186478, 0.02683646504962341, 0.011501342164124318, 0.005750671082062159, 0.07771787410743729, 0.04499455869377948, 0.26178652330926244, 0.3190523252831636, 0.1758878203484107, 0.04908497312048671, 0.012271243280121677, 0.02045207213353613, 0.03681372984036503, 0.012975644658241688, 0.8520673325578709, 0.05622779351904732, 0.021626074430402816, 0.03892693397472507, 0.012975644658241688, 0.9142794862396153, 0.11622216578850125, 0.6423350055632346, 0.12244835324145668, 0.039432520535384354, 0.03424403099125484, 0.019716260267692177, 0.003113093726477712, 0.01556546863238856, 0.007263885361781328, 0.106915213741735, 0.6317717175647978, 0.1522731832079256, 0.03239854961870758, 0.019439129771224545, 0.009719564885612273, 0.035638404580578334, 0.009719564885612273, 0.0032398549618707574, 0.0032398549618707574, 0.15796930294842462, 0.5452978822931196, 0.13670420447459822, 0.02582190528964633, 0.06075742421093255, 0.02582190528964633, 0.031897647710739586, 0.006075742421093254, 0.009113613631639882, 0.03578116724176211, 0.1908328919560646, 0.2266140591978267, 0.4532281183956534, 0.04770822298901615, 0.023854111494508074, 0.011927055747254037, 0.2994277226126197, 0.26931767229403225, 0.177314740765015, 0.05687453949066519, 0.0769479063697235, 0.04349229490462632, 0.02341892802556802, 0.03512839203835203, 0.015891415445921157, 0.002509170859882288, 0.06196659712811859, 0.8675323597936603, 0.02065553237603953, 0.02065553237603953, 0.12254691059477227, 0.28594279138780193, 0.3063672764869307, 0.3063672764869307, 0.8450749544351215, 0.31442518040278633, 0.23246601015561452, 0.16913392405552724, 0.08344933697893854, 0.031293501367101954, 0.07376325322245461, 0.06780258629538757, 0.021607417610618015, 0.004470500195300279, 0.0029803334635335197, 0.022994822762507246, 0.8738032649752754, 0.04598964552501449, 0.04598964552501449, 0.9398302969298025, 0.7415891654660601, 0.015134472764613473, 0.030268945529226945, 0.1059413093522943, 0.04540341829384042, 0.015134472764613473, 0.06053789105845389, 0.23467666218718322, 0.0195563885155986, 0.0097781942577993, 0.5378006841789615, 0.0195563885155986, 0.1271165253513909, 0.029334582773397903, 0.0097781942577993, 0.16316576962068863, 0.07416625891849483, 0.3955533808986391, 0.19283227318808657, 0.07911067617972782, 0.014833251783698967, 0.034610920828630924, 0.03955533808986391, 0.8917231099469998, 0.06369450785335713, 0.16382934164460908, 0.778189372811893, 0.9427215258191879, 0.2462213455675518, 0.03327315480642592, 0.18300235143534255, 0.08651020249670739, 0.13641993470634625, 0.04658241672899628, 0.05656436317092406, 0.14307456566763144, 0.05323704769028147, 0.01996389288385555, 0.17083796676972124, 0.7687708504637456, 0.04270949169243031, 0.3035450174404348, 0.24283601395234783, 0.12141800697617391, 0.3035450174404348, 0.2928587251007624, 0.06997509360814677, 0.13995018721629354, 0.0751584338754169, 0.044058392271796115, 0.02332503120271559, 0.03628338187089092, 0.21251695095807538, 0.09848346507813249, 0.007775010400905197, 0.0441420034456009, 0.0882840068912018, 0.7504140585752154, 0.1324260103368027, 0.30151315445402727, 0.0411154301528219, 0.11649371876632872, 0.09593600368991777, 0.054820573537095865, 0.30151315445402727, 0.02055771507641095, 0.047968001844958884, 0.02055771507641095, 0.2970339132724771, 0.12871469575140673, 0.15247740881320487, 0.06732768700842813, 0.16435876534410396, 0.10297175660112538, 0.06138700874297859, 0.0039604521769663606, 0.007920904353932721, 0.011881356530899083, 0.13092057128173185, 0.6733057951631923, 0.046757346886332804, 0.023378673443166402, 0.09351469377266561, 0.023378673443166402, 0.004675734688633281, 0.038869455635996475, 0.760383725879181, 0.11903770788523921, 0.009717363908999119, 0.03644011465874669, 0.009717363908999119, 0.017005386840748458, 0.004858681954499559, 0.2580197077776389, 0.40848811451770484, 0.08635128496766672, 0.03774637007568665, 0.10134641828540525, 0.06153175395899605, 0.029473193072796427, 0.010341471253612781, 0.004136588501445112, 0.0031024413760838343, 0.2526131638550407, 0.14667861127066878, 0.2314262533381663, 0.16297623474518755, 0.057041682160815635, 0.03422500929648938, 0.047263108076104385, 0.06519049389807502, 0.0016297623474518753, 0.0032595246949037506, 0.29109042954889497, 0.028105282852996752, 0.2489325052693998, 0.09234592937413219, 0.12245873243091443, 0.056210565705993504, 0.00803008081514193, 0.06223312631734995, 0.06022560611356447, 0.03212032326056772, 0.9139260970560102, 0.07030200746584694, 0.9693009838392972], \"Term\": [\"1965\", \"aamir\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"addams\", \"affleck\", \"africa\", \"africa\", \"africa\", \"african-american\", \"agatha\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agents\", \"agents\", \"agents\", \"agents\", \"aircraft\", \"aircraft\", \"aladdin\", \"alba\", \"alex\", \"alex\", \"alex\", \"alex\", \"alexander\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alter\", \"altman\", \"altman\", \"amelie\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"amusingly\", \"andrews\", \"andrews\", \"andrews\", \"andrews\", \"andrews\", \"angus\", \"angus\", \"anna\", \"anna\", \"anna\", \"anna\", \"another\", \"another\", \"another\", \"another\", \"another\", \"another\", \"another\", \"another\", \"another\", \"another\", \"anti\", \"ants\", \"ants\", \"apollo\", \"arm\", \"arm\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"arthur\", \"arthur\", \"arthur\", \"arthurian\", \"asian\", \"asian\", \"astor\", \"authenticity\", \"awful\", \"awful\", \"awful\", \"awful\", \"awful\", \"awful\", \"awful\", \"awful\", \"awful\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"balls\", \"balls\", \"bambi\", \"band\", \"band\", \"band\", \"band\", \"band\", \"bands\", \"banter\", \"barbra\", \"barrie\", \"barrymore\", \"barrymore\", \"basement\", \"bath\", \"bath\", \"bath\", \"batman\", \"batman\", \"battlestar\", \"bava\", \"bear\", \"bear\", \"bear\", \"bear\", \"beneath\", \"bennett\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bette\", \"bette\", \"bette\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"beverly\", \"beverly\", \"bewitched\", \"bible\", \"big-budget\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black-and-white\", \"blacks\", \"blacks\", \"blues\", \"blues\", \"boogeyman\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"branagh\", \"branagh\", \"brett\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"broadway\", \"bronson\", \"bronson\", \"bronson\", \"brown\", \"brown\", \"brown\", \"bumping\", \"bunuel\", \"caine\", \"caine\", \"caine\", \"carol\", \"carol\", \"carol\", \"carol\", \"carole\", \"carole\", \"carole\", \"carole\", \"carole\", \"carter\", \"carter\", \"carter\", \"carter\", \"carvey\", \"cary\", \"cary\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"chad\", \"chaney\", \"chaney\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"che\", \"cheesy\", \"cheesy\", \"cheesy\", \"cheesy\", \"cheesy\", \"cheesy\", \"cheesy\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"children\", \"children\", \"children\", \"children\", \"children\", \"children\", \"children\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"christie\", \"christie\", \"christie\", \"christine\", \"church\", \"church\", \"church\", \"church\", \"church\", \"claire\", \"claire\", \"classic\", \"classic\", \"classic\", \"classic\", \"classic\", \"classic\", \"classic\", \"classic\", \"classic\", \"claudius\", \"clinton\", \"clive\", \"clockwork\", \"clockwork\", \"clothes\", \"clothes\", \"clothes\", \"clothes\", \"clothes\", \"coach\", \"coach\", \"coast\", \"coast\", \"collette\", \"collette\", \"combo\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"consciousness\", \"contention\", \"cop\", \"cop\", \"cop\", \"cop\", \"cop\", \"cop\", \"cop\", \"coppola\", \"cops\", \"cops\", \"cops\", \"cops\", \"costello\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"crap\", \"crap\", \"crap\", \"crap\", \"crap\", \"crappy\", \"crappy\", \"crappy\", \"crappy\", \"crawford\", \"crawford\", \"crawford\", \"creature\", \"creature\", \"creature\", \"creature\", \"criminals\", \"criminals\", \"criminals\", \"crow\", \"cuba\", \"cult\", \"cult\", \"cult\", \"cult\", \"cult\", \"cusack\", \"damme\", \"dan\", \"dan\", \"dan\", \"dan\", \"dan\", \"dan\", \"dan\", \"dana\", \"dana\", \"danish\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"dean\", \"dean\", \"dean\", \"dean\", \"dean\", \"decline\", \"decline\", \"del\", \"del\", \"del\", \"demon\", \"demon\", \"demon\", \"der\", \"derek\", \"derek\", \"derek\", \"desmond\", \"detective\", \"detective\", \"detective\", \"detective\", \"detective\", \"detective\", \"devotion\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"discuss\", \"discuss\", \"discuss\", \"distorted\", \"dixon\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"dolls\", \"dolls\", \"dracula\", \"dream\", \"dream\", \"dream\", \"dream\", \"dream\", \"dream\", \"dual\", \"duck\", \"dumped\", \"dusty\", \"dutch\", \"dutch\", \"duvall\", \"dwarf\", \"dwarf\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"edgar\", \"edith\", \"editors\", \"egotistical\", \"einstein\", \"el\", \"el\", \"elite\", \"elite\", \"elite\", \"elmer\", \"elvis\", \"elvis\", \"em\", \"em\", \"employee\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"endings\", \"esther\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"evocative\", \"examination\", \"excalibur\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"execute\", \"experiment\", \"experiment\", \"experiment\", \"expresses\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fairbanks\", \"fake\", \"fake\", \"fake\", \"fake\", \"fake\", \"fake\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"farnsworth\", \"father\", \"father\", \"father\", \"father\", \"father\", \"father\", \"father\", \"father\", \"father\", \"father\", \"felix\", \"felix\", \"fi\", \"figured\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fiona\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fitness\", \"fletcher\", \"fletcher\", \"flynn\", \"flynn\", \"foley\", \"footage\", \"footage\", \"footage\", \"footage\", \"footage\", \"footage\", \"footage\", \"forcing\", \"formation\", \"france\", \"france\", \"france\", \"france\", \"france\", \"freddy\", \"freddy\", \"freddy\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"fritz\", \"fritz\", \"frustrating\", \"funky\", \"funny\", \"funny\", \"funny\", \"funny\", \"funny\", \"funny\", \"funny\", \"funny\", \"funny\", \"futurama\", \"futurama\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"garbage\", \"garbage\", \"garbage\", \"garbage\", \"garner\", \"gay\", \"gay\", \"gay\", \"gay\", \"gay\", \"gay\", \"gay\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"germans\", \"germans\", \"germans\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"gleason\", \"gleason\", \"glenn\", \"glover\", \"glover\", \"glued\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goings\", \"golf\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gordon\", \"gordon\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"graham\", \"graham\", \"graham\", \"grandfather\", \"grandfather\", \"grandfather\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"griffin\", \"gripping\", \"gripping\", \"gripping\", \"gripping\", \"guevara\", \"guilty\", \"guilty\", \"guilty\", \"guilty\", \"guilty\", \"guilty\", \"haines\", \"halfway\", \"halfway\", \"handicapped\", \"hans\", \"harold\", \"harold\", \"harold\", \"harold\", \"harold\", \"harriet\", \"harry\", \"harry\", \"harry\", \"harry\", \"hayek\", \"heavy\", \"heavy\", \"heavy\", \"heavy\", \"heavy\", \"heavy\", \"heavy-handed\", \"hey\", \"hey\", \"hey\", \"hey\", \"hidalgo\", \"highlighted\", \"hinted\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hitler\", \"hitler\", \"hitler\", \"hitler\", \"hmm\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"hooper\", \"horrible\", \"horrible\", \"horrible\", \"horrible\", \"horrible\", \"horrible\", \"horrible\", \"horror\", \"horror\", \"horror\", \"horror\", \"horror\", \"horror\", \"horror\", \"horror\", \"horror\", \"husband\", \"husband\", \"husband\", \"husband\", \"husband\", \"husband\", \"husband\", \"husband\", \"husband\", \"icing\", \"idiot\", \"idiot\", \"im\", \"im\", \"immigrant\", \"immigrant\", \"indian\", \"indian\", \"indian\", \"indian\", \"indian\", \"indian\", \"inject\", \"insanely\", \"inspirational\", \"inspire\", \"institute\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interests\", \"intrigue\", \"investigation\", \"investigation\", \"investigation\", \"iq\", \"islam\", \"islamic\", \"island\", \"island\", \"island\", \"island\", \"island\", \"island\", \"island\", \"island\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issues\", \"issues\", \"issues\", \"issues\", \"issues\", \"italian\", \"italian\", \"italian\", \"italian\", \"ivy\", \"jackie\", \"jackie\", \"jackie\", \"jackie\", \"jackie\", \"jagger\", \"jagger\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jerry\", \"jerry\", \"jerry\", \"jerry\", \"jerry\", \"jesus\", \"jesus\", \"jesus\", \"jesus\", \"joan\", \"joan\", \"joan\", \"joan\", \"joan\", \"joan\", \"joe\", \"joe\", \"joe\", \"joe\", \"joe\", \"joe\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"juliet\", \"kapoor\", \"karl\", \"karl\", \"karloff\", \"karloff\", \"katharine\", \"keaton\", \"keaton\", \"keaton\", \"kellogg\", \"kidnapping\", \"kidnapping\", \"kids\", \"kids\", \"kids\", \"kids\", \"kids\", \"kids\", \"kids\", \"kids\", \"kids\", \"king\", \"king\", \"king\", \"king\", \"king\", \"king\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kramer\", \"kramer\", \"kubrick\", \"kubrick\", \"kudrow\", \"la\", \"la\", \"la\", \"la\", \"la\", \"la\", \"la\", \"la\", \"la\", \"ladd\", \"lagaan\", \"lame\", \"lame\", \"lame\", \"lame\", \"lame\", \"lame\", \"lancaster\", \"lancaster\", \"lancaster\", \"laser\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"lata\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"leone\", \"leone\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life-long\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"ling\", \"lister\", \"lists\", \"literal\", \"literal\", \"literature\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lively\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"liza\", \"lol\", \"lon\", \"lon\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"loretta\", \"louis\", \"louis\", \"louis\", \"louis\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lowe\", \"luana\", \"luke\", \"luke\", \"lundgren\", \"lynch\", \"lynch\", \"lynch\", \"lyrical\", \"macmurray\", \"macmurray\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"madman\", \"mae\", \"magazines\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"malkovich\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"mantee\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"marc\", \"marie\", \"marie\", \"marie\", \"marie\", \"marisa\", \"marriage\", \"marriage\", \"marriage\", \"marriage\", \"marriage\", \"marriage\", \"marriage\", \"marvellous\", \"massey\", \"massey\", \"match\", \"match\", \"match\", \"match\", \"match\", \"match\", \"match\", \"matthau\", \"matthau\", \"matthau\", \"memento\", \"meryl\", \"meryl\", \"metal\", \"metal\", \"metal\", \"mexican\", \"mexican\", \"mexican\", \"mick\", \"midler\", \"military\", \"military\", \"military\", \"military\", \"military\", \"military\", \"mins\", \"missile\", \"mitchum\", \"mitchum\", \"mitchum\", \"mitchum\", \"mocking\", \"mole\", \"monday\", \"monk\", \"monk\", \"monkey\", \"monkey\", \"monkeys\", \"montgomery\", \"montgomery\", \"montgomery\", \"mortensen\", \"morton\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"movie.the\", \"movie.the\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"mrs\", \"mrs\", \"mrs\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"mustache\", \"na\", \"na\", \"na\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"nasa\", \"nathan\", \"nathan\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"niece\", \"ninja\", \"ninja\", \"noir\", \"noir\", \"noir\", \"noir\", \"nope\", \"norma\", \"norma\", \"norman\", \"norman\", \"norman\", \"norwegian\", \"notices\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"oliver\", \"oliver\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"orange\", \"orange\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"over-sized\", \"owen\", \"owen\", \"owen\", \"page\", \"page\", \"palace\", \"palace\", \"palace\", \"palpable\", \"pare\", \"parodies\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"patton\", \"patton\", \"paul\", \"paul\", \"paul\", \"paul\", \"paul\", \"paul\", \"paul\", \"paul\", \"pauline\", \"pauline\", \"peace\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perfection\", \"perfection\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"phillips\", \"phoenix\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"pitched\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"pod\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"poirot\", \"poland\", \"police\", \"police\", \"police\", \"police\", \"police\", \"police\", \"police\", \"police\", \"politicians\", \"pond\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"potter\", \"pounds\", \"pov\", \"powell\", \"powell\", \"powerhouse\", \"preachy\", \"predator\", \"predator\", \"preminger\", \"press\", \"press\", \"priest\", \"priest\", \"priest\", \"priest\", \"priest\", \"princess\", \"princess\", \"princess\", \"princess\", \"programme\", \"programme\", \"programs\", \"programs\", \"proposes\", \"pryor\", \"pulp\", \"pulp\", \"purports\", \"pursue\", \"quarters\", \"rapper\", \"rat\", \"reagan\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"relentlessly\", \"reno\", \"rent\", \"rent\", \"rent\", \"rent\", \"rent\", \"renting\", \"renting\", \"replay\", \"representing\", \"requisite\", \"resident\", \"retro\", \"rex\", \"ricci\", \"rice\", \"rice\", \"rickman\", \"ringu\", \"rip-off\", \"rko\", \"robertson\", \"robertson\", \"robinson\", \"robinson\", \"robinson\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"romeo\", \"roof\", \"root\", \"root\", \"root\", \"root\", \"rosie\", \"rosie\", \"rubin\", \"ruby\", \"rutger\", \"sadako\", \"sadako\", \"saif\", \"saintly\", \"sam\", \"sam\", \"sam\", \"sam\", \"sam\", \"sam\", \"sat\", \"sat\", \"sat\", \"sat\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scooby\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screens\", \"seagal\", \"seagal\", \"seagal\", \"seals\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seed\", \"seed\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"selection\", \"sen\", \"serbs\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"servant\", \"shaggy\", \"shanghai\", \"shannon\", \"shark\", \"shark\", \"shearer\", \"sheds\", \"sherlock\", \"shiny\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"silk\", \"simmons\", \"simon\", \"simon\", \"simon\", \"simon\", \"simon\", \"simon\", \"simon\", \"simpsons\", \"sin\", \"sin\", \"sin\", \"sing\", \"sing\", \"sing\", \"sing\", \"sing\", \"ski\", \"skull\", \"skull\", \"slashers\", \"sleek\", \"slumber\", \"soldier\", \"soldier\", \"soldier\", \"soldier\", \"soldier\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"southerners\", \"sparse\", \"specials\", \"species\", \"species\", \"splatter\", \"stan\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stiller\", \"stinker\", \"stooges\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"streep\", \"streep\", \"streisand\", \"streisand\", \"stupid\", \"stupid\", \"stupid\", \"stupid\", \"stupid\", \"stupid\", \"stupid\", \"sucks\", \"sucks\", \"sucks\", \"sunshine\", \"sunshine\", \"susan\", \"susan\", \"susan\", \"susan\", \"susan\", \"susan\", \"sylvester\", \"ta\", \"ta\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tall\", \"tall\", \"tame\", \"tank\", \"tank\", \"tarantino\", \"tarantino\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"tenant\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"terrorism\", \"texas\", \"texas\", \"texas\", \"thailand\", \"thats\", \"thats\", \"thats\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"throne\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timothy\", \"timothy\", \"tolerable\", \"toro\", \"touching\", \"touching\", \"touching\", \"touching\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"towns\", \"transcends\", \"transparent\", \"trevor\", \"tries\", \"tries\", \"tries\", \"tries\", \"tries\", \"tries\", \"tries\", \"tries\", \"tries\", \"trigger\", \"trough\", \"troupe\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"uncanny\", \"unger\", \"universally\", \"university\", \"university\", \"university\", \"unpleasant\", \"unpleasant\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"vaudeville\", \"vega\", \"vega\", \"vera\", \"verhoeven\", \"verhoeven\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"vicinity\", \"vicki\", \"viggo\", \"villain\", \"villain\", \"villain\", \"villain\", \"villain\", \"villain\", \"villain\", \"villain\", \"villainous\", \"violence\", \"violence\", \"violence\", \"violence\", \"violence\", \"violence\", \"violence\", \"violence\", \"violence\", \"violin\", \"walken\", \"walken\", \"walken\", \"wallace\", \"wallace\", \"wallace\", \"wallace\", \"wallet\", \"walters\", \"walters\", \"wan\", \"wan\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"waste\", \"waste\", \"waste\", \"waste\", \"waste\", \"waste\", \"wastes\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watched\", \"watched\", \"watched\", \"watched\", \"watched\", \"watched\", \"watched\", \"watched\", \"watched\", \"watched\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"water\", \"water\", \"water\", \"water\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wayne\", \"wayne\", \"wayne\", \"wayne\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"weed\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"werewolf\", \"werewolf\", \"werewolf\", \"werewolf\", \"wes\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"western\", \"western\", \"western\", \"western\", \"western\", \"western\", \"western\", \"western\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"whites\", \"whites\", \"wicked\", \"wicked\", \"widescreen\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wilder\", \"wilder\", \"wilder\", \"wind\", \"wind\", \"wind\", \"wind\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woody\", \"woody\", \"woody\", \"woody\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"zhang\", \"zhang\", \"zorro\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 3, 9, 4, 1, 10, 5, 7, 2, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1759140533210849104471899557\", ldavis_el1759140533210849104471899557_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1759140533210849104471899557\", ldavis_el1759140533210849104471899557_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1759140533210849104471899557\", ldavis_el1759140533210849104471899557_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7      0.041062  0.005836       1        1  28.192153\n",
       "2      0.152349  0.019474       2        1  21.320579\n",
       "8      0.046754  0.014814       3        1  14.492809\n",
       "3      0.036337  0.005151       4        1   9.408517\n",
       "0      0.032302 -0.001135       5        1   8.563814\n",
       "9      0.028884 -0.003229       6        1   5.763453\n",
       "4      0.046982  0.027691       7        1   5.191893\n",
       "6     -0.004028 -0.072092       8        1   4.215061\n",
       "1     -0.175888 -0.167431       9        1   1.810040\n",
       "5     -0.204754  0.170921      10        1   1.041681, topic_info=        Term         Freq        Total Category  logprob  loglift\n",
       "24      film  5612.000000  5612.000000  Default  30.0000  30.0000\n",
       "921   series   448.000000   448.000000  Default  29.0000  29.0000\n",
       "46       one  3823.000000  3823.000000  Default  28.0000  28.0000\n",
       "1244     man   781.000000   781.000000  Default  27.0000  27.0000\n",
       "197    story  1664.000000  1664.000000  Default  26.0000  26.0000\n",
       "...      ...          ...          ...      ...      ...      ...\n",
       "272   played    12.690059   366.009270  Topic10  -5.7482   1.2025\n",
       "414   doctor     9.690056   121.932774  Topic10  -6.0180   2.0320\n",
       "180     role     9.787185   514.430472  Topic10  -6.0080   0.6023\n",
       "786       tv     9.670699   411.361108  Topic10  -6.0200   0.8140\n",
       "833      son     9.295760   197.660845  Topic10  -6.0595   1.5073\n",
       "\n",
       "[850 rows x 6 columns], token_table=       Topic      Freq    Term\n",
       "term                          \n",
       "10741      9  0.859232    1965\n",
       "7443       6  0.949925   aamir\n",
       "6811       1  0.349587  accent\n",
       "6811       2  0.139835  accent\n",
       "6811       3  0.027967  accent\n",
       "...      ...       ...     ...\n",
       "467        9  0.060226   young\n",
       "467       10  0.032120   young\n",
       "6187       4  0.913926   zhang\n",
       "6187       7  0.070302   zhang\n",
       "3335       7  0.969301   zorro\n",
       "\n",
       "[2379 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 3, 9, 4, 1, 10, 5, 7, 2, 6])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim_models.prepare(lda_tf, corpus_tf, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with LDA vectors (we'll have to use only the labeled sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Corpora for predictions\n",
      "corpus tf done\n",
      "corpus lda tf done\n",
      "Creating lda vectors\n",
      "lda vectors done\n"
     ]
    }
   ],
   "source": [
    "#For predictions we need only the labeled set:\n",
    "print(\"Creating Corpora for predictions\")\n",
    "corpus_tf2 = [dictionary.doc2bow(sentence) for sentence in labeled_sentences]\n",
    "print('corpus tf done')\n",
    "\n",
    "lda_tf2 = models.LdaModel(corpus_tf2, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf2 = lda_tf2[corpus_tf2]\n",
    "print('corpus lda tf done')\n",
    "\n",
    "print('Creating lda vectors')\n",
    "X = gensim.matutils.corpus2csc(corpus_lda_tf2)\n",
    "X = X.T\n",
    "print('lda vectors done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.98859644, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(X,\n",
    "                                                                                        train[\"sentiment\"],\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf_lda = LR(penalty='l2',\n",
    "                   dual=False,\n",
    "                   tol=0.0001,\n",
    "                   C=1.0,\n",
    "                   fit_intercept=True,\n",
    "                   intercept_scaling=1,\n",
    "                   class_weight=None,\n",
    "                   random_state=0,\n",
    "                   solver='liblinear',\n",
    "                   max_iter=100,\n",
    "                   multi_class='ovr',\n",
    "                   verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.528\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_lda = clf_LR_tf_lda.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Word Vectors\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors  \n",
    "\n",
    "Introducing Distributed Word Vectors: This part of the tutorial will focus on using distributed word vectors created by the Word2Vec algorithm, using the Gensim implementation.  \n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "\n",
    "Word2vec, published by Google in 2013, is a neural network implementation that learns distributed representations for words. Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. Word2vec learns quickly relative to other models.\n",
    "\n",
    "Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\"\n",
    "\n",
    "Distributed word vectors are powerful and can be used for many applications, particularly word prediction and translation. Here, we will try to apply them to sentiment analysis.\n",
    "\n",
    "Using word2vec in Python: In Python, we will use the excellent implementation of word2vec from the gensim package. If you don't already have gensim installed, you'll need to install it. There is an excellent tutorial that accompanies the Python Word2Vec implementation, here.\n",
    "\n",
    "Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is compute intensive. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). ln order to train your model in a reasonable amount of time, you will need to install cython (instructions here). Word2Vec will run without cython installed, but it will take days to run instead of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing to Train a Model\n",
    "\n",
    "We are going to use the whole set of sentences for training, the labeled and the unlabeled, which contains 50,000 additional reviews with no labels. When we built the Bag of Words model in Part 1, extra unlabeled training reviews were not useful. However, since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used.  \n",
    "\n",
    "To train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Saving Your Model  \n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "\n",
    "Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n",
    "\n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 5   # Minimum word count\n",
    "num_workers = -1       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-5   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:16:52,016 : INFO : collecting all words and their counts\n",
      "2022-05-18 14:16:52,017 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-18 14:16:52,198 : INFO : collected 43387 word types from a corpus of 816941 raw words and 3750 sentences\n",
      "2022-05-18 14:16:52,199 : INFO : Creating a fresh vocabulary\n",
      "2022-05-18 14:16:52,254 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 10265 unique words (23.66% of original 43387, drops 33122)', 'datetime': '2022-05-18T14:16:52.253956', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "2022-05-18 14:16:52,254 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 766302 word corpus (93.80% of original 816941, drops 50639)', 'datetime': '2022-05-18T14:16:52.254854', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "2022-05-18 14:16:52,311 : INFO : deleting the raw counts dictionary of 43387 items\n",
      "2022-05-18 14:16:52,313 : INFO : sample=1e-05 downsamples 3260 most-common words\n",
      "2022-05-18 14:16:52,314 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 179359.49566535302 word corpus (23.4%% of prior 766302)', 'datetime': '2022-05-18T14:16:52.314945', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "2022-05-18 14:16:52,419 : INFO : estimated required memory for 10265 words and 300 dimensions: 29768500 bytes\n",
      "2022-05-18 14:16:52,420 : INFO : resetting layer weights\n",
      "2022-05-18 14:16:52,438 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-18T14:16:52.438225', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'build_vocab'}\n",
      "2022-05-18 14:16:52,439 : INFO : Word2Vec lifecycle event {'msg': 'training model with -1 workers on 10265 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=5 window=10 shrink_windows=True', 'datetime': '2022-05-18T14:16:52.439136', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'train'}\n",
      "2022-05-18 14:16:52,445 : INFO : EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:52,446 : WARNING : EPOCH 0: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:52,447 : WARNING : EPOCH 0: supplied raw word count (0) did not equal expected count (816941)\n",
      "2022-05-18 14:16:52,453 : INFO : EPOCH 1: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:52,454 : WARNING : EPOCH 1: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:52,455 : WARNING : EPOCH 1: supplied raw word count (0) did not equal expected count (816941)\n",
      "2022-05-18 14:16:52,461 : INFO : EPOCH 2: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:52,462 : WARNING : EPOCH 2: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:52,463 : WARNING : EPOCH 2: supplied raw word count (0) did not equal expected count (816941)\n",
      "2022-05-18 14:16:52,469 : INFO : EPOCH 3: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:52,470 : WARNING : EPOCH 3: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:52,471 : WARNING : EPOCH 3: supplied raw word count (0) did not equal expected count (816941)\n",
      "2022-05-18 14:16:52,476 : INFO : EPOCH 4: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:52,477 : WARNING : EPOCH 4: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:52,477 : WARNING : EPOCH 4: supplied raw word count (0) did not equal expected count (816941)\n",
      "2022-05-18 14:16:52,478 : INFO : Word2Vec lifecycle event {'msg': 'training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s', 'datetime': '2022-05-18T14:16:52.478210', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'train'}\n",
      "2022-05-18 14:16:52,478 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10265, vector_size=300, alpha=0.025>', 'datetime': '2022-05-18T14:16:52.478869', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "model = Word2Vec(all_sentences_sw,\n",
    "                 workers = num_workers,\n",
    "                 vector_size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:16:52,486 : INFO : collecting all words and their counts\n",
      "2022-05-18 14:16:52,488 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-05-18 14:16:53,539 : INFO : collected 391016 token types (unigram + bigrams) from a corpus of 816941 words and 3750 sentences\n",
      "2022-05-18 14:16:53,540 : INFO : merged Phrases<391016 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-18 14:16:53,541 : INFO : Phrases lifecycle event {'msg': 'built Phrases<391016 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 1.05s', 'datetime': '2022-05-18T14:16:53.541327', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'created'}\n",
      "2022-05-18 14:16:53,543 : INFO : collecting all words and their counts\n",
      "2022-05-18 14:16:53,544 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-18 14:16:55,206 : INFO : collected 45235 word types from a corpus of 765464 raw words and 3750 sentences\n",
      "2022-05-18 14:16:55,207 : INFO : Creating a fresh vocabulary\n",
      "2022-05-18 14:16:55,264 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 11858 unique words (26.21% of original 45235, drops 33377)', 'datetime': '2022-05-18T14:16:55.264545', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "2022-05-18 14:16:55,265 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 714214 word corpus (93.30% of original 765464, drops 51250)', 'datetime': '2022-05-18T14:16:55.265359', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "2022-05-18 14:16:55,330 : INFO : deleting the raw counts dictionary of 45235 items\n",
      "2022-05-18 14:16:55,332 : INFO : sample=1e-05 downsamples 3815 most-common words\n",
      "2022-05-18 14:16:55,332 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 190015.32524693754 word corpus (26.6%% of prior 714214)', 'datetime': '2022-05-18T14:16:55.332859', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "2022-05-18 14:16:55,454 : INFO : estimated required memory for 11858 words and 300 dimensions: 34388200 bytes\n",
      "2022-05-18 14:16:55,454 : INFO : resetting layer weights\n",
      "2022-05-18 14:16:55,472 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-18T14:16:55.472673', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'build_vocab'}\n",
      "2022-05-18 14:16:55,473 : INFO : Word2Vec lifecycle event {'msg': 'training model with -1 workers on 11858 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=5 window=10 shrink_windows=True', 'datetime': '2022-05-18T14:16:55.473534', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'train'}\n",
      "2022-05-18 14:16:55,479 : INFO : EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:55,527 : WARNING : EPOCH 0: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:55,584 : WARNING : EPOCH 0: supplied raw word count (0) did not equal expected count (765464)\n",
      "2022-05-18 14:16:55,631 : INFO : EPOCH 1: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:55,637 : WARNING : EPOCH 1: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:55,664 : WARNING : EPOCH 1: supplied raw word count (0) did not equal expected count (765464)\n",
      "2022-05-18 14:16:56,268 : INFO : EPOCH 2: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:56,295 : WARNING : EPOCH 2: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:56,311 : WARNING : EPOCH 2: supplied raw word count (0) did not equal expected count (765464)\n",
      "2022-05-18 14:16:56,583 : INFO : EPOCH 3: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:16:56,945 : WARNING : EPOCH 3: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:16:57,202 : WARNING : EPOCH 3: supplied raw word count (0) did not equal expected count (765464)\n",
      "2022-05-18 14:17:00,342 : INFO : EPOCH 4: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2022-05-18 14:17:00,396 : WARNING : EPOCH 4: supplied example count (0) did not equal expected count (3750)\n",
      "2022-05-18 14:17:00,560 : WARNING : EPOCH 4: supplied raw word count (0) did not equal expected count (765464)\n",
      "2022-05-18 14:17:01,241 : INFO : Word2Vec lifecycle event {'msg': 'training on 0 raw words (0 effective words) took 5.8s, 0 effective words/s', 'datetime': '2022-05-18T14:17:01.241768', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'train'}\n",
      "2022-05-18 14:17:01,353 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11858, vector_size=300, alpha=0.025>', 'datetime': '2022-05-18T14:17:01.353684', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.276-211.499.amzn2.x86_64-x86_64-with-debian-buster-sid', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "##Optionally converting the model for Bigrams (to capture more context):\n",
    "bigram_transformer = gensim.models.Phrases(all_sentences_sw)\n",
    "model = Word2Vec(bigram_transformer[all_sentences_sw],\n",
    "                 workers = num_workers,\n",
    "                 vector_size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.wv.fill_norms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Model Results\n",
    "\n",
    "Congratulations on making it successfully through everything so far! Let's take a look at the model we created out of our 75,000 training reviews.\n",
    "\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alien\n",
      "daughter\n",
      "france\n",
      "[('on_hbo', 0.22268562018871307), ('absolutely_nothing', 0.2186601310968399), ('gothic', 0.2185399830341339), ('miller', 0.21110859513282776), ('martial_arts', 0.20782388746738434), ('greatness', 0.20282837748527527), ('steadily', 0.19514214992523193), ('hatred', 0.1910945177078247), ('klein', 0.1887647956609726), ('formal', 0.18504518270492554)]\n",
      "[('characters', 0.22363445162773132), ('u.s', 0.2162657231092453), ('revolutionary', 0.21171391010284424), ('inmates', 0.20882846415042877), ('sung_by', 0.20228241384029388), ('hawaii', 0.18297584354877472), ('derive', 0.182753324508667), ('effectively', 0.1823064237833023), ('announced', 0.18072237074375153), ('1981', 0.1799950897693634)]\n",
      "[('endless', 0.20211930572986603), ('jolly', 0.1900801956653595), ('spontaneous', 0.18662548065185547), ('non', 0.17841736972332), ('brats', 0.1777513474225998), ('blatant', 0.17736709117889404), ('barker', 0.17700155079364777), ('gag', 0.17608344554901123), ('beloved', 0.17487458884716034), ('accomplished', 0.17357338964939117)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"captain onion starship alien\".split()))\n",
    "print(model.wv.doesnt_match(\"father mother son daughter film\".split()))\n",
    "print(model.wv.doesnt_match(\"france england germany berlin\".split()))\n",
    "print(model.wv.most_similar(\"man\"))\n",
    "print(model.wv.most_similar(\"queen\"))\n",
    "print(model.wv.most_similar(\"awful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have a reasonably good model for semantic meaning - at least as good as Bag of Words. But how can we use these fancy distributed word vectors for supervised learning? The next section takes a stab at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: More Fun With Word Vectors\n",
    "--\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors  \n",
    "\n",
    "Numeric Representations of Words\n",
    "\n",
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11858, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5171679e-03, -2.3923111e-03,  7.8737736e-05,  2.6374864e-03,\n",
       "       -1.8806243e-03,  1.8973422e-03, -3.8731814e-04,  2.4518650e-04,\n",
       "        6.8122149e-04, -1.7461363e-03], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'of', 'to', 'is', 'it', 'in', 'that', 'this', 'was']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words To Paragraphs,\n",
    "### Attempt 1: Vector Averaging\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype=float)\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 2000th review\n",
    "        if counter%2000. == 0.:\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        #Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call these functions to create average vectors for each paragraph. The following operations will take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 1250\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, using the functions \n",
    "# we defined above. Notice that we now use stop word removal.\n",
    "trainDataVecs = getAvgFeatureVecs(labeled_sentences_sw, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.5228414e-05,  3.0249506e-04,  1.8169235e-04,  4.9329101e-05,\n",
       "       -1.4669850e-04, -4.3263612e-04,  6.5841916e-05,  5.5383920e-04,\n",
       "       -3.2464231e-04, -2.0554307e-04], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[100][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(trainDataVecs).any()) #testando se no h valores que inviabilizam o treinamento\n",
    "print(np.isfinite(trainDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are Nan values we can use inputer  \n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "trainDataVecs = Imputer().fit_transform(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(trainDataVecs).any()) \n",
    "print(np.isfinite(trainDataVecs).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the average paragraph vectors with the classifiers from Part 1.  \n",
    "Note that, as in Part 1, we can only use the labeled training reviews to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traincvWV, X_testcvWV, y_traincvWV, y_testcvWV = model_selection.train_test_split(trainDataVecs,\n",
    "                                                                                    train[\"sentiment\"],\n",
    "                                                                                    test_size=0.2,\n",
    "                                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.628\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_WV = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=3, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_RF_WV_tts = clf_RF_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_RF_WV_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_WV = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=None,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_LR_WV_tts = clf_LR_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_LR_WV_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 1250\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(test_labeled_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(testDataVecs).any()) \n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = Imputer().fit_transform(testDataVecs)\n",
    "\n",
    "print(np.isnan(testDataVecs).any()) \n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.69379069e-04, -4.18808086e-05,  1.33924390e-04, -1.10707486e-04,\n",
       "       -7.29216990e-05,  1.47403131e-04,  1.07962114e-04, -3.27756366e-04,\n",
       "        1.01574456e-04,  7.08282969e-05], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataVecs[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 0 1 1 1]\n",
      "[[0.57833333 0.42166667]\n",
      " [0.43166667 0.56833333]\n",
      " [0.44833333 0.55166667]\n",
      " [0.48166667 0.51833333]\n",
      " [0.50166667 0.49833333]\n",
      " [0.48833333 0.51166667]\n",
      " [0.53       0.47      ]\n",
      " [0.475      0.525     ]\n",
      " [0.49333333 0.50666667]\n",
      " [0.49666667 0.50333333]]\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = clf_RF_WV.predict(testDataVecs)\n",
    "result_prob = clf_RF_WV.predict_proba(testDataVecs)\n",
    "print(result[0:10])\n",
    "print(result_prob[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv('./data/Word2Vec_AverageVectors.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5203_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"11917_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"2973_8\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"5671_3\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"11566_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0    \"5203_1\"          0\n",
       "1  \"11917_10\"          1\n",
       "2    \"2973_8\"          1\n",
       "3    \"5671_3\"          1\n",
       "4   \"11566_1\"          0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this produced results much better than chance, but underperformed Bag of Words by a few percentage points.\n",
    "\n",
    "Since the element-wise average of the vectors didn't produce spectacular results, perhaps we could do it in a more intelligent way? A standard way of weighting word vectors is to apply \"tf-idf\" weights, which measure how important a given word is within a given set of documents. One way to extract tf-idf weights in Python is by using scikit-learn's TfidfVectorizer, which has an interface similar to the CountVectorizer that we used in Part 1. However, when we tried weighting our word vectors in this way, we found no substantial improvement in performance.\n",
    "\n",
    "## From Words to Paragraphs, Attempt 2: Clustering\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "In K-Means, the one parameter we need to set is \"K,\" or the number of clusters. How should we decide how many clusters to create? Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Clustering code is given below. We use scikit-learn to perform our K-Means.\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code took more than 40 minutes on my computer. Below, we set a timer around the K-Means function to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11858, 300)\n",
      "Initialization complete\n",
      "Iteration 0, inertia 16.268474578857422\n",
      "Iteration 1, inertia 9.641810417175293\n",
      "Iteration 2, inertia 9.639556884765625\n",
      "Iteration 3, inertia 9.63915729522705\n",
      "Iteration 4, inertia 9.63909912109375\n",
      "Converged at iteration 4: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 16.266820907592773\n",
      "Iteration 1, inertia 9.641510009765625\n",
      "Iteration 2, inertia 9.63865852355957\n",
      "Iteration 3, inertia 9.638092041015625\n",
      "Iteration 4, inertia 9.63801097869873\n",
      "Iteration 5, inertia 9.637880325317383\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 16.269073486328125\n",
      "Iteration 1, inertia 9.651985168457031\n",
      "Iteration 2, inertia 9.650073051452637\n",
      "Iteration 3, inertia 9.649685859680176\n",
      "Iteration 4, inertia 9.649396896362305\n",
      "Iteration 5, inertia 9.649317741394043\n",
      "Iteration 6, inertia 9.649259567260742\n",
      "Converged at iteration 6: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 16.274368286132812\n",
      "Iteration 1, inertia 9.636007308959961\n",
      "Iteration 2, inertia 9.634337425231934\n",
      "Iteration 3, inertia 9.634092330932617\n",
      "Iteration 4, inertia 9.634029388427734\n",
      "Converged at iteration 4: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 16.276512145996094\n",
      "Iteration 1, inertia 9.633094787597656\n",
      "Iteration 2, inertia 9.631359100341797\n",
      "Iteration 3, inertia 9.630962371826172\n",
      "Iteration 4, inertia 9.630884170532227\n",
      "Converged at iteration 4: strict convergence.\n",
      "Time taken for K Means clustering:  253.30347156524658 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.wv.vectors\n",
    "print(word_vectors.shape)\n",
    "\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters,\n",
    "                           n_init=5,\n",
    "                           verbose=2,\n",
    "                           #n_jobs=-2,\n",
    "                           random_state=0)\n",
    "\n",
    "idx = kmeans_clustering.fit_predict(word_vectors) # trimmed last line because of an error\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.wv.index_to_key, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain.  \n",
    "Your clusters may differ, as Word2Vec relies on a random number seed.  \n",
    "Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n",
    "Run k-means on the word vectors and print a few clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['scenery', 'graveyard']\n",
      "\n",
      "Cluster 1\n",
      "['smash']\n",
      "\n",
      "Cluster 2\n",
      "['were', '10', 'running_around', '30', 'disturbed', 'nostalgic', 'citizens', 'severely']\n",
      "\n",
      "Cluster 3\n",
      "['lately', 'directs', 'psychopath', 'stacey']\n",
      "\n",
      "Cluster 4\n",
      "['b-movie', 'lights', 'uncertainty']\n",
      "\n",
      "Cluster 5\n",
      "['innocent', '2006', 'ages', 'population', 'bone', 'switches', 'guaranteed', 'bourgeois']\n",
      "\n",
      "Cluster 6\n",
      "['forced', 'go_through', 'terrified', 'skillfully']\n",
      "\n",
      "Cluster 7\n",
      "['street', 'hutton', '1992']\n",
      "\n",
      "Cluster 8\n",
      "['loose', 'tell_me', 'pan', 'steaming_pile', 'p.s']\n",
      "\n",
      "Cluster 9\n",
      "['anyone', 'noted', 'characterisation', 'prisoner', 'firemen']\n"
     ]
    }
   ],
   "source": [
    "# Print the first ten clusters\n",
    "for cluster in range(0,10):\n",
    "    # Print the cluster number\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        #print(len(word_centroid_map.values()))\n",
    "        #print(cluster)\n",
    "        #print(word_centroid_map.keys())\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are of varying quality. Some make sense, some cointain mostly names, and some contain related adjectives. On the other hand, some are a little mystifying. Perhaps our algorithm works best on adjectives.\n",
    "\n",
    "At any rate, now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    # The number of clusters is equal to the highest cluster index in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above will give us a numpy array for each review, each with a number of features equal to the number of clusters. Finally, we create bags of centroids for our training and test set, then train a random forest and extract results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** Create bags of centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traincvCT, X_testcvCT, y_traincvCT, y_testcvCT = model_selection.train_test_split(train_centroids,\n",
    "                                                                                    train[\"sentiment\"],\n",
    "                                                                                    test_size=0.2,\n",
    "                                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "clf_RF_CT = RandomForestClassifier(n_estimators=100, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=True, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvCT, y_traincvCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456\n"
     ]
    }
   ],
   "source": [
    "eval_RF_CT_tts = clf_RF_CT.score(X_testcvCT, y_testcvCT)\n",
    "print(eval_RF_CT_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote BagOfCentroids.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/linalg/__init__.py:217: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n",
      "/opt/conda/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/linalg/__init__.py:217: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n"
     ]
    }
   ],
   "source": [
    "result = clf_RF_CT.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"./data/BagOfCentroids.csv\", index=False, quoting=3)\n",
    "print(\"Wrote BagOfCentroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the code above gives about the same (or slightly worse) results compared to the Bag of Words in Part 1.\n",
    "\n",
    "## Part 4: Comparing deep and non-deep learning methods\n",
    "You may ask: Why is Bag of Words better?\n",
    "\n",
    "The biggest reason is, in our tutorial, averaging the vectors and using the centroids lose the order of words, making it very similar to the concept of Bag of Words. The fact that the performance is similar (within range of standard error) makes all three methods practically equivalent.\n",
    "\n",
    "A few things to try:\n",
    "\n",
    "First, training Word2Vec on a lot more text should greatly improve performance. Google's results are based on word vectors that were learned out of more than a billion-word corpus; our labeled and unlabeled training sets together are only a measly 18 million words or so. Conveniently, Word2Vec provides functions to load any pre-trained model that is output by Google's original C tool, so it's also possible to train a model in C and then import it into Python.\n",
    "\n",
    "Second, in published literature, distributed word vector techniques have been shown to outperform Bag of Words models. In this paper, an algorithm called Paragraph Vector is used on the IMDB dataset to produce some of the most state-of-the-art results to date. In part, it does better than the approaches we try here because vector averaging and clustering lose the word order, whereas Paragraph Vectors preserves word order information.\n",
    "\n",
    "What is Deep Learning?\n",
    "\n",
    "The term \"deep learning\" was coined in 2006, and refers to machine learning algorithms that have multiple non-linear layers and can learn feature hierarchies [1].\n",
    "\n",
    "Most modern machine learning relies on feature engineering or some level of domain knowledge to obtain good results. In deep learning systems, this is not the case -- instead, algorithms can automatically learn feature hierarchies, which represent objects in increasing levels of abstraction. Although the basic ingredients of many deep learning algorithms have been around for many years, they are currently increasing in popularity for many reasons, including advances in compute power, the falling cost of computing hardware, and advances in machine learning research.\n",
    "\n",
    "Deep learning algorithms can be categorized by their architecture (feed-forward, feed-back, or bi-directional) and training protocols (purely supervised, hybrid, or unsupervised) [2].\n",
    "\n",
    "Some good background materials include:\n",
    "\n",
    "[1] \"Deep Learning for Signal and Information Processing\", by Li Deng and Dong Yu (out of Microsoft)\n",
    "\n",
    "[2] \"Deep Learning Tutorial\" (2013 Presentation by Yann LeCun and Marc'Aurelio Ranzato)\n",
    "\n",
    "Where Does Word2Vec Fit In?\n",
    "\n",
    "Word2Vec works in a way that is similar to deep approaches such as recurrent neural nets or deep neural nets, but it implements certain algorithms, such as hierarchical softmax, that make it computationally more efficient.\n",
    "\n",
    "See Part 2 of this tutorial for more on Word2Vec, as well as this paper: Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "In this tutorial, we use a hybrid approach to training -- consisting of an unsupervised piece (Word2Vec) followed by supervised learning (the Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
