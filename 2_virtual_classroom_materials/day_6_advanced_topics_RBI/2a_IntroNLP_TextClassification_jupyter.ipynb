{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2100447-8243-4301-9858-051d2f6ff757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Text classification\n",
    "In this notebook, we're going to experiment with a few \"traditional\" approaches to text classification. These approaches pre-date the deep learning revolution in Natural Language Processing, but are often quick and effective ways of training a text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78dee80d-8c34-41df-b36d-578ff8d97cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958b082f-16db-4c1e-918d-a9563f2cf589",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We will be analyzing a dataset comprising nearly 21,000 titles and abstracts of research papers.\n",
    "Our objective is to determine the topics of each article based on this data.\n",
    "It is important to note that articles may be associated with multiple topics.\n",
    "We will discuss the implications of this aspect in more detail later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a07c723-e30c-4c94-a104-1470893046d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading data files\n",
    "\n",
    "We begin by extracting the data from a zip files which contains the dataset in form of a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69773801-8f71-4f90-b0fa-fd72f029b123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with ZipFile(os.path.join(\"../../Data\", \"topics\", \"train.csv.zip\"), \"r\") as myzip:\n",
    "    with myzip.open(\"train.csv\") as myfile:\n",
    "        train_df = pd.read_csv(myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "334c9039-b27d-45ef-a60c-9d423c3e5930",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's have a look at the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc15c932-47fa-45cb-8aba-42ced833fc46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d732239b-8a88-4cb8-9e30-12d7d8ab2fe1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The last 6 columns encode the topics of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3046e80-da0d-4599-8d64-90b04e0f98be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Mathematics\",\n",
    "    \"Statistics\",\n",
    "    \"Quantitative Biology\",\n",
    "    \"Quantitative Finance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba7f4284-ae19-4da1-b68d-9ddede057e42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets now check for NULL values and the data types of columns.  \n",
    "(Sometimes columns which contain float or integer values are assigned the data type object. In that case we need to change the data type.)  \n",
    "The [`info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method conveniently gives us all of these informations plus the shape of the data frame and the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a92323-ace6-41b9-8131-33019d4713f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655008c4-cd04-4e4b-b58c-4a8960b2945d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Preprocessing\n",
    "The first step in the development of any NLP model is text preprocessing.\n",
    "This means we're going to transform our texts from word sequences to feature vectors.\n",
    "These feature vectors each contain the values of' a large number of features.  \n",
    "\n",
    "In this experiment, we're going to work with so-called **\"bag-of-word\"** approaches.\n",
    "Bag-of-word methods treat every text as an unordered collection of words (or optionally, _ngrams_),\n",
    "and the raw feature vectors simply tell us how often each word (or ngram) occurs in a text.\n",
    "In Scikit-learn, we can construct these raw feature vectors with\n",
    "[`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html),\n",
    "which tokenizes a text and counts the number of times any given text contains every token in the corpus.\n",
    "During this step we'll also discard so called stop words.\n",
    "Stop words are words like *and*, *the*, *her*, which are presumed to be uninformative in representing the content of a text.\n",
    "Always be aware that these words are removed, as there is no general solution for this task.\n",
    "\n",
    "However, these raw counts are not very informative yet.\n",
    "This is because the raw feature vectors of most texts, even though stop words are removed, in the same language will still be very similar.\n",
    "We are interested in words that occur often in one text, but not very often in the corpus as a whole.\n",
    "Therefore we're going to weight all features by their\n",
    "[**tf-idf score**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf),\n",
    "which counts the number of times every token appears in a text and divides it by (the logarithm of) the percentage of corpus documents that contain that token.\n",
    "This weighting is performed by Scikit-learn's\n",
    "[`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).  \n",
    "\n",
    "To obtain the weighted feature vectors, we combine the\n",
    "[`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "and\n",
    "[`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
    "in a Pipeline, and fit this pipeline on the training data.\n",
    "Conveniently Scikit-learn has this Pipeline already implemented as\n",
    "[`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "We then transform both the training texts and the test texts to a collection of such weighted feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba85a10c-a474-4b21-950a-72413d5178fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cleaning the text\n",
    "\n",
    "Before we vectorize our text we'll perform some manual cleaning.\n",
    "Let's concatenate 'Title' and 'Abstract' and make it one big text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe23135-5b00-46e5-bf35-ccefa3a7a52f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"text\"] = train_df[\"TITLE\"] + \" \" + train_df[\"ABSTRACT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb54ab4-e06f-4252-ab80-728341204e02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We drop the 'Title' and 'Abstract' columns as they are not needed anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a17721e-66f5-4de8-97df-bccfd6b6aad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.drop([\"TITLE\", \"ABSTRACT\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f4e517-6c90-49be-9b05-fe2171b16c04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e8995d-ef16-4cb3-b58c-270f2b4f3ba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    x = re.sub(\"[^\\w]|_\", \" \", input_text)  # only keep numbers and letters and spaces\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"[^\\x00-\\x7f]\", r\"\", x)  # remove non ascii texts\n",
    "    x = [y for y in x.split(\" \") if y]  # remove empty words\n",
    "    x = [\"[number]\" if y.isdigit() else y for y in x]\n",
    "    cleaned_text = \" \".join(x)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a93d7c5-01ab-455f-9513-21a4dfd5d055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"cleaned_text\"] = train_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a80723-aa1a-46f2-b882-bb9e2c4f58c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68eea5d0-7844-42aa-b505-533eaafaf0ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below we have a look at one of the cleaned texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e69595-8e08-489a-9be1-721c670eaa24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.cleaned_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9187797d-81e5-48b1-9b8d-7ca0b67bde06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Changing text into numericals using Tfidf technique\n",
    "\n",
    "Before we can apply the Tfidf technique we need to split the data into training and test sets.\n",
    "Otherwise information from the test set would leak into the training data and any result would be spoilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8c0067-b49d-4401-a84b-982759ac73a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df.loc[:, \"cleaned_text\"], train_df.loc[:, categories], test_size=0.2\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cc9e938-7717-4f29-9754-87914f8d34a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that our text is cleaned we will apply Tfidf on the text data to convert it into a matrix of numericals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7196ae3-daf7-4e9a-8e9d-0f491d6a3a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_features=10000,\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    token_pattern=r\"\\w{1,}\",\n",
    "    ngram_range=(1, 2),\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbf3f79-0b38-4e42-961c-21c4e93a4102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a582f03-654d-422d-9917-a74ac2031535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ed07a0-4220-46a7-965f-7986ec919af7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "527869bd-be7e-4264-b837-405891e0d3a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we are done with preprocessing the text corpus.\n",
    "We transformed each title + abstract of the articles to a numerical vecotor of length 10'000.\n",
    "This representation enables us to train classifiers we alread know and use them for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e52cc61-165c-48ae-b7a9-be1b9db2f0b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Multilabel (text) classification\n",
    "\n",
    "There are four main types of classification problems:\n",
    "- **Binary**: The traget label has only two values and all observations belong to either one label or the other.\n",
    "- **Multiclass**: The traget label has more than two values and all observations are assigned exactly one label.\n",
    "- **Multilabel**: The target label has two or more values but any observation is assigned one or more labels.\n",
    "- **Multitask**: The target is label each observation with multiple lables with non-binary properties.\n",
    "\n",
    "As mentioned earlier we are dealing with a multilabel classification task.\n",
    "(Our target, `y_train` has multiple columns but each column is binary.)\n",
    "Let's explore the data further and investigate the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef9b2a8-5232-4186-bcae-0fb1e316a4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "number_labels = y_train.sum(axis=1)\n",
    "no_label_count = number_labels[number_labels < 1].count()\n",
    "\n",
    "print(\"Number of articles in the training data = \", y_train.shape[0])\n",
    "print(\"Total number of  training articles without label = \", no_label_count)\n",
    "print(\"Total labels in training data = \", y_train.sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da517d4d-45ef-490d-a181-f0ea0e0d1956",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have more than 16,000 articles in the train data. \n",
    "All the articles are labeled under at least one topic.\n",
    "There are some articles with more than one topic.\n",
    "As our dataset contains articles with multiple tags, we are dealing with a **multi-label classification problem**.\n",
    "\n",
    "Let us plot a graph to look at the class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624d4124-dd06-43b2-af36-508cfed28c20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's check each how many abstracts belongs to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6aec406-69f5-4c88-abde-87f25e0334c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "category_count = y_train.sum()\n",
    "print(category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d491dae-1cad-4b9b-b2d4-57db64532a0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(category_count.index, category_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a8a7411-7438-4be7-9dbb-1c9f70e68083",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The plot indicates that “Quantitative Biology” and “Quantitative Finance” categories contain significantly fewer entries compared to other categories, highlighting an imbalance in the dataset. This imbalance poses a challenge in accurately predicting outcomes for the minority classes. Although multilabel classification tasks complicate the application of resampling techniques, it is important to be aware that [specialized methods](https://www.sciencedirect.com/science/article/abs/pii/S0950705115002737) do exist to tackle such imbalances. We will continue with the current dataset for this analysis, but these strategies should be considered for future refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159440b7-3af1-4c6d-b33b-98d133454705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluating the model\n",
    "Since we are dealing with a new kind of problem we also need to think about how we will measure the preformance of our models.\n",
    "\n",
    "To evaluate the performance we will be using three different metrics.\n",
    "- [`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html): \n",
    "In multilabel classification, this function computes subset accuracy.\n",
    "The set of labels predicted for a sample must exactly match the corresponding set of labels in `y_true`.\n",
    "- [`f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):\n",
    "By setting `average=\"macro\"` the metric is calculating the mean of the f1 score for each class. This way each class is of equal importance and a dominant class has less influce.\n",
    "- [`hamming_loss()`]():\n",
    "The Hamming loss corresponds to the [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) between `y_true` and `y_pred`.\n",
    "It ranges from 0 to 1, where a smaller value indicates better performance.\n",
    "\n",
    "Scikit-learn enables us to calculate a confusion matrix for each class by using [`multilabel_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html).\n",
    "Below we have wrapped this function to automatically plot all of classes in a neat grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef15321c-98e6-4243-a491-177387b0f348",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def plot_confusion_matrices(y_true, y_pred, nrows=2, figsize=5):\n",
    "    confusion_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    number_matrices = y_true.shape[1]\n",
    "    nrows = nrows\n",
    "    ncols = ceil(number_matrices / nrows)\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(figsize * ncols, figsize * nrows))\n",
    "\n",
    "    for (i, confusion_matrix, title) in zip(\n",
    "        np.arange(number_matrices), confusion_matrices, y_train.columns\n",
    "    ):\n",
    "        sns.heatmap(\n",
    "            confusion_matrix,\n",
    "            cmap=\"Blues\",\n",
    "            ax=axs[i // ncols, i % ncols],\n",
    "            cbar=False,\n",
    "            fmt=\"g\",\n",
    "            annot=True,\n",
    "        )\n",
    "        axs[i // ncols, i % ncols].set_title(title)\n",
    "        axs[i // ncols, i % ncols].set_xlabel(\"Predicted\")\n",
    "        axs[i // ncols, i % ncols].set_ylabel(\"Actual\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbdb7a5c-e7d7-49a1-9cb4-a07795a18fda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that all the components are in place we can finally begin to fit a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383e42f7-2102-4dfa-8334-657514838f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Naive Approach\n",
    "\n",
    "There are multiple ways of dealing with such a classification task.\n",
    "First we will showcase the most straight forward one.\n",
    "For each of the classes we will train an independet binary classifier.\n",
    "\n",
    "The types of classifiers are explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05e28db-dd51-4236-9053-2bc35bb771ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Classifiers\n",
    "We're going to experiment with three classic text classification models: Naive Bayes, Support Vector Machines and Logistic Regression. \n",
    "\n",
    "[Naive Bayes classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) are extremely simple classifiers that assume all features are independent of each other. They just learn how frequent all classes are and how frequently each feature occurs in a class. To classify a new text, they simply multiply the probabilities for every feature \\\\(x_i\\\\) given each class \\\\(C\\\\) and pick the class that gives the highest probability: \n",
    "\n",
    "$$ \\hat y = argmax_k \\, [ \\, p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k)\\, ]  $$\n",
    "\n",
    "Naive Bayes Classifiers are very quick to train, but usually fall behind in terms of performance.\n",
    "\n",
    "[Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) are much more advanced than Naive Bayes classifiers. They try to find the hyperplane in the feature space that best separates the data from the different classes. They do so by picking the hyperplane that maximizes the distance to the nearest data point on each side. When the classes are not linearly separable, SVMs map the data into a higher-dimensional space where a linear separation can hopefully be found. SVMs often achieve very good performance in text classification tasks.\n",
    "\n",
    "[Logistic Regression models](https://en.wikipedia.org/wiki/Logistic_regression), finally, model the log-odds \\\\(l\\\\), or \\\\(\\log[p\\,/\\,(1-p)]\\\\), of a class as a linear model and estimate the parameters \\\\(\\beta\\\\) of the model during training: \n",
    "\n",
    "\\\\(l = \\beta_0 + \\sum_{i=1}^n \\beta_i x_i\\\\)\n",
    "\n",
    "Like SVMs, they often achieve great performance in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a86da6-86de-4d91-ac25-1ba84018dd6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f56bab-c097-4078-870b-19dc0be8f586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"nb_mo\": MultiOutputClassifier(MultinomialNB()),\n",
    "    \"svm_mo\": MultiOutputClassifier(LinearSVC()),\n",
    "    \"lr_mo\": MultiOutputClassifier(LogisticRegression()),\n",
    "}\n",
    "\n",
    "print(\"Training MultiOutput Naive Bayes classifier...\")\n",
    "classifiers[\"nb_mo\"].fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training MultiOutput SVM classifier...\")\n",
    "classifiers[\"svm_mo\"].fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training MultiOutput Logistic Regressor...\")\n",
    "classifiers[\"lr_mo\"].fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b00130dc-6649-4b95-91c9-5ea7d14a4250",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'll have a look at another approach and have a look at the results later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eff9be8-f3ce-49f3-b57f-861350fbad81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Classifier chain approach\n",
    "Often the labels are correlated.\n",
    "With the previous naive approach we have dismissed such information entierly.\n",
    "A classifier chain fits a classifier for each label sequentially and uses the prediction of all the previous labels as input as well to leverage correlation between labels.\n",
    "\n",
    "Let's fit the models and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7412cc7a-6557-4f73-a62d-e9ddb0694aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifiers[\"nb_chain\"] = ClassifierChain(MultinomialNB())\n",
    "classifiers[\"svm_chain\"] = ClassifierChain(LinearSVC())\n",
    "classifiers[\"lr_chain\"] = ClassifierChain(LogisticRegression())\n",
    "\n",
    "print(\"Training chain Naive Bayes classifier...\")\n",
    "classifiers[\"nb_chain\"].fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training chain SVM classifier...\")\n",
    "classifiers[\"svm_chain\"].fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training chain Logistic Regressor...\")\n",
    "classifiers[\"lr_chain\"].fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f84bb7d-b3e0-43c1-80ab-8baeaf927eb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b70029f-a570-4b95-8a1f-e3f2af4f7940",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_predictions = {\n",
    "    type: classifiers[type].predict(X_train_tfidf) for type in classifiers\n",
    "}\n",
    "test_predictions = {\n",
    "    type: classifiers[type].predict(X_test_tfidf) for type in classifiers\n",
    "}\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"Multilabel\": [\"MultiOutput\"] * 3 + [\"ClassifierChain\"] * 3,\n",
    "        \"Classifier\": [\"Naive Bayes\", \"SVM\", \"Logistic Regression\"] * 2,\n",
    "        \"Train Accuracy\": [\n",
    "            accuracy_score(y_train, train_predictions[type])\n",
    "            for type in train_predictions\n",
    "        ],\n",
    "        \"Train (Macro) F1 score\": [\n",
    "            f1_score(y_train, train_predictions[type], average=\"macro\")\n",
    "            for type in train_predictions\n",
    "        ],\n",
    "        \"Train Hamming Loss\": [\n",
    "            hamming_loss(y_train, train_predictions[type]) for type in train_predictions\n",
    "        ],\n",
    "        \"Test Accuracy\": [\n",
    "            accuracy_score(y_test, test_predictions[type]) for type in test_predictions\n",
    "        ],\n",
    "        \"Test (Macro) F1 score\": [\n",
    "            f1_score(y_test, test_predictions[type], average=\"macro\")\n",
    "            for type in test_predictions\n",
    "        ],\n",
    "        \"Test Hamming Loss\": [\n",
    "            hamming_loss(y_test, test_predictions[type]) for type in test_predictions\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# metrics.set_index([\"Multilabel\", \"Classifier\"], inplace=True)\n",
    "\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f02ac6c6-f53c-4cfc-b1f7-5319772bdade",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- If we check out the results above we can see that the SVM classifiers perform well on the training data however they tend to overfit, since the results are not as good on the test set.\n",
    "- On the other hand the Logistic Regression models have similar performance on the test set.\n",
    "- For all classifiers it holds true that the chain approach performs better than the naive approach.\n",
    "\n",
    "Let's check out the confusion matrices for the chain Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f8ccd2-9113-459c-8bec-19e3c3f8dc84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrices(y_test, test_predictions[\"lr_chain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c782a1-bcab-4aef-a39a-719508ea2eab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The low number of true positives highlights the problem of the imbalanced data set.\n",
    "There is not enough data to properly learn how to identify \"Quantitative Biology\" and \"Quantitative Finance\" articles.\n",
    "\n",
    "For all the results above keep in mind that the hyperparameters of the models are not tuned.\n",
    "Feel free to experiment with the hyperparameters to see if you can improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b08b411-bc58-4324-8fef-1f4a1f791696",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/  \n",
    "* https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff  \n",
    "* https://www.thepythoncode.com/article/text-classification-using-tensorflow-2-and-keras-in-python   \n",
    "* https://www.kaggle.com/datasets/blessondensil294/topic-modeling-for-research-articles/code"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2a_IntroNLP_TextClassification_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
