{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22bcc196-796f-43b9-8064-a97bda77f20f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad52f2c7-9d4e-4d18-81e0-d0694170369a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "716d3189-d60d-4574-b2d2-3ddc40b6ee9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "We load the [Boston housing data](http://lib.stat.cmu.edu/datasets/boston) and split it into train and test data. \n",
    "As in the last notebook, we generate [polynomial features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "of the second degree.\n",
    "We will work further with `X_train_poly`, `y_train`, `X_test_poly` and `y_test`. \n",
    "Run the cell below.\n",
    "\n",
    "### Ethical considerations\n",
    "\n",
    "The dataset, which we are using in this exercise, has an ethical problem.\n",
    "A thorough discussion of the issues can be found in [this article](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8).  \n",
    "The key take aways are, that there is a attribute called 'B' in the data.\n",
    "The\n",
    "[original authors](https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air)\n",
    "of the dataset engineered this feature assuming that racial self-segregation has a positive impact on house prices.\n",
    "Such an attribute furthers systemic racism and must not be used outside of educational purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de678226-d580-4f17-8dbc-61f1ab01ae4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The data set is originally downloaded from  \"http://lib.stat.cmu.edu/datasets/boston\".\n",
    "\n",
    "raw_df = pd.read_csv('../../../Data/Boston.csv')\n",
    "\n",
    "y = raw_df['target']\n",
    "X = pd.DataFrame(raw_df.iloc[:,1:-1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "# depending on the version of sklearn, this will cause an error\n",
    "# in that case, replace \"get_feature_names_out\" with \"get_feature_names\"\n",
    "poly_names = poly.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f087c33-650b-4297-a072-06bcb30cf27a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "How many features are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be21ca07-e1a0-4bea-a975-8418f2d1459f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "X_train_poly.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab32042f-e3ef-41c6-9388-78d9c72608d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will further use the user-defined function `plot_coef` that takes as input coefficients as output of the fitted model. It plots the coefficient values and calculates average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158194a3-eea8-46bf-84b7-6d5582a19738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_coef(lr_coef, names=[], ordered=True, hide_zero=False, figsize=(12,20)):\n",
    "    \"\"\"\n",
    "    The function plots coefficients' values from the linear model.\n",
    "    --------\n",
    "    params:\n",
    "        lr_coef: coefficients as they are returned from the classifier's attributes\n",
    "        names: names for the coefficients, if left empty x0, x1, ... will be used\n",
    "        ordered: order the coefficients according to their value\n",
    "        hide_zero: hide all coefficients which are equal to 0\n",
    "        figsize: tuple spcifying the size of the plot\n",
    "    \"\"\"\n",
    "    if len(names) < 1:\n",
    "        names = [f\"x{i}\" for i in range(len(lr_coef))]\n",
    "\n",
    "    named_coef = pd.DataFrame({\"attr\": names, \"coef\": lr_coef})\n",
    "    \n",
    "    if hide_zero:\n",
    "        named_coef = named_coef[named_coef[\"coef\"] != 0]\n",
    "\n",
    "    if ordered:\n",
    "        named_coef.sort_values(by=\"coef\", ascending=True, inplace=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.axvline(x=0, c=\"orange\", ls=\"--\")\n",
    "    ax.scatter(x=\"coef\", y=\"attr\", data=named_coef)\n",
    "    ax.margins(y=0.01)\n",
    "    ax.set_title(\"Coefficients' values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6828b7b2-4571-4ea5-b2f5-6a6bf8147d71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit linear regression without regularization\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Instantiate a [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) under the variable `lr`.\n",
    "- Fit `lr` to `X_train_poly`, `y_train `.\n",
    "- Predict with `lr` on `X_train_poly` and store the results to `y_hat_train`.\n",
    "- Predict with `lr` on `X_test_poly` and store the results to `y_hat_test`.\n",
    "- Return the RMSE for `y_hat_train` as well as for `y_hat_test`. \n",
    "\n",
    "How do you interpret the difference in performance of the model on train and on test dataset? Can you tell if the model overfits/underfits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377355d4-c524-4687-9f24-7984b3193b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_poly, y_train)\n",
    "\n",
    "y_hat_train = lr.predict(X_train_poly)\n",
    "y_hat_test = lr.predict(X_test_poly)\n",
    "\n",
    "print(f\"RMSE train: {root_mean_squared_error(y_train, y_hat_train)}\")\n",
    "print(f\"RMSE test: {root_mean_squared_error(y_test, y_hat_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23e57596-fbcc-43cc-88b1-28c0977b948f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The RMSE is almost twice as big for the test set than for the train set. This suggests overfitting and a poor generalization power of the model.\n",
    "\n",
    "We use the function `plot_coef` on the coefficients of the fitted model to see the values of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7177b6-e7b4-4d6c-89ca-740540ad5545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_coef(lr.coef_, poly_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bde847-a320-4487-adb6-aa809bf9150e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The error values on train and test suggest that we deal here with overfitting of the model on the given set of polynomial features. \n",
    "We should therefore use **regularization**. \n",
    "\n",
    "## Standardization\n",
    "\n",
    "Before fitting any regularized model, the scaling of the features is crucial.\n",
    "Otherwise the regularization would not be fair to features of different scales.\n",
    "Regularized linear models assume that the inputs to the model have a zero mean and a variance in the same magnitude.\n",
    "[`StandarScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "deducts the mean and divides by the standard deviation. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Instantiate\n",
    "[`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "under the name `scaler`.\n",
    "- Apply the `fit_transform` method with the input `X_train_poly` to `scaler` and store the result into `X_train_scaled`.\n",
    "- Once the scaler is fit to `X_train_poly` you can directly transform `X_test_poly` and store it in the variable `X_test_scaled`. You never want to fit on a test sample, because that way information from the test data might leak. Test data serves only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25048dd4-d2eb-4d96-a502-1d1942d85f2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_scaled = scaler.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b62cfa6a-656b-4914-b9ab-2d7c58088dcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you applied the standardization correctly you should see on the bottom chart the distributions of all the features concentrated around zero with similar ranges of deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9610ed-35af-4b04-9e0e-b5cee7a8e97c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(12, 20))\n",
    "\n",
    "axs[0].boxplot(X_train_poly, vert=False, labels=poly_names)\n",
    "axs[0].set_title('Original polynomial features')\n",
    "\n",
    "axs[1].boxplot(X_train_scaled, vert=False, labels=poly_names)\n",
    "axs[1].set_title('Scaled features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e02407-2532-47c0-b24f-acf1a985f0bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Lasso\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "### Exercise\n",
    "- Instantiate a Lasso regression under the name `lr_l`.\n",
    "- Fit the model to `X_train_scaled` and `y_train`.\n",
    "- Predict on `X_train_scaled` and `X_test_scaled` and store the predictions in `y_hat_train` and `y_hat_test`, respectively.\n",
    "\n",
    "Did the overfit change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb91aa7d-f8d9-44e0-9793-fe3e5b9e8172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lr_l = Lasso()\n",
    "lr_l.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_hat_train = lr_l.predict(X_train_scaled)\n",
    "y_hat_test = lr_l.predict(X_test_scaled)\n",
    "\n",
    "print(f\"RMSE train: {root_mean_squared_error(y_train, y_hat_train)}\")\n",
    "print(f\"RMSE test: {root_mean_squared_error(y_test, y_hat_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741e191a-ed71-426c-972f-ec3deb4e54b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The performance seems to be comparable on train and test dataset. Hence, the model's generalization power is better now.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Use `plot_coef()` on the coefficients of the lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a196ed2-5aec-4313-8a58-2ef388e597a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 5\n",
    "\n",
    "plot_coef(lr_l.coef_, poly_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d210bf5-42a6-4c94-a0c4-927faf1fa9dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The average value of the coefficients is much smaller now. Also, many of the coefficients are equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d9a471-2bb6-4ed3-aa56-43b69d96ef02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'After applying Lasso on polynomial scaled features we remain with {np.sum(lr_l.coef_!=0)} variables.')\n",
    "print('\\nThe selected variables are:\\n- ', end=\"\")\n",
    "print(\"\\n- \".join(poly_names[lr_l.coef_ != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbaac431-2259-4992-80e4-8a8dda3f9ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Take the subset of `X_train_scaled` with only those variables that have a non-zero coefficient and store it in the variable `X_train_lasso`\n",
    "- Do the same selection on `X_test_scaled` and save it to `X_test_lasso`.\n",
    "- How many variables are remaining? Check it with the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dc3687-f20e-4341-b32f-0c9b8dd5223f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6\n",
    "\n",
    "X_train_lasso = X_train_scaled[:,lr_l.coef_!=0]\n",
    "X_test_lasso = X_test_scaled[:,lr_l.coef_!=0]\n",
    "X_test_lasso.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a7f5618-34e2-47c2-b991-af5d8847828f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ridge\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "We have effectively performed a feature selection with Lasso. Now we will compare it to Ridge regression.\n",
    "\n",
    "Let's try different values for the strength of the optimization, alpha. By default it is equal to 1 and it must be a positive value. Larger values specify stronger regularization. Alpha can be set also in Lasso and Elastic Net.\n",
    "\n",
    "### Exercise\n",
    "- Fit the ridge regression to `X_train_scaled` and `y_train` with the values of alpha being 0.001, 0.01, 0.1, 1, 10 and 100 to see the effect of the regularization strength.\n",
    "- Return the RMSE for `X_train_scaled` and `X_test_scaled` for each of the alpha options.\n",
    "- Visulaize both RMSE curves.\n",
    "Are you able to find the ranges where the model is over- or underfitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81051ac7-f552-4902-aed7-81cadb8ab4c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 7\n",
    "\n",
    "rmses = []\n",
    "alphas = [10**i for i in range(-3, 3)]\n",
    "\n",
    "for alpha in alphas:    \n",
    "    lr_r = Ridge(alpha=alpha)\n",
    "    lr_r.fit(X_train_scaled, y_train)\n",
    "    y_hat_train = lr_r.predict(X_train_scaled)\n",
    "    y_hat_test = lr_r.predict(X_test_scaled)\n",
    "    rmse_train = root_mean_squared_error(y_train, y_hat_train)\n",
    "    rmse_test = root_mean_squared_error(y_test, y_hat_test)\n",
    "    rmses.append(pd.DataFrame([{\"alpha\": alpha, \"train\": rmse_train, \"test\": rmse_test}]))\n",
    "\n",
    "rmses = pd.concat(rmses)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(\"alpha\", \"train\", data=rmses, label=\"RMSE for train set\", c=\"b\", ls=\"--\")\n",
    "ax.plot(\"alpha\", \"test\", data=rmses, label=\"RMSE for test set\", c=\"r\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae198cf0-1917-4bad-9231-e5a365ef5e83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b99664d-bfcb-48af-b8fc-d2aed6372312",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the above plot, we can observe a clear trend in the training data: as the regularization parameter \\\\(\\alpha\\\\) increases, the Root Mean Square Error (RMSE) also increases monotonically. \n",
    "This is expected, as a higher \\\\(\\alpha\\\\) imposes more restriction on the coefficients, leading to a simpler model.  \n",
    "The more intriguing effect is seen when we look at the RMSE on the test data. \n",
    "As anticipated, the RMSE is high for large \\\\(\\alpha\\\\) values, a phenomenon known as underfitting.\n",
    "However, as α decreases, the RMSE starts to rise again.\n",
    "This is because the coefficients are not sufficiently constrained, leading to an overly complex model, a situation referred to as overfitting.\n",
    "\n",
    "**Note:** It’s crucial not to use your test data when optimizing the hyperparameter.\n",
    "If you aim to optimize the hyperparameter, consider using cross-validation or alternative metrics such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\n",
    "These metrics penalize complex models and enable you to make an informed decision based solely on your training data.\n",
    "\n",
    "All of these observations also hold for Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d12ba86-576d-4cf8-a9bc-ec13fd192701",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise\n",
    "- Fit the model with a high value of \\\\(\\alpha=100\\\\).\n",
    "- Check how many coefficients equal 0 and plot their valuse using `plot_coef`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b614ca-5c26-444f-b4f0-127face84bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 8\n",
    "\n",
    "lr_r_high = Ridge(alpha=100).fit(X_train_scaled, y_train)\n",
    "print(f\"There are {(lr_r_high.coef_ == 0).sum()} coefficients equal to 0 for this model.\")\n",
    "\n",
    "plot_coef(lr_r_high.coef_, poly_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a7f31a-2079-482b-95c3-0f7a4b6a47c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Even for a highly penalized Ridge regression model, all the coefficients are non zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afd67833-f27e-4c7a-9831-6b90199f2f13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Elastic Net\n",
    "[Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "is a combination of Lasso and Ridge which is defined by a parameter `l1_ratio`.\n",
    "If it is equal to 1 the model is equivalent to Lasso, if it is 0 then it is as if we had a Ridge regression.\n",
    "The regularization strength alpha can be defined just as in Ridge or Lasso. \n",
    "\n",
    "You can enforce the values of the parameters to be positive with the parameter `positive = True`.\n",
    "Such an option is also available for Lasso. \n",
    "\n",
    "For all the variations of the linear regression you can enforce it to fit the model without an intercept.\n",
    "This can be done by setting the parameter `fit_intercept=False`.\n",
    "If `False` the data is assumed to be already centered.\n",
    "\n",
    "There is an option to scale data by the norm of each feature.\n",
    "If normalization is applied to fitting of the model it is automatically also applied to the `predict()`.\n",
    "We can use this method instead of standard scaling done at the beginning. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Experiment with the parameters of `ElasticNet()`.\n",
    "Fit the model to `X_train_scaled` and `y_train` with different set of options, e.g.\n",
    "- `positive=False`\n",
    "- `l1_ratio = 0`, `0.5`, `1`\n",
    "- `alpha = 0.001`, `0.01`, `0.1`, `1`, `10`, `100` \n",
    "\n",
    "Plot the coefficients with `plot_coef` to see the effect on the options.\n",
    "Return the RMSE on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e5e87e-db71-40d0-988e-e1e589f33619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 9\n",
    "\n",
    "lr_en = ElasticNet(l1_ratio=0.5, alpha=1, positive=False)\n",
    "lr_en.fit(X_train_scaled, y_train)\n",
    "plot_coef(lr_en.coef_, poly_names)\n",
    "\n",
    "y_hat_train = lr_en.predict(X_train_scaled)\n",
    "y_hat_test = lr_en.predict(X_test_scaled)\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_hat_train)\n",
    "rmse_test = root_mean_squared_error(y_test, y_hat_test)\n",
    "\n",
    "print(f\"RMSE train: {rmse_train}\")\n",
    "print(f\"RMSE test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebc8d701-198e-4eee-b95b-c9909588c5eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Material adapted for RBI internal purposes with full permissions from original authors."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Answer]2_Regularized_Linear_Models_Exercise_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
