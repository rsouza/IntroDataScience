{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36df0444-1ff5-47b2-a860-aa7f2cecac7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "\n",
    "In this notebook, we are going to explore the k-Nearest Neighbors (kNN) algorithm, a simple yet powerful machine learning technique. The core principle of the kNN algorithm revolves around the concept of similarity. It assumes that data points that are close to each other in the feature space share similar characteristics or belong to the same class. \n",
    "When presented with a new data point, kNN identifies its k nearest neighbors from the training data based on a specified distance metric (e.g., Euclidean, Manhattan). These neighbors' classes or values are then used to determine the classification or regression output for the new data point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "999443dc-4aba-4991-a691-8c2b2f6855bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Two critical parameters in kNN are:\n",
    "- **k**: The number of nearest neighbors to consider. Choosing an appropriate value for k is essential, as it directly impacts the model's performance and generalization ability.\n",
    "- **Distance Metric**: The method used to measure the similarity between data points. The choice of distance metric depends on the nature of the data and the problem domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e1dee14-85c9-4ba7-8d64-c91a8875f8ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. kNN for classification task\n",
    "\n",
    "In scikit-learn, implementing kNN for classification tasks is straightforward using the [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class. It provides flexibility in choosing the value of k and the distance metric. \n",
    "\n",
    "\n",
    "Let's start with importing all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eca14f8-ec2a-4f6b-94cc-1dee9fe4c7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752325db-ac6d-4e3c-870f-401f4c0ff297",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we want to generate synthetic data for classification. We generate synthetic data using the [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function. This function allows us to create a random n-class classification problem with specified features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee715c0d-8ea2-4aab-bf05-f9f2fe796873",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, \n",
    "                           n_clusters_per_class=1, n_redundant=0, random_state=30)\n",
    "\n",
    "# Splitting data into training and testign sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "445d2363-7790-4288-88e8-606385732f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Note: we should normalize the features when using a kNN classifier. To prevent data leakage, normalization should be done after splitting the dataset into the train and test sets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1451de76-e7e7-47b3-9dc9-b95f406cc8aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's have a look at our generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d741ce-3f64-4165-a3f8-676a47c7f561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the generated data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Generated Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5273219f-7db6-4c3d-b8cd-eab3a3ce6e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the generated test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de82e3b-f186-42cd-894d-b423c79d46e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1 Basic kNN-classifier\n",
    "\n",
    "We will now implement the kNN classifier with default parameters (k = 5, metric = minkowski):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fbaac12-e1b5-41d6-a51e-4756f4e6d053",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Implementing basic kNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of kNN Classifier with 5 neighbors: {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3892a5-6610-469a-a3c6-dcb74729bc8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since our test set size is 20 points and the accuracy is 90%, our model misclassified 2 data points.\n",
    "\n",
    "But how does the classification of a new observation actually work?\n",
    "First the k nearest neighbors, according to the specified metric, are searched in the training data.\n",
    "With the k nearest observations the algorithm performs majority voting.\n",
    "If there is a tie, either because there are more than two classes or an even numbered k, we randomly pick one of the top classes.\n",
    "Sometimes the voting is augmented by a weight function but more on that later.\n",
    "\n",
    "Let's visualize the decision boundary of the classifier. \n",
    "The code below creates a meshgrid of points covering the feature space, predicts the class labels for each point, and plots the decision boundary along with the data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432f538e-0755-4744-9d8a-0633730e2a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "h = 0.02 \n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                      np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plotting decision boundary\n",
    "Z = knn_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.8)\n",
    "plt.contour(xx, yy, Z, colors=\"black\", linewidths=0.2)\n",
    "\n",
    "# Plotting data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision boundary')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e882f7-f2ff-4375-9520-62b8cbaa1d01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2 Choosing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "806300b5-2c83-4f53-96ec-e899e2aba93d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see, using the default classifier is very easy. However, we know that kNN has two important parameters - the number of neighbors and the distance metric. What should we consider when choosing the **number of neighbors**? \n",
    "* A small value of K in k-Nearest Neighbors can be influenced by outliers, potentially leading to incorrect classifications.\n",
    "* Conversely, a large K value might explore data too far from the point in consideration, reducing the accuracy of classifications.\n",
    "* Small K values are computationally efficient, while large K values can become computationally expensive.\n",
    "\n",
    "Another important decision is chosing a **distance metric**. The most popular once are:\n",
    "* [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance):\n",
    "  * Suitable for datasets with continuous features.\n",
    "  * Works well when the features have similar scales.\n",
    "  * May not perform optimally if the data has outliers or is not linearly separable.\n",
    "\n",
    "* [Manhattan Distance](https://en.wikipedia.org/wiki/Taxicab_geometry):\n",
    "  * Suitable for high-dimensional data or data with many categorical features.\n",
    "  * Less sensitive to outliers compared to Euclidean distance.\n",
    "  * Works well when the data lies on a grid or when the distances along different dimensions are not directly comparable.\n",
    "\n",
    "* [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance):\n",
    "  * Adaptable to datasets with mixed feature types, accommodating both continuous and categorical variables.\n",
    "  * Particularly useful when diverse feature scales or customized distance measurements are required.\n",
    "\n",
    "\n",
    "In our case euclidean distance is the most suitable metric. \n",
    "\n",
    "\n",
    "What about the number of neighbors? Now it is your turn to experiment with different k!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f3cd0f1-dd8b-4dc5-90bd-b3163299303f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 1: implement the classifier with 3 neighbors\n",
    "\n",
    "knn_classifier3 = KNeighborsClassifier(n_neighbors=3, metric = 'euclidean')\n",
    "knn_classifier3.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions = knn_classifier3.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of kNN Classifier with 3 neighbors: {accuracy:.0%}\")\n",
    "\n",
    "#Task 2: implement the classifier with 7 neighbors\n",
    "knn_classifier7 = KNeighborsClassifier(n_neighbors=7, metric = 'euclidean')\n",
    "knn_classifier7.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions = knn_classifier7.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of kNN Classifier with 7 neighbors: {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ae95528-1ac6-4614-b453-8027be6564e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "k_list = [3, 5, 7]\n",
    "# Create classifiers and plot decision boundaries\n",
    "for i, classifier in enumerate([knn_classifier3, knn_classifier, knn_classifier7]):\n",
    "    # Plot decision boundary\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axes[i].contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    axes[i].contour(xx, yy, Z, colors=\"black\", linewidths=0.2)\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "    axes[i].set_xlim(xx.min(), xx.max())\n",
    "    axes[i].set_ylim(yy.min(), yy.max())\n",
    "    axes[i].set_title(f'Decision Boundary (k = {k_list[i]})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0657d7-d9c3-47e1-9b14-d06193614825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The visualizations above illustrate a key concept: as the parameter k increases, the decision boundary becomes progressively smoother. This is because a larger k value means that the classification of a new point is determined by considering a larger number of neighboring points, which inherently leads to more generalized decision boundaries.\n",
    "\n",
    "Another approach, which gives more reliable and robust result, is cross-validation. It serves a more comprehensive understanding of how different k values affect model performance across different subsets of the data. We'll explore the range of k-values from 1 to 30 to observe how accuracy fluctuates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d73f8ef-82c2-4260-8d76-f983d40cd85e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "k_values = range(1, 31)\n",
    "accuracy_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    # The dataset will be divided into cv=5 equal-sized folds\n",
    "    # with each fold serving as the testing set once and the remaining folds as the training set\n",
    "    # the accuracy score of the classifier is computed on the corresponding testing set\n",
    "    accuracy = cross_val_score(knn_classifier, X, y, cv=5)\n",
    "    # The mean accuracy across the folds will be added to the list\n",
    "    accuracy_scores.append(np.mean(accuracy))\n",
    "\n",
    "sns.lineplot(x = k_values, y = accuracy_scores, marker = 'o')\n",
    "plt.xlabel(\"k-values\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xticks(range(1, 31, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c913bf33-2adf-4b5b-819e-144065bc5241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The maximum accuracy score of 96% can be reached with 2 or 5 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c93032f-b3d6-4b4b-b1c4-e70f0d7e11e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training the model with the optimal k-value\n",
    "\n",
    "max_accuracy = np.max(accuracy_scores)\n",
    "best_k = accuracy_scores.index(max_accuracy)+1\n",
    "\n",
    "knn_classifier_uniform = KNeighborsClassifier(n_neighbors=best_k, metric = 'euclidean')\n",
    "knn_classifier_uniform.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn_classifier_uniform.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of the best kNN Classifier with {best_k} neighbors: {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2c9d54b-d6cd-443f-9042-ff433b563e92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another parameter of\n",
    "[`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "is `weights`. \n",
    "It affects which weight function is used in prediction and can take two options: \n",
    "\n",
    "* uniform (by default), i.e. all points in each neighborhood are weighted equally\n",
    "* distance, i.e. closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "Let's try using a classifier with the weight function based on distance and compare it to a classifier with uniform weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbe1e9b6-c8ea-4be2-9af7-72dd220d80b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 3: Train your model using the best k value we found with cross-validation and weights based on distances\n",
    "\n",
    "knn_classifier_distance = KNeighborsClassifier(n_neighbors=best_k, metric = 'euclidean', weights='distance')\n",
    "knn_classifier_distance.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = knn_classifier_distance.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of kNN Classifier with {best_k} neighbors and weights based on distance: {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac4cbae-bb1a-4e9d-a52a-5f048263c6d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "k_list = ['uniform', 'distance']\n",
    "# Create classifiers and plot decision boundaries\n",
    "for i, classifier in enumerate([knn_classifier_uniform, knn_classifier_distance]):\n",
    "    # Plot decision boundary\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axes[i].contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    axes[i].contour(xx, yy, Z, colors=\"black\", linewidths=0.2)\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "    axes[i].set_xlim(xx.min(), xx.max())\n",
    "    axes[i].set_ylim(yy.min(), yy.max())\n",
    "    axes[i].set_title(f'Decision Boundary (weights = {k_list[i]})')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7031b1ea-6798-4b3d-b47f-c077a6db3cf1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the weights parameter in kNN affects the decision boundary. With \"uniform\" weights, all nearest neighbors carry equal weight in decisions. Conversely, with \"distance\" weighting, closer neighbors hold more influence due to their inverse relationship with distance. Using weights based on distances may enhance model performance in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63cba78-0fdc-4595-aa28-deba9e91c429",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.3 [OPTIONAL] RadiusNeighborsClassifier\n",
    "There is one more nearest neighbors classifier implemented in scikit-learn -\n",
    "[`RadiusNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html).\n",
    "While\n",
    "[`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "relies on the number of nearest neighbors k, \n",
    "[`RadiusNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html)\n",
    "operates based on the number of neighbors within a fixed radius of each training point, where 'r' is a floating-point value specified by the user.\n",
    "The parameter `outlier_label` determines how to deal with observations, where no training data is within the radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc2bdc1-4b9c-44d0-a0a8-df7a36887018",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "radius = np.arange(0.5, 2, 0.1)\n",
    "accuracy_scores = []\n",
    "\n",
    "# Looking for the best radius using cross-validation\n",
    "for r in radius:\n",
    "    knn_classifier = RadiusNeighborsClassifier(radius=r, outlier_label=\"most_frequent\")\n",
    "    accuracy = cross_val_score(knn_classifier, X, y, cv=5)\n",
    "    # Sometimes during cross-validation there may be no points in the given radius\n",
    "    # To prevent the error, we need to clean our accuracy list from nan-values\n",
    "    accuracy = [x for x in accuracy if not np.isnan(x)]\n",
    "    accuracy_scores.append(np.mean(accuracy))\n",
    "\n",
    "max_accuracy = np.max(accuracy_scores)\n",
    "best_radius = radius[accuracy_scores.index(max_accuracy)]\n",
    "\n",
    "knn_classifier = RadiusNeighborsClassifier(radius = best_radius, outlier_label=\"most_frequent\")\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Best accuracy of kNN Classifier with radius {best_radius:.2}: {accuracy:.0%}\")\n",
    "\n",
    "# Visualizing accuracy score with different radius-values\n",
    "sns.lineplot(x = radius, y = accuracy_scores, marker = 'o')\n",
    "plt.xlabel(\"Radius\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xticks(np.arange(0.5, 2, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "984603a6-400e-4372-97be-c45d5071f9f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Compare the graph above with the graph showing how accuracy changes for different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45897f3a-ab2b-4438-8bb7-f9e98d6d8564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "k_list = ['k neighbors classifier', 'Radius neighbors classifier']\n",
    "# Create classifiers and plot decision boundaries\n",
    "for i, classifier in enumerate([knn_classifier_uniform, knn_classifier]):\n",
    "    # Plot decision boundary\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axes[i].contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    axes[i].contour(xx, yy, Z, colors=\"black\", linewidths=0.2)\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "    axes[i].set_xlim(xx.min(), xx.max())\n",
    "    axes[i].set_ylim(yy.min(), yy.max())\n",
    "    axes[i].set_title(f'{k_list[i]}')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41f81bc-c996-4667-afab-7d1c34b310d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. kNN for regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac86adb-242c-4d13-b999-8616c9f3eee7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's implement kNN regression. The algorithm calculates the distance between the input data point and all other data points in the training set. It then selects the k-nearest neighbors and averages their target values to predict the target value for the input data point.\n",
    "\n",
    "Hint: Check out the\n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "or the\n",
    "[User Guide](https://scikit-learn.org/stable/modules/neighbors.html#regression)\n",
    "for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8219769a-7109-4823-bccb-cefc0b3839e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f72535-6bb4-49ab-b694-f250901c6bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10  \n",
    "y = 2 * X.squeeze() + np.random.normal(0, 2, size=X.shape[0]) \n",
    "\n",
    "# Visualize the dataset\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.title('Generated Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f70b825-6e6c-4d0f-94a6-b4777ea21913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.1: Split the dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f326462-9266-479e-86cd-c86f0c97e0ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.2: Create and train the basic kNN-regressor\n",
    "\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "knn_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b098abf5-c20b-48fe-8f58-42e8dc815df0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 5: Make predictions on the test set and evaluate the model\n",
    "\n",
    "predictions = knn_regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55359af4-e0b9-4b82-82b0-93ed5b54b778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 6: Perform cross-validation for each value of k\n",
    "\n",
    "# Create a range of k values for hyperparameter tuning\n",
    "k_values = range(1, 20)\n",
    "\n",
    "# List to store mean squared errors for each value of k\n",
    "mse_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
    "    mse = -cross_val_score(knn_regressor, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_values.append(np.mean(mse))\n",
    "\n",
    "# Visualize MSE\n",
    "sns.lineplot(x = k_values, y = mse_values, marker = 'o')\n",
    "plt.xlabel(\"k-values\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xticks(range(1, 21, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a36e21-7183-4a62-a661-da2dbe050df2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 7: Create and train a model with an optimal k-value\n",
    "\n",
    "# Find an optimal k from the array mse_values\n",
    "smallest_mse = np.min(mse_values)\n",
    "best_k = mse_values.index(smallest_mse)+1\n",
    "\n",
    "# Implement and fit the model\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=best_k)\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44ba63a6-8920-4975-8c9a-4bd8d5904370",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's have a look at the final model, we have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac59ff67-65be-43d8-8b89-583536ec4250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.title('Final Model')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Add the model\n",
    "X_vis = np.arange(0, 10, 0.01).reshape(-1,1)\n",
    "y_vis = knn_regressor.predict(X_vis)\n",
    "plt.plot(X_vis, y_vis, c=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "[Answer]5_Nearest_Neighbors_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
