{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics # to calculate accuracy measure and confusion matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "plt.rcParams[\"figure.figsize\"] = (15,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset for binary regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "print(datasets.load_breast_cancer().DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the data imbalanced\n",
    "\n",
    "For the purpose of this exercise we will make the data imbalanced by removing 80% of the cases where y==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([X,y], axis=1) # join X and y\n",
    "data_neg = data.loc[data.target==0,:] # select only rows with negative target \n",
    "data_pos = data.loc[data.target==1,:].sample(frac=0.07, random_state=42) # select 7% of rows with positive target\n",
    "\n",
    "data_imb = pd.concat([data_neg, data_pos]) # concatenate 7% of positive cases and all negative ones to have imbalanced data\n",
    "X_imb = data_imb.drop(columns=['target'])\n",
    "y_imb = data_imb.target\n",
    "plt.title('frequency of the target variable')\n",
    "plt.xlabel('target value')\n",
    "plt.ylabel('count')\n",
    "plt.hist(y_imb);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split to train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "X_train , X_test , y_train , y_test = ...(, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- fit the default LogisticRegression() to X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model failed to converge due to low number of iterations of the optimization solver. There are multiple solvers that can be chosen as a hyperparameter of the model. These also depend on the strategy that is chosen for regularization and for multiclass problem. Description of which solver suits which problem is in the documentation. We have 3 options now. \n",
    "\n",
    "- increase number of iterations until the default solver converges\n",
    "- select a different optimization algorithm with a hyperparameter solver\n",
    "- scale input data which usually helps optimization algorithms to converge. However, if you do not use regularization, the scaling is not required for a logistic regression. It only helps with a convergence \n",
    "\n",
    "### Exercise\n",
    "\n",
    "- scale the data with a StandardScaler()\n",
    "- fit and transform X_train and save to *X_train_scaled*\n",
    "- transform X_test and save to *X_test_scaled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "scaler = ...\n",
    "X_train_scaled = ...\n",
    "X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- fit the logistic regression to the scaled data\n",
    "- predict on X_train_scaled and save the values to *y_hat*\n",
    "- what are the values that are returned from the predict() method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "...\n",
    "y_hat = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- print different metrics from sklearn.metrics for the predictions on the train set\n",
    "    - accuracy\n",
    "    - confusion matrix\n",
    "    - classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'accuracy {metrics.accuracy_score(y_train, y_hat)}')\n",
    "print(f'confusion matrix\\n {metrics.confusion_matrix(..., ...)}')\n",
    "print(f'classification report\\n {metrics.classification_report(..., ...)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING__: You should never optimize for the results of the test set. Test set should be always set aside and you should evaluate only once you have decided for the final model. You will learn later in the course how to treat such situations in the lecture about hyperparameter tuning.\n",
    "\n",
    "You can see from the confusion matrix that there are only 19 cases of the positive class in the train set while 2 of them were classified incorrectly and 17 correctly. We would rather want to predict correctly all those cases where target = 1. It is not a big deal if we tell the patient that she/he has a cancer while actually there is no cancer. The bigger problem is if we predict that the patient does not have a cancer while she/he actually has it. We can achieve it by changing the value of the threshold that is by default 50%. We should therefore lower the threshold for the probability.\n",
    "\n",
    "After calling .predict() on your model it returned predicted classes. Instead of predicting classes directly you can return probabilites for each instance using predict_proba() method of logistic regression model. One row is one observation. The first column is the probability that the instance belongs to the first class and the second column tells you about the probability of the instance belonging to the second class. Sum of the first and second column for each instance is equal to 1. Which class is the first and which is the second? You can find out with classes_ attribute of the model.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- return classes with classes_ attribute\n",
    "- return probabilites of the X_train_scaled with a predict_proba() method\n",
    "- save the probabilities of the positive class into a variable *probs_train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "print(...)\n",
    "print(...)\n",
    "probs_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "- define the value of a threshold equal to 20%\n",
    "- use probabilities saved in the variable *probs_train*. If the value of the probability is >= than threshold then the prediction should be equal to 1. Hint: boolean values can be converted to 0/1 with boolean_values.astype(int)\n",
    "- return confusion_matrix as well as classification_report for a train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "...\n",
    "preds_train = ...\n",
    "print(...)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems now that all the positive cases are classified correctly thanks to the change of the prediction threshold. Let's check the performance on the test data.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- return probabilites of a positive class from the model on the X_test_scaled dataset\n",
    "- convert the probabilities into predictions with a threshold 20% as above\n",
    "- return confusion_matrix and a classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "probs_test = ...\n",
    "preds_test = ...\n",
    "print(...)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. The model classifies all the 6 positive cases correctly on a test set. There are 2 cases when the patient did not have a cancer but the model predicted a cancer. What we actually wanted to optimize here is a recall for a positive class as we want to catch as many positive cases as possible. You can see the values of recall for class 1 as a function of a threshold on the chart below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = []\n",
    "for threshold in np.linspace(0,1,100):\n",
    "    preds_train = (probs_train>=threshold).astype(int)\n",
    "    recalls.append(metrics.classification_report(y_train, preds_train, output_dict=True,zero_division=1)['1']['recall'])\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('recall for class 1')\n",
    "plt.title(\"A search for optimal threshold\")\n",
    "plt.plot(np.linspace(0,1,100), recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can return parameters of the fitted model. This is convenient for automatic retraining of the model where you can extract the parameters of the best model and also set the parameters of the model with set_params(**params)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Similarly to linear regression you can apply any of the l1, l2 and elastic net regularization techniques. The strength of the regularization is here defined with a parameter C which is an inverse of alpha. This means that smaller the C the stronger the regularization. Default value is 1.\n",
    "\n",
    "Different regularization techniques work only with certain solver, e.g. for L1 penalty we have to use either liblinear or saga solver, L2 can be handled with newton-cg, lbfgs and sag solvers, elasticnet works only with saga solver. For elasticnet you can adjust parameter l1_ratio.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- fit the logistic regression on X_train_scaled with a regularization of your choice with a parameter penalty\n",
    "- change the solver if needed, see documentation\n",
    "- try different values of C to see the effect on results, try also stroner values like 0.1, 0.01,...\n",
    "- predict on X_test_scaled and return classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "...\n",
    "...\n",
    "...\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'coefficients of the logistic regression:\\n {lr.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you fitted for example LogisticRegression(penalty='l1', C = 0.1, solver='liblinear') you would see that many of the coefficients are equal to 0. This behavior of l1 is expected not only for linear but also for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We will use here a dataset with a handwritten numbers in a low resolution of 8x8 pixels. One row is 64 values of pixels. There are 10 classes. You can see few examples of obserations in the picture below. We perform also a usual train test split and a scaling of features to help optimizers converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_digits()\n",
    "X, y = data.data, data.target\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    num = random.randint(0, len(data))\n",
    "    plt.imshow(data.images[num], cmap=plt.cm.gray, vmax=16, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- fit a default logistic regression on X_train_scaled, y_train\n",
    "- predict and print the classification report on X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "print(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in the classification report there is 1 row per 1 class with all the statistics.\n",
    "\n",
    "If you return probabilites with the predict_proba() method you will see that it has 1 column per 1 class. It is a generalization of the binary case. The sum of all the probabilities per 1 row is equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(X_test_scaled)\n",
    "print(f'predict_proba shape: {probs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can handle multinomial regression without any special setting. There is however a parameter that lets you choose the strategy for the multinomial problem. It is either one_vs_rest or softmax regression. The choice of the strategy is also dependent on the selected solver. I.e. if the solver = 'liblinear' then a softmax regression is not possible. In this case and if the problem is binary, the default strategy for multi_class is one vs rest. Otherwise it is softmax\n",
    "\n",
    "### Exercise\n",
    "- fit logistic regression on X_train_scaled, y_train. use parameter multi_class with a value 'ovr' which is one versus rest strategy\n",
    "- return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task:\n",
    "\n",
    "...\n",
    "...\n",
    "...\n",
    "probs = ...\n",
    "print(f'predict_proba shape: {probs.shape}')\n",
    "np.sum(probs,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
