{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1da5cdc-1fcf-463d-b044-c1efd86fb0e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da61718-57bb-42b1-a986-7583d5bfbdb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c5b3f66-00bf-464b-bba5-92b5fdfc784d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "We load the Boston data from `sklearn.datasets` and split it into train and test data. As in the last notebook, we generate polynomial features of the second degree. We will work further with `x_train_poly`, `y_train`, `x_test_poly` and `y_test`. \n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4af6994-109a-426c-8ac3-0bd9aa216b6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The data set is originally downloaded from  \"http://lib.stat.cmu.edu/datasets/boston\".\n",
    "\n",
    "raw_df = pd.read_csv('../Data/Boston.csv')\n",
    "\n",
    "y = pd.DataFrame(raw_df['target'])\n",
    "x = pd.DataFrame(raw_df.iloc[:,1:-1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "x_train_poly = poly.fit_transform(X_train)\n",
    "x_test_poly = poly.transform(X_test)\n",
    "# depending on the version of sklearn, this will cause an error\n",
    "# in that case, replace \"get_feature_names_out\" with \"get_feature_names\"\n",
    "poly_names = poly.get_feature_names_out()\n",
    "\n",
    "names_dict = {'x0': X_train.columns[0],\n",
    "             'x1': X_train.columns[1],\n",
    "             'x2': X_train.columns[2],\n",
    "             'x3': X_train.columns[3],\n",
    "             'x4': X_train.columns[4],\n",
    "             'x5': X_train.columns[5],\n",
    "             'x6': X_train.columns[6],\n",
    "             'x7': X_train.columns[7],\n",
    "             'x8': X_train.columns[8],\n",
    "             'x9': X_train.columns[9],\n",
    "             'x10': X_train.columns[10],\n",
    "             'x11': X_train.columns[11],\n",
    "             'x12': X_train.columns[12]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eccb7e5-37cf-403e-83a6-38612db565b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "How many features are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a4b240-62aa-4a77-9630-1631d3a410ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "x_train_poly.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edfea58f-2e37-470f-a9de-a0dc3de5507d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will further use the user-defined function `plot_coef` that takes as input coefficients as output of the fitted model. It plots the coefficient values and calculates average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7e3fcf7-6f27-45c4-9c30-2aebd1df88f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_coef(lr_coef):\n",
    "    '''\n",
    "    The function plots coefficients' values from the linear model.\n",
    "    --------\n",
    "    params:\n",
    "        lr_coef: coefficients as they are returned from the classifier's attributes\n",
    "    '''\n",
    "    lr_coef = lr_coef.reshape(-1,1)\n",
    "    print(f'AVG coef value: {np.mean(lr_coef)}')\n",
    "    plt.plot(lr_coef)\n",
    "    plt.title(\"Coefficients' values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea9df160-2542-46c4-848c-f3b528aa3661",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit linear regression without regularization\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Instantiate alinear regression under the variable `lr`.\n",
    "- Fit `lr` to `x_train_poly`, `y_train `.\n",
    "- Predict with `lr` on `x_train_poly` and store the results to `y_hat_train`.\n",
    "- Predict with `lr` on `x_test_poly` and store the results to `y_hat_test`.\n",
    "- Return the RMSE for `y_hat_train` as well as for `y_hat_test`. \n",
    "\n",
    "How do you interpret the difference in performance of the model on train and on test dataset? Can you tell if the model overfits/underfits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04d4801d-933f-48fd-974f-efbbcc49d1a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_poly, y_train)\n",
    "\n",
    "y_hat_train = lr.predict(x_train_poly)\n",
    "y_hat_test = lr.predict(x_test_poly)\n",
    "\n",
    "print(f\"RMSE train: {mean_squared_error(y_train, y_hat_train, squared=False)}\")\n",
    "print(f\"RMSE test: {mean_squared_error(y_test, y_hat_test, squared=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e66b26f-6398-4926-adfe-1a4b76f0766e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The RMSE is almost twice as big for the test set than for the train set. This suggests overfitting and a poor generalization power of the model.\n",
    "\n",
    "We use the function `plot_coef` on the coefficients of the fitted model to see the values of the coefficients and the average value of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9c6482-d06b-42bd-8322-14e6161b10bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_coef(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae71adb-3c91-44ac-bdce-ad06d2b9e13f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The coefficients in combination with the error values on train and test suggest that we deal here with overfitting of the model on the given set of polynomial features. We should therefore use **regularization**. \n",
    "\n",
    "## Standardization\n",
    "\n",
    "Before fitting any regularized model, the scaling of the features is crucial. Otherwise the regularization would not be fair to features of different scales. Regularized linear models assume that the inputs to the model have a zero mean and a variance in the same magnitude. `StandarScaler()` deducts the mean and divides by the standard deviation. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Instantiate `StandardScaler()` under the name `scaler`.\n",
    "- Apply the `fit_transform` method with the input `x_train_poly` to `scaler` and store the result into `x_train_scaled`.\n",
    "- Once the scaler is fit to `x_train_poly` you can directly transform `x_test_poly` and store it in the variable `X_test_scaled`. You never want to fit on a test sample, because that way information from the test data might leak. Test data serves only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9c5238-5231-46ea-82aa-ab8c54c0fc4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train_poly)\n",
    "X_test_scaled = scaler.transform(x_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "196ff1bf-6b61-4bb4-a2a7-b03db3849e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you applied the standardization correctly you should see on the bottom chart the distributions of all the features concentrated around zero with similar ranges of deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0743446c-1ed7-4169-91a8-b0f02fb44613",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Original polynomial features')\n",
    "plt.boxplot(x_train_poly)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Scaled features')\n",
    "plt.boxplot(X_train_scaled)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34bf5bc-c62d-4485-a52f-14f393b9bcce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lasso\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "### Exercise\n",
    "- Instantiate a Lasso regression under the name `lr_l`.\n",
    "- Fit the model to `X_train_scaled` and `y_train`.\n",
    "- Predict on `X_train_scaled` and `X_test_scaled` and store the predictions in `y_hat_train` and `y_hat_test`, respectively.\n",
    "\n",
    "Did the overfit change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256f0be6-9676-4627-8fb9-54925c2280d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lr_l = Lasso()\n",
    "lr_l.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_hat_train = lr_l.predict(X_train_scaled)\n",
    "y_hat_test = lr_l.predict(X_test_scaled)\n",
    "\n",
    "print(f\"RMSE train: {mean_squared_error(y_train, y_hat_train, squared=False)}\")\n",
    "print(f\"RMSE test: {mean_squared_error(y_test, y_hat_test, squared=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398c3572-31b2-43be-8849-00cf696fb354",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The performance seems to be comparable on train and test dataset. Hence, the model's generalization power is better now.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Use `plot_coef()` on the coefficients of the lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cef8dcf-5334-4d36-91e0-26a67beccf72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "plot_coef(lr_l.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3afa082b-6e24-47cf-8ea2-735a7d7e1607",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The average value of the coefficients is much smaller now. Also, many of the coefficients are equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e7421b0-461a-44c4-bfae-d5124c211feb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'After applying Lasso on polynomial scaled features we remain with {np.sum(lr_l.coef_!=0)} variables.')\n",
    "print('\\nThe selected variables are:\\n')\n",
    "[print(val) for val in pd.DataFrame(poly_names)[lr_l.coef_!=0].values];\n",
    "print('\\nmapping from polynomial names to original feature names: ')\n",
    "display(names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04d3dbd0-97d3-4ae8-b5d3-62fbccb78589",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Take the subset of `X_train_scaled` with only those variables that have a non-zero coefficient and store it in the variable `x_train_lasso`\n",
    "- Do the same selection on `X_test_scaled` and save it to `x_test_lasso`.\n",
    "- How many variables are remaining? Check it with the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0785c50-acb6-4db2-8810-f507a14b8460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "x_train_lasso = X_train_scaled[:,lr_l.coef_!=0]\n",
    "x_test_lasso = X_test_scaled[:,lr_l.coef_!=0]\n",
    "x_test_lasso.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a604dfd-b2df-439e-868a-a73a8075a85e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d650a0f7-0065-4ab4-aa57-fd62cd6ca5d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "We have effectively performed a feature selection with Lasso. Now we will build on it and use only the selected features in `x_train_lasso` and `x_test_lasso`. \n",
    "\n",
    "Let's try different values for the strength of the optimization, alpha. By default it is equal to 1 and it must be a positive value. Larger values specify stronger regularization. Alpha can be set also in Lasso and Elastic Net.\n",
    "\n",
    "### Exercise\n",
    "- Fit the ridge regression to `x_train_lasso` and `y_train` with the values of alpha being 0.001, 0.01, 0.1, 1, 10 and 100 to see the effect of the regularization strength.\n",
    "- Return the RMSE for `x_train_lasso` for each of the alpha options.\n",
    "- Select the parameter alpha for which the model has the best RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eed9989e-1a91-484c-adaa-02eecafc99c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "rmses = []\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for alpha in alphas:    \n",
    "    lr_r = Ridge(alpha=alpha)\n",
    "    lr_r.fit(x_train_lasso, y_train)\n",
    "    y_hat_train = lr_r.predict(x_train_lasso)\n",
    "    rmses.append(mean_squared_error(y_train, y_hat_train, squared=False))\n",
    "\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title('Errors as a function of a regularization strength')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.plot(alphas, rmses);\n",
    "print(f'The lowest RMSE on a train set is {np.round(np.min(rmses),2)} with alpha = {alphas[np.argmin(rmses)]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebdee40f-14bb-4034-a2ef-d08c89e9fd3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise\n",
    "- Fit the model with the best performance on train data.\n",
    "- Calculate the RMSE on `x_test_lasso` for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b59f0f9-24c4-496b-99ac-c21543357584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "lr_r_best = Ridge(alpha=alphas[np.argmin(rmses)]).fit(x_train_lasso, y_train)\n",
    "y_hat_test = lr_r_best.predict(x_test_lasso)\n",
    "rmse_test = np.round(mean_squared_error(y_test, y_hat_test, squared=False))\n",
    "print(f\"RMSE test: {np.round(rmse_test,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdac27b3-865c-4301-9895-ba3575dcf547",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The RMSEs on the train and the test set are similar!\n",
    "\n",
    "### Exercise\n",
    "Use the function `plot_coef` on the coefficients from the best model to see the coefficients values with their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1618df09-3fa4-4876-b88b-de9551f0a042",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "plot_coef(lr_r_best.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f461a94-983c-4633-97c6-fffb6cf81929",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ae62a8-9b57-49d9-94ed-80f98b8958fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Elastic Net is a combination of Lasso and Ridge which is defined by a parameter `l1_ratio`. If it is equal to 1 the model is equivalent to Lasso, if it is 0 then it is as if we had a Ridge regression. The regularization strength alpha can be defined just as in Ridge or Lasso. \n",
    "\n",
    "You can enforce the values of the parameters to be positive with the parameter `positive = True`. Such an option is also available for Lasso. \n",
    "\n",
    "For all the variations of the linear regression you can enforce it to fit the model without an intercept. This can be done by setting the parameter `fit_intercept=False`.\n",
    "\n",
    "There is an option to scale data by the norm of each feature. If normalization is applied to fitting of the model it is automatically also applied to the `predict()`. We can use this method instead of standard scaling done at the beginning. \n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Experiment with the parameters of `ElasticNet()`. Fit the model to `x_train_lasso` and `y_train` with different set of options, e.g.\n",
    "- `positive=True`\n",
    "- `fit_intercept=False`\n",
    "- `l1_ratio = 0`, `0.5`, `1`\n",
    "- `alpha = 0.001`, `0.01`, `0.1`, `1`, `10`, `100`\n",
    "- `normalize=True`    \n",
    "\n",
    "Plot the coefficients with `plot_coef` to see the effect on the coefficients.\n",
    "Return the RMSE on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b647da2-71bc-4865-981a-2b11e6df8299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "lr_en = ElasticNet(l1_ratio=0.5, alpha=1, positive=True, fit_intercept=False)\n",
    "lr_en.fit(x_train_lasso, y_train)\n",
    "plot_coef(lr_en.coef_)\n",
    "\n",
    "y_hat_train = lr_en.predict(x_train_lasso)\n",
    "y_hat_test = lr_en.predict(x_test_lasso)\n",
    "\n",
    "\n",
    "rmse_train = mean_squared_error(y_train, y_hat_train, squared=False)\n",
    "rmse_test = mean_squared_error(y_test, y_hat_test, squared=False)\n",
    "\n",
    "print(f\"RMSE train: {rmse_train}\")\n",
    "print(f\"RMSE test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c458de8-94fd-4fef-bd70-6f449f638f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------------------------------------------------------------------------------------------------------\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2_Regularized_Linear_Models_Exercise_answers",
   "notebookOrigID": 706562957547893,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
