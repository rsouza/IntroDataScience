{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b83ed6-4edd-4156-a569-393613eee7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ensembles\n",
    "\n",
    "We have seen in the slide presentation that ensemble methods are better than simple classifiers such as decision trees. But how do ensembles compare? Which one should we use? The answer is that there is no silver bullet, and it dependes on the data, the task and the parameters.  \n",
    "Let's explore some ensemble techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285196ec-79b5-4289-963d-9c68bc239116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34576982-e763-4b10-8a89-0b40947242b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "+ [**Bagging/Pasting**](https://en.wikipedia.org/wiki/Bootstrap_aggregating)    \n",
    "    Building multiple models (typically of the same type) from different subsamples of the training dataset.  \n",
    "+ [**Boosting**](https://en.wikipedia.org/wiki/Boosting_(machine_learning%29)   \n",
    "    Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.  \n",
    "+ [**Stacking/Voting**](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking)    \n",
    "    Building multiple models (typically of differing types) and creating a meta-learner with the features of the models, or simple statistics (like calculating the mean) to combine predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf25090-d181-45cb-808f-f3d7b5b8e429",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load dataset for ensembles comparison\n",
    "\n",
    "We are loading the Pima Indians Diabetes Database .\n",
    "This data set is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the data set is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the data set. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "**Content**\n",
    "\n",
    "The datasets consists of several medical predictor variables and one target variable, 'Outcome'. Predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "Acknowledgements  \n",
    "Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d679e57d-de66-4777-8e25-da67c48f189a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "names = [\"preg\", \"plas\", \"pres\", \"skin\", \"test\", \"mass\", \"pedi\", \"age\", \"class\"]\n",
    "df = pd.read_csv(\"../../Data/pima-indians-diabetes.data.csv\", names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41cf0fe-7ba0-4110-a102-ce0a1c68c7c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8817d55e-8b54-4914-8909-9954e95d995a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that even if we set a random seed, our results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e466819-352b-4ebc-81cb-bc9a8ba52044",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Decision Tree\n",
    "First, let's try with a simple [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef663166-0044-4027-9c91-d6b7d949e91f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cart1 = DecisionTreeClassifier(random_state=seed)\n",
    "results1 = cross_val_score(cart1, X, Y, cv=kfold)\n",
    "print(results1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb9c2c9-e73e-4ab1-a1d1-920c85b9ae36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bagged Decision Trees  \n",
    "Now, let's use the [Scikit-Learn bagging classifier](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator) to manually create a Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4f46cb-0b25-4a15-a8f9-2053f8f24247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "num_trees = 100\n",
    "cart2 = DecisionTreeClassifier()\n",
    "model2 = BaggingClassifier(estimator=cart2, n_estimators=num_trees, random_state=seed)\n",
    "results2 = cross_val_score(model2, X, Y, cv=kfold)\n",
    "print(results2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bf0a79-ce91-4892-abc6-e7dedc72b6f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "[Random forest](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles) is an extension of bagged decision trees.\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc50348-f541-4aeb-911c-09d3dab07eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_features = \"sqrt\"\n",
    "model3 = RandomForestClassifier(\n",
    "    n_estimators=num_trees, \n",
    "    max_features=max_features, \n",
    "    random_state=seed\n",
    ")\n",
    "results3 = cross_val_score(model3, X, Y, cv=kfold)\n",
    "print(results3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b450b20e-f5fd-42aa-ba94-cae9dc444718",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Extra Trees\n",
    "\n",
    "[Extra Trees](https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-forest/)\n",
    "are another modification of bagging where random forests are constructed from the whole training dataset.\n",
    "You can construct an Extra Trees model for classification using the\n",
    "[`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "of\n",
    "[`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913a605f-f018-44fa-925e-4d8c4356b37a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "max_features = 7\n",
    "model4 = ExtraTreesClassifier(\n",
    "    n_estimators=num_trees, max_features=max_features, random_state=seed\n",
    ")\n",
    "results4 = cross_val_score(model4, X, Y, cv=kfold)\n",
    "print(results4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eb897f7-5c98-4fe3-b18f-bf44e0bfc212",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Boosting Algorithms  \n",
    "\n",
    "\n",
    "Boosting ensemble algorithms create a sequence of models that attempt to correct the mistakes of the models before them in the sequence. \n",
    "Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "+ AdaBoost\n",
    "+ Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00384a6a-01bb-4cd2-a2f7-c18d0a70fc9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "[AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or or less attention to them in the construction of subsequent models.  \n",
    "You can construct an AdaBoost model for classification using the AdaBoostClassifier class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730bcd32-b828-4dd7-91ef-b3aa908b18df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model5 = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results5 = cross_val_score(model5, X, Y, cv=kfold)\n",
    "print(results5.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ffc4705-537f-44ba-9836-59a6ecb49978",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Stochastic Gradient Boosting  \n",
    "\n",
    "[Stochastic Gradient Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) (also called Gradient Boosting Machines) is one of the most sophisticated ensemble techniques. It is also a technique that is currently proving to be perhaps of the the best techniques available for improving performance via ensembles.  \n",
    "You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad684b0-d418-4903-a508-c1f4e13e6d74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model6 = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results6 = cross_val_score(model6, X, Y, cv=kfold)\n",
    "print(results6.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8574b31b-c625-465f-b2ba-b887db26bf51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Voting Ensemble\n",
    "\n",
    "[Voting](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier) is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "\n",
    "The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. \n",
    "You can create a voting ensemble model for classification using the\n",
    "[`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) class.\n",
    "The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8b966c-b605-48ae-b53e-7e925644606e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators1 = [\n",
    "    (\"logistic\", LogisticRegression(solver=\"lbfgs\", max_iter=300, random_state=seed)),\n",
    "    (\"cart\", DecisionTreeClassifier(random_state=seed)),\n",
    "    (\"svm\", SVC(random_state=seed)),\n",
    "]\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble1 = VotingClassifier(estimators1)\n",
    "results7 = cross_val_score(ensemble1, X, Y, cv=kfold)\n",
    "print(results7.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e8644a4-f0c4-4b49-98a9-03c59634eedc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Stacking Classifier\n",
    "\n",
    "More advanced methods, similar to voting, can learn how to best weight the predictions from submodels ([stacked generalization](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization)).   \n",
    "We can use the\n",
    "[`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html) from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9ac2c9-8a0f-4e28-88b2-0db2c5c5e0c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators2 = [\n",
    "    (\n",
    "        \"logistic\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(solver=\"lbfgs\", max_iter=300, random_state=seed),\n",
    "        ),\n",
    "    ),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=num_trees, random_state=seed)),\n",
    "    (\"svm\", make_pipeline(StandardScaler(), SVC(random_state=seed))),\n",
    "]\n",
    "\n",
    "\n",
    "ensemble2 = StackingClassifier(\n",
    "    estimators=estimators2,\n",
    "    final_estimator=LogisticRegression(solver=\"lbfgs\", max_iter=300, random_state=seed),\n",
    ")\n",
    "results8 = cross_val_score(ensemble2, X, Y, cv=kfold)\n",
    "print(results8.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fee45d5-1906-4d61-99d0-89386476850d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can check how we have improved simple classifiers using ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f822c7-e0ed-4b86-a5e8-6622098bc44b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Task\n",
    "\n",
    "+ Change the parameters of the estimators above.\n",
    "+ Try to optmize and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f50ffc-5bea-42a6-8264-cc6469d4781e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sample solution\n",
    "\n",
    "There is no single one correct way of solving this task.\n",
    "It is meant to get you working with Ensemble estimators.\n",
    "The code below is meant to inspire you to come up with your own insights.\n",
    "\n",
    "First we want to get an idea what we are comparing our classifiers with. \n",
    "To this end we fit a [`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html).\n",
    "By default the dummy classifier always predicts the majority class in the training set.\n",
    "In our task this is `class == 0` which is about 65% of the observations and thus the mean accuracy for the cross vaildation is about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e420011c-d812-4da6-a3cd-14ae06bf583a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "task1 = cross_val_score(dummy, X, Y, cv=kfold)\n",
    "print(task1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "745818ee-183b-462b-8281-5b5d543317d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Considering the above the performance of the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/tree.html) is not very good.\n",
    "\n",
    "The random Forrest bagging classifiers all improve the result by about 10 percent points each.\n",
    "A similar result is achieved when we utilize the Bagging classifiers with default parameters.\n",
    "\n",
    "At the end we fit two ensemble classifiers both of them improving the baseline result by 12 percent points.\n",
    "However non of the classifiers were properly tuned.\n",
    "An extensive introduction to Hyperparameter tuning will be given in later modules of this course.\n",
    "\n",
    "We will try to improve the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/tree.html) by trying different values for the `max_depth` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc954ed-2fb5-40a1-af4d-5e6a6af9ca27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=i)\n",
    "    task2 = cross_val_score(decision_tree, X, Y, cv=kfold)\n",
    "    print(f\"Decision Tree, max_depth={i} result: Mean Accuracy = {task2.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dcb3d82-c22b-4ab0-b6eb-d2d93f415cb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see above, if we limit the depth of the tree the result can be improved to match the default random forrest classifieres of about 75%. \n",
    "It it worth keeping in mind that for some problems it is actually usefull to spend some time on tuning simpler models befor using more complex ones.\n",
    "\n",
    "Of course we can also try to improve one of ensemble classifiers.\n",
    "Again we don't expect you to use proper hyperparameter tuning at this point in the course.\n",
    "If you are interested in the topic have a look at [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) which is one of the fundamental techniques.\n",
    "\n",
    "Apart from optimizing our hyperparameters we can also experiment with adding more classifiers to our voting ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fce87d-cf0b-4321-951c-e861e53597a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "\n",
    "estimators3 = [\n",
    "    (\"logistic\", LogisticRegression(solver=\"lbfgs\", max_iter=300, random_state=seed)),\n",
    "    (\"tree\", DecisionTreeClassifier(random_state=seed)),\n",
    "    (\"svm\", SVC(random_state=seed)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=num_trees, random_state=seed)),\n",
    "    (\"ada\", AdaBoostClassifier(n_estimators=num_trees, random_state=seed)),\n",
    "    (\"nb\", GaussianNB()),\n",
    "]\n",
    "\n",
    "# Create the ensemble model\n",
    "voting = VotingClassifier(estimators3)\n",
    "task3 = cross_val_score(voting, X, Y, cv=kfold)\n",
    "print(task3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f56bee2-293b-4cd8-8d25-661299e536fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As always more does not automatically mean better.\n",
    "Most of the time simply adding more classifiers will not improve the result.\n",
    "\n",
    "All of the classifiers themself need to perform well to leverage the benefits of voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21debba-df19-455c-badb-b59d478f3a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "\n",
    "estimators3 = [\n",
    "    (\"svm\", SVC(random_state=seed)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=num_trees, random_state=seed)),\n",
    "    (\"nb\", GaussianNB()),\n",
    "]\n",
    "\n",
    "# Create the ensemble model\n",
    "voting = VotingClassifier(estimators3)\n",
    "task3 = cross_val_score(voting, X, Y, cv=kfold)\n",
    "print(task3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01cf6a90-84d2-405a-85d7-95b2a323e42b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Additional reading: [scaling and adjust LR models](https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati)  \n",
    "Some material inspired by this [Source](https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Answer]4_Ensembles_Exercise_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
